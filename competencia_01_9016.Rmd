
title: 'Análisis'
output: html_document


9016

HT 
dataset: fe123rank  (rank todas continuas)
11 hiperparámetros (acortados)
train 1,2,3
0.2
AUC = 


final train 1, 2, 3
dataset:  fe123rank agui (rank todas continuas )
5 semillas

future 06
false future 04

#INICIO


```{r chunk29}
# limpio la memoria
rm(list=ls(all.names=TRUE)) # remove all objects
gc(full=TRUE, verbose=FALSE) # garbage collection
```

### nro exp seeds
```{r}

PARAM <- list()
PARAM$experimento <- 9016

PARAM$semilla_primigenia <- 999199
PARAM$semillas <- c(999199, 999499, 999599, 999959, 999979
                    #, 104729, 523987
                    #, 7919,1299709, 2097593
)

```

```{r chunk22}
getwd()
```

### carpeta exp para R
```{r}

# PARA R
# Definir directorio base
dir_base <- "C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia 1/experimentos"
dir.create(dir_base, showWarnings = FALSE, recursive = TRUE)
# # Crear directorio exp
experimento <- paste0("exp", PARAM$experimento)
dir_experimento <- file.path(dir_base, experimento)
dir.create(dir_experimento, showWarnings = FALSE)

setwd(dir_experimento)
# # Verificar
cat("Directorio de trabajo:", getwd(), "\n")

```

```{r chunk22}
getwd()
```

### períodos
```{r chunk33}
# training y future
PARAM$train <- c(202101, 202102, 202103)
PARAM$train_final <- c(202102, 202101, 202103)
PARAM$future <- c(202106)
PARAM$false_future <- c(202104)
PARAM$semilla_kaggle <- 314159
PARAM$cortes <- seq(6000, 18000, by= 500)
```

### undersampling
```{r chunk34}
# un undersampling de 0.1  toma solo el 10% de los CONTINUA
# undersampling de 1.0  implica tomar TODOS los datos

PARAM$trainingstrategy$undersampling <- 0.2

```
  
### librerías
```{r chunk4}
# cargo las librerias que necesito
require("data.table")
require("parallel")

if(!require("R.utils")) install.packages("R.utils")
require("R.utils")

if( !require("primes") ) install.packages("primes")
require("primes")

if( !require("utils") ) install.packages("utils")
require("utils")

if( !require("rlist") ) install.packages("rlist")
require("rlist")

if( !require("yaml")) install.packages("yaml")
require("yaml")

if( !require("lightgbm") ) install.packages("lightgbm")
require("lightgbm")

if( !require("DiceKriging") ) install.packages("DiceKriging")
require("DiceKriging")

if( !require("mlrMBO") ) install.packages("mlrMBO")
require("mlrMBO")
```

Esta parte se debe correr con el runtime en lenguaje **R** Ir al menu, Runtime -> Change Runtime Tipe -> Runtime type -> R

#cargo dataset crudo
```{r}

require( "data.table" )

# leo el dataset

#VM
# dataset <- fread("/content/datasets/competencia_01_crudo.csv" )

#Rstudio
dataset <- fread("C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia 1/competencia_01_crudo.csv" )

```

## Clase ternaria
```{r chunk1}

#CLASE TERNARIA
# calculo el periodo0 consecutivo
dsimple <- dataset[, list(
    "pos" = .I,
    numero_de_cliente,
    periodo0 = as.integer(foto_mes/100)*12 +  foto_mes%%100 ) ]


# ordeno
setorder( dsimple, numero_de_cliente, periodo0 )

# calculo topes
periodo_ultimo <- dsimple[, max(periodo0) ]
periodo_anteultimo <- periodo_ultimo - 1


# calculo orden 1 y 2
dsimple[, c("periodo1", "periodo2") :=
    shift(periodo0, n=1:2, fill=NA, type="lead"),  numero_de_cliente ]

# assign most common class values = "CONTINUA"
dsimple[ periodo0 < periodo_anteultimo, clase_ternaria := "CONTINUA" ]

# calculo BAJA+1
dsimple[ periodo0 < periodo_ultimo &
    ( is.na(periodo1) | periodo0 + 1 < periodo1 ),
    clase_ternaria := "BAJA+1" ]

# calculo BAJA+2
dsimple[ periodo0 < periodo_anteultimo & (periodo0+1 == periodo1 )
    & ( is.na(periodo2) | periodo0 + 2 < periodo2 ),
    clase_ternaria := "BAJA+2" ]


# pego el resultado en el dataset original y grabo
setorder( dsimple, pos )
dataset[, clase_ternaria := dsimple$clase_ternaria ]

# VM
# fwrite( dataset,
# fwrite( dataset,
#     file =  "/content/datasets/competencia_01.csv.gz",
#     sep = ","
# )

#guardo clase ternaria en segundo lugar persistente
# fwrite(dataset,
# fwrite(dataset,
#        file = "/content/buckets/b1/datasets/competencia_01.csv.gz",
#        sep = ",")


#Rstudio

```

```{r chunk2}
dim(dataset)
```

```{r chunk3}
setorder( dataset, foto_mes, clase_ternaria, numero_de_cliente)
dataset[, .N, list(foto_mes, clase_ternaria)]
```

# DATA ENGINEERING


## LAGS Y DELTAS

```{r chunk6}

generar_lags_y_deltas <- function(dt, variables, max_lags = NULL) {
  # Calcular max_lags si no se especifica (MÁXIMO POSIBLE)
  if(is.null(max_lags)) {
    periodos_totales <- length(unique(dt$foto_mes))
    max_periodos_cliente <- dt[, .N, by = numero_de_cliente][, max(N)]
    max_lags <- min(periodos_totales - 1, max_periodos_cliente - 1)  # TODOS LOS LAGS
    
    #max_lags <- 1  # SOLAMENTE ORDEN X
    cat("Max_lags calculado:", max_lags, "\n")
  }
  # Hacer copia y ordenar
  dt_work <- copy(dt)
  setkeyv(dt_work, c("numero_de_cliente", "foto_mes"))
  cat("Generando lags para", length(variables), "variables hasta lag", max_lags, "\n")
  # GENERAR LAGS
  for(var in variables) {
    cat("Procesando:", var, "\n")
    # Crear todos los lags para esta variable
    for(lag in 1:max_lags) {
      nombre_lag <- paste0(var, "_lag", lag)
      dt_work[, (nombre_lag) := shift(.SD, lag), by = numero_de_cliente, .SDcols = var]
    }
    # Crear todos los delta_lags para esta variable
    for(lag in 1:max_lags) {
      nombre_lag <- paste0(var, "_lag", lag)
      nombre_delta <- paste0(var, "_delta", lag)
      dt_work[, (nombre_delta) := get(var) - get(nombre_lag)]
    }
  }
  cat("✅ Completado!\n")
  cat("Dimensiones finales:", dim(dt_work), "\n")
  return(dt_work)
}

```

## tipos datos
```{r chunk7}
#TIPOS DE DATOS POR COLUMNA
cat("=== TIPOS DE DATOS POR COLUMNA ===\n")
tipos_datos <- sapply(dataset, class)
print(tipos_datos)

```

```{r chunk8}
resumen_tipos <- data.table(
  columna = names(dataset),
  tipo = sapply(dataset, function(x) class(x)[1]),
  tipo_r = sapply(dataset, typeof),
  valores_unicos = sapply(dataset, function(x) length(unique(x))),
  valores_na = sapply(dataset, function(x) sum(is.na(x))),
  porcentaje_na = sapply(dataset, function(x) round(sum(is.na(x))/length(x)*100, 2))
)

print(resumen_tipos)
```

## v excluir
```{r chunk9}
# variables_excluir
variables_excluir <- c("numero_de_cliente", "foto_mes", "clase_ternaria")
```

## v continuas
```{r chunk10}
# variables continuas

variables_continuas <- c(
  # Variables de rentabilidad y comisiones
  "mrentabilidad", "mrentabilidad_annual", "mcomisiones",
  "mactivos_margen", "mpasivos_margen",

  # Cuentas y saldos principales
  "mcuenta_corriente", "mcaja_ahorro", "mcuentas_saldo",

  # Tarjetas - consumos
  "mtarjeta_visa_consumo", "mtarjeta_master_consumo",

  # Préstamos - montos
  "mprestamos_personales", "mprestamos_prendarios", "mprestamos_hipotecarios",

  # Inversiones - montos
  "mplazo_fijo_pesos", "mplazo_fijo_dolares",
  "minversion1_pesos", "minversion1_dolares", "minversion2",

  # Payroll
  "mpayroll", "mpayroll2",

  # Débitos automáticos
  "mcuenta_debitos_automaticos",

  # Servicios y pagos
  "mpagodeservicios", "mpagomiscuentas",

  # Comisiones
  "mcomisiones_mantenimiento", "mcomisiones_otras",

  # Forex
  "mforex_buy", "mforex_sell",

  # Transferencias
  "mtransferencias_recibidas", "mtransferencias_emitidas",

  # Extracciones y ATM
  "mextraccion_autoservicio", "matm", "matm_other",

  # Cheques
  "mcheques_depositados", "mcheques_emitidos",

  # Tarjetas Master y Visa - saldos y pagos
  "Master_msaldototal", "Master_msaldopesos", "Master_msaldodolares",
  "Master_mconsumospesos", "Master_mconsumosdolares",
  "Master_mlimitecompra", "Master_madelantopesos", "Master_madelantodolares",
  "Master_mpagado", "Master_mpagospesos", "Master_mpagosdolares",
  "Master_mconsumototal", "Master_mpagominimo",

  "Visa_msaldototal", "Visa_msaldopesos", "Visa_msaldodolares",
  "Visa_mconsumospesos", "Visa_mconsumosdolares",
  "Visa_mlimitecompra", "Visa_madelantopesos", "Visa_madelantodolares",
  "Visa_mpagado", "Visa_mpagospesos", "Visa_mpagosdolares",
  "Visa_mconsumototal", "Visa_mpagominimo",

  "mcuenta_corriente_adicional", "Master_mfinanciacion_limite", "Visa_mfinanciacion_limite",
"mcheques_emitidos_rechazados", "mcheques_depositados_rechazados", "mtarjeta_master_descuentos",
"mcaja_ahorro_adicional", "mtarjeta_visa_descuentos", "mttarjeta_master_debitos_automaticos",
"mcaja_ahorro_dolares", "mttarjeta_visa_debitos_automaticos", "mautoservicio", "mcajeros_propios_descuentos"
 )

cat("Variables continuas:", length(variables_continuas), "\n")
cat("Variables:\n")
print(variables_continuas)

# Verificar que todas las variables existen en el dataset
variables_existentes <- intersect(variables_continuas, names(dataset))
variables_faltantes <- setdiff(variables_continuas, names(dataset))

if(length(variables_faltantes) > 0) {
  cat("\n⚠️ ADVERTENCIA: Variables no encontradas en dataset:\n")
  print(variables_faltantes)
}

variables_continuas <- variables_existentes
cat("\nVariables confirmadas en dataset:", length(variables_continuas), "\n")
```

```{r chunk11}
#aplico la función a continuas
dataset <- generar_lags_y_deltas(dataset, variables_continuas)

cat(" PROCESO COMPLETADO")
cat("\n Nuevas dimensiones del dataset:", dim(dataset))
cat("\n Dataset actualizado con lags y delta_lags de variables continuas\n")
```

```{r chunk12}
# CONTROL RÁPIDO - Verificar lags y deltas generados

# Variables lag y delta creadas
variables_continuas_lag <- grep("_lag[0-9]+$", names(dataset), value = TRUE)
variables_continuas_delta <- grep("_delta[0-9]+$", names(dataset), value = TRUE)

cat("Lags creados:", length(variables_continuas_lag), "\n")
cat("Deltas creados:", length(variables_continuas_delta), "\n")
cat("Dimensiones dataset:", dim(dataset), "\n\n")

# Ejemplo con un cliente específico
# cliente_ejemplo <- dataset$numero_de_cliente[1000]
# ejemplo <- dataset[numero_de_cliente == cliente_ejemplo,
#                    .(numero_de_cliente, foto_mes,
#                      mrentabilidad, mrentabilidad_lag1, mrentabilidad_lag2,
#                      mrentabilidad_delta1, mrentabilidad_delta2)]
# 
# cat("=== EJEMPLO CLIENTE", cliente_ejemplo, "===\n")
# cat("Períodos del cliente:", nrow(ejemplo), "\n")
# print(ejemplo)

# Verificar que los cálculos sean correctos
# cat("\n=== VERIFICACIÓN DELTA MANUAL ===\n")
# 
# cat("mrentabilidad[2] - mrentabilidad_lag1[2] =",
#     ejemplo$mrentabilidad[2] - ejemplo$mrentabilidad_lag1[2], "\n")
# cat("mrentabilidad_delta1[2] =", ejemplo$mrentabilidad_delta1[2], "\n")
# cat("¿Coinciden?",
#     abs(ejemplo$mrentabilidad[2] - ejemplo$mrentabilidad_lag1[2] - ejemplo$mrentabilidad_delta1[2]) < 1e-10, "\n")

# Conteo de NAs por variable
cat("\n=== CONTEO NAs (primeras 5 variables) ===\n")
for(var in head(variables_continuas, 5)) {
  cat(var, ":", sum(is.na(dataset[[var]])), "NAs\n")
  lag1 <- paste0(var, "_lag1")
  delta1 <- paste0(var, "_delta1")
  if(lag1 %in% names(dataset)) cat("  ", lag1, ":", sum(is.na(dataset[[lag1]])), "NAs\n")
  if(delta1 %in% names(dataset)) cat("  ", delta1, ":", sum(is.na(dataset[[delta1]])), "NAs\n")
}
```

```{r chunk13}
# Variables a explorar = todas - (continuas (con sus lags y deltas), excluir)

variables_explorar <- setdiff(names(dataset),
                              c(variables_continuas, variables_excluir,
                                variables_continuas_lag, variables_continuas_delta))

cat("Variables continuas:", length(variables_continuas), "\n")
cat("Variables excluir:", length(variables_excluir), "\n")
cat("Variables continuas lag creadas:", length(variables_continuas_lag), "\n")
cat("Variables continuas delta creadas:", length(variables_continuas_delta), "\n")
cat("Variables a explorar:", length(variables_explorar), "\n")

# Análisis detallado por niveles
analisis_explorar <- data.table(
  variable = variables_explorar,
  tipo = sapply(dataset[, ..variables_explorar], function(x) class(x)[1]),
  valores_unicos = sapply(dataset[, ..variables_explorar], function(x) length(unique(x[!is.na(x)]))),
  nas = sapply(dataset[, ..variables_explorar], function(x) sum(is.na(x))),
  min_val = sapply(dataset[, ..variables_explorar], function(x) min(x, na.rm = TRUE)),
  max_val = sapply(dataset[, ..variables_explorar], function(x) max(x, na.rm = TRUE))
)

analisis_explorar <- analisis_explorar[order(valores_unicos)]
print(analisis_explorar)
```

## v binarias
```{r chunk14}

# Variables binarias
variables_binarias <- c(
  "active_quarter", "cliente_vip", "cdescubierto_preacordado", "tcallcenter", "thomebanking", "tmobile_app", "Master_delinquency", "Visa_delinquency"
)

#tcuentas no va porque tiene tres valores 0, 1, 2
#ccajas_transacciones  cantidad
#cmobile_app_trx  cantidad

cat("Total variables binarias:", length(variables_binarias), "\n\n")

# Análisis detallado de cada binaria
for(var in variables_binarias) {
  cat("Variable:", var, "\n")

  # Valores únicos y frecuencias
  valores <- sort(unique(dataset[[var]][!is.na(dataset[[var]])]))
  freq_table <- table(dataset[[var]], useNA = "ifany")

  cat("  Valores:", paste(valores, collapse = ", "), "\n")
  cat("  Distribución:", paste(names(freq_table), "=", as.numeric(freq_table), collapse = " | "), "\n")

  # Recomendación para LightGBM
  if(all(valores %in% c(0, 1))) {
    cat(" LightGBM: Mantener como INTEGER (0/1)\n")
  } else {
    cat("LightGBM: Considerar recodificar a 0/1\n")
  }

  cat("  Lags: SÍ recomendado (captura cambios de estado temporal)\n")
  cat("\n")
}

cat("=== RECOMENDACIÓN FINAL ===\n")
cat(" LIGHTGBM: Mantener todas como INTEGER\n")
cat(" LAGS: Aplicar a todas las", length(variables_binarias), "variables binarias\n")
cat(" RAZÓN: Variables binarias capturan estados que tienen continuidad temporal\n")
```

```{r chunk15}
#aplico la función a binarias
dataset <- generar_lags_y_deltas(dataset, variables_binarias)

cat(" PROCESO COMPLETADO")
cat("\n Nuevas dimensiones del dataset:", dim(dataset))
cat("\n Dataset actualizado con lags y delta_lags de variables binarias\n")
```

```{r chunk16}
# CONTROL LAGS Y DELTAS DE BINARIAS

# Variables lag y delta de binarias
variables_binarias_lag <- grep(paste0("(", paste(variables_binarias, collapse = "|"), ")_lag[0-9]+$"),
                     names(dataset), value = TRUE)
variables_binarias_delta <- grep(paste0("(", paste(variables_binarias, collapse = "|"), ")_delta[0-9]+$"),
                       names(dataset), value = TRUE)

cat("=== RESUMEN VARIABLES BINARIAS ===\n")
cat("Variables binarias originales:", length(variables_binarias), "\n")
cat("Lags de binarias creados:", length(variables_binarias_lag), "\n")
cat("Deltas de binarias creados:", length(variables_binarias_delta), "\n\n")

cat("Dimensiones dataset:", dim(dataset), "\n\n")

# Ejemplo con binaria
# if(length(variables_binarias_lag) > 0) {
#   cliente_ejemplo <- dataset$numero_de_cliente[1000]
#   var_ejemplo <- variables_binarias[1]  # Tomar primera binaria
# 
#   ejemplo_binaria <- dataset[numero_de_cliente == cliente_ejemplo,
#                              c("numero_de_cliente", "foto_mes", var_ejemplo,
#                                paste0(var_ejemplo, "_lag1"), paste0(var_ejemplo, "_lag2"),
#                                paste0(var_ejemplo, "_delta1"), paste0(var_ejemplo, "_delta2")),
#                              with = FALSE]
# 
#   cat("=== EJEMPLO BINARIA - CLIENTE", cliente_ejemplo, "- VARIABLE", var_ejemplo, "===\n")
#   cat("Períodos del cliente:", nrow(ejemplo_binaria), "\n")
#   print(ejemplo_binaria)

  # Verificación delta binaria
#   cat("\n=== VERIFICACIÓN DELTA BINARIA ===\n")
#   if(nrow(ejemplo_binaria) >= 2) {
#     original_val <- ejemplo_binaria[[var_ejemplo]][2]
#     lag1_val <- ejemplo_binaria[[paste0(var_ejemplo, "_lag1")]][2]
#     delta1_val <- ejemplo_binaria[[paste0(var_ejemplo, "_delta1")]][2]
# 
#     cat(var_ejemplo, "[2] - ", var_ejemplo, "_lag1[2] =", original_val - lag1_val, "\n")
#     cat(var_ejemplo, "_delta1[2] =", delta1_val, "\n")
#     cat("¿Coinciden?", abs((original_val - lag1_val) - delta1_val) < 1e-10, "\n\n")
#   }
# }

# Conteo de NAs para binarias
cat("=== CONTEO NAs VARIABLES BINARIAS (primeras 3) ===\n")
for(var in head(variables_binarias, 3)) {
  cat(var, ":", sum(is.na(dataset[[var]])), "NAs\n")
  lag1 <- paste0(var, "_lag1")
  delta1 <- paste0(var, "_delta1")
  if(lag1 %in% names(dataset)) cat("  ", lag1, ":", sum(is.na(dataset[[lag1]])), "NAs\n")
  if(delta1 %in% names(dataset)) cat("  ", delta1, ":", sum(is.na(dataset[[delta1]])), "NAs\n")
}

```

## v discretas
```{r chunk17}
# Variables discretas = todas - (continuas (con sus deltas y lags), binarias (con sus deltas y lags), excluir)

variables_discretas <- setdiff(names(dataset),
                              c(variables_continuas, variables_excluir,
                                variables_continuas_lag, variables_continuas_delta,
                                variables_binarias, variables_binarias_lag, variables_binarias_delta
                                ))

cat("Variables continuas:", length(variables_continuas), "\n")
cat("Variables excluir:", length(variables_excluir), "\n")
cat("Variables binarias:", length(variables_binarias), "\n")
cat("Variables lag continuas creadas:", length(variables_continuas_lag), "\n")
cat("Variables delta continuas creadas:", length(variables_continuas_delta), "\n")
cat("Variables lag binarias creadas:", length(variables_binarias_lag), "\n")
cat("Variables delta binarias creadas:", length(variables_binarias_delta), "\n")
cat("Variables discretas:", length(variables_discretas), "\n")

# Análisis detallado por niveles
analisis_discretas <- data.table(
  variable = variables_discretas,
  tipo = sapply(dataset[, ..variables_discretas], function(x) class(x)[1]),
  valores_unicos = sapply(dataset[, ..variables_discretas], function(x) length(unique(x[!is.na(x)]))),
  nas = sapply(dataset[, ..variables_discretas], function(x) sum(is.na(x))),
  min_val = sapply(dataset[, ..variables_discretas], function(x) min(x, na.rm = TRUE)),
  max_val = sapply(dataset[, ..variables_discretas], function(x) max(x, na.rm = TRUE))
)

analisis_discretas <- analisis_discretas[order(valores_unicos)]
print(analisis_discretas)
```

```{r chunk18}
#aplico la función a discretas
dataset <- generar_lags_y_deltas(dataset, variables_discretas)

cat("PROCESO COMPLETADO")
cat("\n Nuevas dimensiones del dataset:", dim(dataset))
cat("\n Dataset actualizado con lags y delta_lags de variables discretas\n")
```

```{r chunk19}
# CONTROL LAGS Y DELTAS DISCRETAS


# Variables lag y delta de discretas
variables_discretas_lag <- grep(paste0("(", paste(variables_discretas, collapse = "|"), ")_lag[0-9]+$"),
                      names(dataset), value = TRUE)
variables_discretas_delta <- grep(paste0("(", paste(variables_discretas, collapse = "|"), ")_delta[0-9]+$"),
                        names(dataset), value = TRUE)

cat("=== RESUMEN VARIABLES DISCRETAS ===\n")
cat("Variables discretas originales:", length(variables_discretas), "\n")
cat("Lags de discretas creados:", length(variables_discretas_lag), "\n")
cat("Deltas de discretas creados:", length(variables_discretas_delta), "\n\n")

cat("Dimensiones dataset:", dim(dataset), "\n\n")

# Ejemplo con discreta
# if(length(variables_discretas_lag) > 0) {
#   cliente_ejemplo <- dataset$numero_de_cliente[1000]
#   var_ejemplo <- variables_discretas[1]  # Tomar primera discreta
# 
#   ejemplo_discreta <- dataset[numero_de_cliente == cliente_ejemplo,
#                               c("numero_de_cliente", "foto_mes", var_ejemplo,
#                                 paste0(var_ejemplo, "_lag1"), paste0(var_ejemplo, "_lag2"),
#                                 paste0(var_ejemplo, "_delta1"), paste0(var_ejemplo, "_delta2")),
#                               with = FALSE]
# 
#   cat("=== EJEMPLO DISCRETA - CLIENTE", cliente_ejemplo, "- VARIABLE", var_ejemplo, "===\n")
#   cat("Períodos del cliente:", nrow(ejemplo_discreta), "\n")
#   print(ejemplo_discreta)

#   # Verificación delta discreta
#   cat("\n=== VERIFICACIÓN DELTA DISCRETA ===\n")
#   if(nrow(ejemplo_discreta) >= 2) {
#     original_val <- ejemplo_discreta[[var_ejemplo]][2]
#     lag1_val <- ejemplo_discreta[[paste0(var_ejemplo, "_lag1")]][2]
#     delta1_val <- ejemplo_discreta[[paste0(var_ejemplo, "_delta1")]][2]
# 
#     cat(var_ejemplo, "[2] - ", var_ejemplo, "_lag1[2] =", original_val - lag1_val, "\n")
#     cat(var_ejemplo, "_delta1[2] =", delta1_val, "\n")
#     cat("¿Coinciden?", abs((original_val - lag1_val) - delta1_val) < 1e-10, "\n\n")
#   }
# }

# Conteo de NAs para discretas
cat("\n=== CONTEO NAs VARIABLES DISCRETAS (primeras 3) ===\n")
for(var in head(variables_discretas, 3)) {
  cat(var, ":", sum(is.na(dataset[[var]])), "NAs\n")
  lag1 <- paste0(var, "_lag1")
  delta1 <- paste0(var, "_delta1")
  if(lag1 %in% names(dataset)) cat("  ", lag1, ":", sum(is.na(dataset[[lag1]])), "NAs\n")
  if(delta1 %in% names(dataset)) cat("  ", delta1, ":", sum(is.na(dataset[[delta1]])), "NAs\n")
}
```


## Resumen lags y deltas
```{r chunk20}
total_lags <- length(grep("_lag[0-9]+$", names(dataset)))
total_deltas <- length(grep("_delta[0-9]+$", names(dataset)))
cat("\nTotal lags creados:", total_lags, "\n")
cat("Total deltas creados:", total_deltas, "\n")

cat("\nFINAL: Nuevas dimensiones del dataset con lags y deltas:", dim(dataset), "\n")
cat("Total variables originales:", length(variables_continuas) + length(variables_binarias) + length(variables_discretas) + length(variables_excluir), "\n")
cat("Total lags y deltas:", total_lags + total_deltas, "\n")
cat("Suma total variables:", length(variables_continuas) + length(variables_binarias) + length(variables_discretas) + length(variables_excluir) + total_lags + total_deltas, "\n")
cat("Multiplicador de variables:", round((dim(dataset)[2]) / (length(variables_continuas) + length(variables_binarias) + length(variables_discretas) + length(variables_excluir)), 1), "x\n")

cat("Variables continuas originales:", length(variables_continuas), "\n")
cat("Lags de continuas creados:", length(variables_continuas_lag), "\n")
cat("Deltas de continuas creados:", length(variables_continuas_delta), "\n\n")

cat("Variables binarias originales:", length(variables_binarias), "\n")
cat("Lags de binarias creados:", length(variables_binarias_lag), "\n")
cat("Deltas de binarias creados:", length(variables_binarias_delta), "\n\n")

cat("Variables discretas originales:", length(variables_discretas), "\n")
cat("Lags de discretas creados:", length(variables_discretas_lag), "\n")
cat("Deltas de discretas creados:", length(variables_discretas_delta), "\n\n")

cat("Variables a excluir originales:", length(variables_excluir), "\n")
```

```{r chunk21}
dim(dataset)
```

```{r chunk22}
getwd()
```

## fwrite competencia_01_fe
dataset con clase ternaria y lags y deltas de orden 1 
```{r chunk23}
# DESCARGAR DATASET ENGINEERED


# fwrite( dataset,
#     file =  "/content/datasets/competencia_01_fe.csv.gz",
#     sep = ","
# )

# # Rstudio
# setwd(dir_experimento)
# fwrite(dataset,
#        file = "competencia_01_fe1.csv.gz", 
#        sep = ",")

```


### levanto fe
```{r}

#levanto dataset con lags 1 VM
# dataset <- fread("~/datasets/competencia_01_fe1.csv.gz", stringsAsFactors= TRUE)

#rstudio local
# dataset <- fread("C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia 1/competencia_01_fe1.csv.gz", stringsAsFactors= TRUE)

# Desde Google Cloud Storage
dataset <- fread("https://storage.googleapis.com/silvanacontreras76_bukito3/datasets/competencia_01_fe.csv.gz", stringsAsFactors = TRUE)

```


```{r chunk24}
nrow(dataset)
ncol(dataset)
```

```{r chunk25}
dataset[, .N, foto_mes]
# dataset[foto_mes == 202101, .N, clase_ternaria]
```

```{r chunk26}
colnames(dataset)
```

```{r chunk27}
dataset[, .N, list(Visa_status, foto_mes)]
```

#fwrite competencia_01_fe123

```{r chunk66a}

# Identificar columnas a eliminar
columnas_eliminar <- grep("lag4|delta4|lag5|delta5", names(dataset), value = TRUE)

# Visualizar las columnas que se van a eliminar
print(paste("Total de columnas a eliminar:", length(columnas_eliminar)))
# print(columnas_eliminar)


```

```{r}
# # Eliminar las columnas
dataset[, (columnas_eliminar) := NULL]
```

```{r chunk67a}
dim(dataset)
```


```{r}
setwd(dir_experimento)
getwd()
fwrite(dataset, file = "competencia01_fe123.csv.gz", sep = ",")

```


## RANKINGS

```{r}
# Borrar todas las columnas que terminen en _rank
cols_rank <- grep("_rank$", names(dataset), value = TRUE)
print(cols_rank)


```


```{r}
# dataset[, (cols_rank) := NULL]
# cat("Columnas borradas:", length(cols_rank), "\n")
```

```{r}
# Rankings con cero fijo: positivos → [0,1], negativos → [-1,0], cero fijo
# Calcula percentiles dentro de cada foto_mes preservando el cero
ranking_cero_fijo <- function(x) {
  positivos <- x > 0 & !is.na(x)
  negativos <- x < 0 & !is.na(x)
  ceros <- x == 0 & !is.na(x)
  nas <- is.na(x)
  
  n_pos <- sum(positivos)
  n_neg <- sum(negativos)
  
  resultado <- numeric(length(x))
  resultado[nas] <- NA
  resultado[ceros] <- 0
  
  if (n_pos > 0) {
    ranks_pos <- frank(x[positivos], ties.method = "average", na.last = "keep")
    resultado[positivos] <- (ranks_pos - 1) / n_pos
  }
  
  if (n_neg > 0) {
    ranks_neg <- frank(x[negativos], ties.method = "average", na.last = "keep")
    resultado[negativos] <- -1 + (ranks_neg - 1) / n_neg
  }
  
  return(resultado)
}

```


```{r}
# Variables para rankear top 20 features solo montos

#  vars_ranking <- c(
#   "mcuentas_saldo",
#   "mcaja_ahorro",
#   "mcuenta_corriente",
#   "mpasivos_margen",
#   "mactivos_margen",
#   "mtarjeta_visa_consumo",
#   "mtarjeta_master_consumo",
#   "mprestamos_personales",
#   "mpayroll",
#   "mrentabilidad",
#   "mrentabilidad_annual"
# )

vars_ranking <- variables_continuas
  
# Aplicar ranking por foto_mes
for (var in vars_ranking) {
  if (var %in% names(dataset)) {
    col_rank <- paste0(var, "_rank")
    dataset[, (col_rank) := ranking_cero_fijo(get(var)), by = foto_mes]
    cat("Ranking creado:", col_rank, "\n")
  }
}

# Validación de rangos
vars_rank <- paste0(vars_ranking[vars_ranking %in% names(dataset)], "_rank")
cat("\n=== VALIDACIÓN DE RANGOS ===\n")
dataset[, lapply(.SD, function(x) {
  c(min = round(min(x, na.rm = TRUE), 6), 
    max = round(max(x, na.rm = TRUE), 6),
    ceros = sum(x == 0, na.rm = TRUE))
}), .SDcols = vars_rank]

cat("\nRankings aplicados correctamente\n")
cat("Interpretación: 1 = mejor cliente, -1 = peor cliente, 0 = valor cero\n")


```



```{r}
# library(data.table)
# library(ggplot2)
# library(gridExtra)
# 
# visualizar_ranking <- function(dataset, variable, mes_ejemplo = 202101) {
#   
#   var_rank <- paste0(variable, "_rank")
#   
#   if (!variable %in% names(dataset)) {
#     stop(paste("Variable", variable, "no existe en el dataset"))
#   }
#   
#   if (!var_rank %in% names(dataset)) {
#     stop(paste("Ranking", var_rank, "no existe. Ejecutar ranking_cero_fijo primero"))
#   }
#   
#   dt_plot <- dataset[foto_mes == mes_ejemplo]
#   
#   cat("\n=== COMPOSICIÓN DE", variable, "===\n")
#   stats <- dt_plot[, .(
#     n_total = .N,
#     n_nas = sum(is.na(get(variable))),
#     pct_nas = round(100 * sum(is.na(get(variable))) / .N, 2),
#     n_ceros = sum(get(variable) == 0, na.rm = TRUE),
#     pct_ceros = round(100 * sum(get(variable) == 0, na.rm = TRUE) / .N, 2),
#     n_positivos = sum(get(variable) > 0, na.rm = TRUE),
#     pct_positivos = round(100 * sum(get(variable) > 0, na.rm = TRUE) / .N, 2),
#     n_negativos = sum(get(variable) < 0, na.rm = TRUE),
#     pct_negativos = round(100 * sum(get(variable) < 0, na.rm = TRUE) / .N, 2)
#   )]
#   print(stats)
#   
#   tiene_negativos <- stats$n_negativos > 0
#   tiene_ceros <- stats$n_ceros > 0
#   tiene_nas <- stats$n_nas > 0
#   
#   # Gráfico 1: Densidad original (sin ceros ni NAs)
#   if (tiene_negativos) {
#     p1 <- ggplot(dt_plot[get(variable) != 0 & !is.na(get(variable))], 
#                  aes(x = abs(get(variable)), fill = get(variable) > 0)) +
#       geom_density(alpha = 0.6) +
#       scale_x_log10(labels = scales::comma) +
#       scale_fill_manual(values = c("TRUE" = "darkgreen", "FALSE" = "darkred"),
#                         labels = c("TRUE" = "Positivos", "FALSE" = "Negativos")) +
#       labs(title = paste("Distribución Original:", variable),
#            subtitle = paste0("foto_mes = ", mes_ejemplo, " | Escala log (sin ceros/NAs)"),
#            x = "Valor Absoluto (log10)", y = "Densidad", fill = "") +
#       theme_minimal() +
#       theme(legend.position = "top")
#   } else {
#     p1 <- ggplot(dt_plot[get(variable) > 0 & !is.na(get(variable))], 
#                  aes(x = get(variable))) +
#       geom_density(fill = "darkgreen", alpha = 0.6) +
#       scale_x_log10(labels = scales::comma) +
#       labs(title = paste("Distribución Original:", variable),
#            subtitle = paste0("foto_mes = ", mes_ejemplo, " | Escala log"),
#            x = "Valor (log10)", y = "Densidad") +
#       theme_minimal()
#   }
#   
#   # Gráfico 2: Histograma del ranking (más informativo que densidad)
#   p2 <- ggplot(dt_plot[!is.na(get(var_rank))], aes(x = get(var_rank))) +
#     geom_histogram(bins = 50, fill = "darkgreen", alpha = 0.7, color = "white") +
#     geom_vline(xintercept = 0, color = "red", linetype = "dashed", linewidth = 1) +
#     geom_vline(xintercept = c(-1, 1), color = "blue", linetype = "dotted") +
#     scale_x_continuous(limits = c(-1.05, 1.05), breaks = seq(-1, 1, 0.25)) +
#     labs(title = "Histograma del Ranking",
#          subtitle = "Línea roja = cero | Líneas azules = extremos [-1, 1]",
#          x = "Ranking [-1, 1]", y = "Frecuencia") +
#     theme_minimal()
#   
#   # Gráfico 3: Boxplot
#   if (tiene_negativos) {
#     p3 <- ggplot(dt_plot[get(variable) != 0 & !is.na(get(variable))], 
#                  aes(x = factor(sign(get(variable))), y = abs(get(variable)))) +
#       geom_boxplot(fill = c("darkred", "darkgreen"), alpha = 0.6) +
#       scale_y_log10(labels = scales::comma) +
#       scale_x_discrete(labels = c("-1" = "Negativos", "1" = "Positivos")) +
#       labs(title = "Boxplot por signo (escala log)",
#            x = "", y = "Valor Absoluto (log10)") +
#       theme_minimal()
#   } else {
#     p3 <- ggplot(dt_plot[get(variable) > 0 & !is.na(get(variable))], 
#                  aes(x = "", y = get(variable))) +
#       geom_boxplot(fill = "darkgreen", alpha = 0.6) +
#       scale_y_log10(labels = scales::comma) +
#       labs(title = "Boxplot (escala log)",
#            x = "", y = "Valor (log10)") +
#       theme_minimal()
#   }
#   
#   # Gráfico 4: Scatter
#   set.seed(123)
#   muestra <- dt_plot[sample(.N, min(.N, 5000))]
#   
#   p4 <- ggplot(muestra[get(variable) != 0 & !is.na(get(variable))], 
#                aes(x = get(variable), y = get(var_rank))) +
#     geom_point(alpha = 0.3, color = "darkgreen", size = 1) +
#     geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
#     geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
#     scale_x_continuous(labels = scales::comma) +
#     labs(title = "Transformación: Original → Ranking",
#          x = "Valor Original", y = "Ranking") +
#     theme_minimal()
#   
#   print(p1)
#   print(p2)
#   grid.arrange(p3, p4, ncol = 2)
#   
#   cat("\n=== ESTADÍSTICAS DETALLADAS ===\n")
#   if (tiene_nas) {
#     cat("\nNAs:", stats$n_nas, "(", stats$pct_nas, "%)\n")
#   }
#   if (tiene_negativos) {
#     cat("\nValores negativos:\n")
#     print(summary(dt_plot[get(variable) < 0][[variable]]))
#   }
#   if (tiene_ceros) {
#     cat("\nValores cero:", stats$n_ceros, "\n")
#   }
#   cat("\nValores positivos:\n")
#   print(summary(dt_plot[get(variable) > 0][[variable]]))
#   
#   cat("\nRanking:\n")
#   print(summary(dt_plot[[var_rank]]))
#   cat("\nRanking NAs:", sum(is.na(dt_plot[[var_rank]])), "\n")
#   
#   invisible(list(stats = stats))
# }
```

```{r}
# visualizar_ranking(dataset, "mcuentas_saldo", mes_ejemplo = 202101) 

```

```{r}
# analizar_ranking <- function(dataset, variable, mes_ejemplo = 202101) {
#   
#   var_rank <- paste0(variable, "_rank")
#   
#   if (!var_rank %in% names(dataset)) {
#     stop(paste("Ranking", var_rank, "no existe en el dataset"))
#   }
#   
#   cat("\n=== ESTADÍSTICAS DEL RANKING:", var_rank, "===\n")
#   cat("Mes:", mes_ejemplo, "\n\n")
#   
#   # Estadísticas completas
#   stats <- dataset[foto_mes == mes_ejemplo, .(
#     min_ranking = min(get(var_rank), na.rm = TRUE),
#     max_ranking = max(get(var_rank), na.rm = TRUE),
#     cuantos_negativos = sum(get(var_rank) < 0, na.rm = TRUE),
#     cuantos_igual_cero = sum(get(var_rank) == 0, na.rm = TRUE),
#     cuantos_positivos = sum(get(var_rank) > 0, na.rm = TRUE),
#     cuantos_na = sum(is.na(get(var_rank)))
#   )]
#   print(stats)
#   
#   # Los 10 peores (más negativos)
#   cat("\n=== 10 PEORES (ranking más negativo) ===\n")
#   peores <- dataset[foto_mes == mes_ejemplo, 
#                     c(variable, var_rank), 
#                     with = FALSE][order(get(var_rank))][1:10]
#   print(peores)
#   
#   # Los 10 cercanos a cero
#   cat("\n=== 10 CLIENTES CERCA DEL CERO ===\n")
#   cercanos_cero <- dataset[foto_mes == mes_ejemplo, 
#                            c(variable, var_rank), 
#                            with = FALSE][order(abs(get(var_rank)))][1:10]
#   print(cercanos_cero)
#   
#   # Los 10 mejores (más positivos)
#   cat("\n=== 10 MEJORES (ranking más positivo) ===\n")
#   mejores <- dataset[foto_mes == mes_ejemplo, 
#                      c(variable, var_rank), 
#                      with = FALSE][order(-get(var_rank))][1:10]
#   print(mejores)
#   
#   invisible(list(stats = stats, peores = peores, 
#                  cercanos_cero = cercanos_cero, mejores = mejores))
# }


```

```{r}

# analizar_ranking(dataset, "mcuentas_saldo", mes_ejemplo = 202101)

```

```{r}

# histograma_ranking_temporal <- function(dataset, variable, periodos = NULL) {
#   
#   var_rank <- paste0(variable, "_rank")
#   
#   if (!var_rank %in% names(dataset)) {
#     stop(paste("Ranking", var_rank, "no existe en el dataset"))
#   }
#   
#   # Si no se especifican períodos, usar todos
#   if (is.null(periodos)) {
#     periodos <- unique(dataset$foto_mes)
#   }
#   
#   # Detectar si hay valores negativos en el ranking
#   tiene_negativos <- dataset[foto_mes %in% periodos, 
#                              any(get(var_rank) < 0, na.rm = TRUE)]
#   
#   # Ajustar límites y breaks según si hay negativos
#   if (tiene_negativos) {
#     limites <- c(-1.05, 1.05)
#     breaks_seq <- seq(-1, 1, 0.5)
#     rango_label <- "[-1, 1]"
#   } else {
#     limites <- c(-0.05, 1.05)
#     breaks_seq <- seq(0, 1, 0.25)
#     rango_label <- "[0, 1]"
#   }
#   
#   # Crear gráfico
#   p <- ggplot(dataset[foto_mes %in% periodos & !is.na(get(var_rank))], 
#               aes(x = get(var_rank))) +
#     geom_histogram(bins = 50, fill = "darkgreen", alpha = 0.7) +
#     facet_wrap(~foto_mes, scales = "free_y") +
#     geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
#     geom_vline(xintercept = c(-1, 1), color = "blue", linetype = "dotted", alpha = 0.5) +
#     scale_x_continuous(limits = limites, breaks = breaks_seq) +
#     labs(title = paste("Evolución del Ranking:", variable),
#          x = paste("Ranking", rango_label), 
#          y = "Frecuencia") +
#     theme_minimal() +
#     theme(axis.text.x = element_text(angle = 45, hjust = 1))
#   
#   print(p)
#   invisible(p)
# }


```


```{r}

# Todos los períodos
# histograma_ranking_temporal(dataset, "mcuentas_saldo")

```

```{r}

evolucion_cliente_ranking <- function(dataset, variable, min_meses = 6, cliente_especifico = NULL) {
  
  var_rank <- paste0(variable, "_rank")
  
  if (!var_rank %in% names(dataset)) {
    stop(paste("Ranking", var_rank, "no existe en el dataset"))
  }
  
  # Si no se especifica cliente, elegir uno al azar
  if (is.null(cliente_especifico)) {
    set.seed(NULL)
    clientes_validos <- dataset[!is.na(get(var_rank)), 
                                .(n_meses = .N), 
                                by = numero_de_cliente][n_meses >= min_meses]
    
    if (nrow(clientes_validos) == 0) {
      stop(paste("No hay clientes con al menos", min_meses, "meses de datos válidos"))
    }
    
    cliente_ejemplo <- clientes_validos[sample(.N, 1)]$numero_de_cliente
  } else {
    cliente_ejemplo <- cliente_especifico
  }
  
  cat("Cliente seleccionado:", cliente_ejemplo, "\n")
  
  # Extraer evolución
  evolucion <- dataset[numero_de_cliente == cliente_ejemplo, 
                       c("foto_mes", variable, var_rank), 
                       with = FALSE]
  
  print(evolucion)
  
  # Detectar si hay valores negativos
  tiene_negativos <- any(evolucion[[var_rank]] < 0, na.rm = TRUE)
  
  # Ajustar escala Y según si hay negativos
  if (tiene_negativos) {
    limites_y <- c(-100, 100)
    breaks_y <- seq(-100, 100, 20)
  } else {
    limites_y <- c(0, 100)
    breaks_y <- seq(0, 100, 20)
  }
  
  # Crear subtítulo con información de la variable
  valor_actual <- evolucion[.N][[variable]]
  if (!is.na(valor_actual)) {
    subtitulo <- paste(variable, "actual:", round(valor_actual, 2))
  } else {
    subtitulo <- paste(variable, ": ver tabla")
  }
  
  # Graficar
  p <- ggplot(evolucion[!is.na(get(var_rank))], 
              aes(x = foto_mes, y = get(var_rank) * 100)) +
    geom_line(color = "darkgreen", linewidth = 1) +
    geom_point(color = "darkgreen", size = 3) +
    geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
    scale_y_continuous(name = "Ranking (percentil)", 
                       breaks = breaks_y,
                       limits = limites_y) +
    labs(title = paste("Evolución Ranking", variable, "- Cliente", cliente_ejemplo),
         subtitle = subtitulo,
         x = "Mes") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
  print(p)
  
  invisible(list(cliente = cliente_ejemplo, evolucion = evolucion, plot = p))
}
```



```{r}

# Cliente aleatorio
evolucion_cliente_ranking(dataset, "mcuentas_saldo")

# # Cliente específico
# evolucion_cliente_ranking(dataset, "mcuentas_saldo", cliente_especifico = 123456)
# 
# # Cambiar mínimo de meses
# evolucion_cliente_ranking(dataset, "mcuentas_saldo", min_meses = 10)

```


```{r}

# Uso:
# Cliente aleatorio
evolucion_cliente_ranking(dataset, "mpayroll")
```
```{r}
# Verificar que el ranking se calculó por mes
cat("\n=== VERIFICACIÓN: ¿Se calculó el ranking por mes? ===\n")

# Ver algunos valores para diferentes meses
dataset[numero_de_cliente == dataset[1]$numero_de_cliente, 
        .(foto_mes, mcuentas_saldo, mcuentas_saldo_rank)]

# Verificar que los rankings son diferentes entre meses
cat("\n=== Estadísticas del ranking por mes ===\n")
dataset[!is.na(mcuentas_saldo_rank), 
        .(min_rank = min(mcuentas_saldo_rank),
          max_rank = max(mcuentas_saldo_rank),
          media_rank = mean(mcuentas_saldo_rank),
          n_obs = .N), 
        by = foto_mes]

# Ver si mcuentas_saldo varía entre meses
cat("\n=== mcuentas_saldo varía entre meses? ===\n")
dataset[, .(min_saldo = min(mcuentas_saldo, na.rm = TRUE),
            max_saldo = max(mcuentas_saldo, na.rm = TRUE),
            media_saldo = mean(mcuentas_saldo, na.rm = TRUE)), 
        by = foto_mes]
```

```{r}
# Función para ver los umbrales de percentiles por mes
umbrales_percentiles <- function(dataset, variable, percentiles = c(0.1, 0.25, 0.5, 0.75, 0.9)) {
  
  var_rank <- paste0(variable, "_rank")
  
  if (!var_rank %in% names(dataset)) {
    stop(paste("Ranking", var_rank, "no existe en el dataset"))
  }
  
  # Calcular umbrales para cada mes
  umbrales <- dataset[!is.na(get(var_rank)), 
                      .(percentil_10 = quantile(get(variable)[get(var_rank) > 0], 0.1, na.rm = TRUE),
                        percentil_25 = quantile(get(variable)[get(var_rank) > 0], 0.25, na.rm = TRUE),
                        percentil_50 = quantile(get(variable)[get(var_rank) > 0], 0.5, na.rm = TRUE),
                        percentil_75 = quantile(get(variable)[get(var_rank) > 0], 0.75, na.rm = TRUE),
                        percentil_90 = quantile(get(variable)[get(var_rank) > 0], 0.9, na.rm = TRUE),
                        n_negativos = sum(get(variable) < 0),
                        n_ceros = sum(get(variable) == 0),
                        n_positivos = sum(get(variable) > 0)), 
                      by = foto_mes]
  
  cat("\n=== UMBRALES DE PERCENTILES PARA", variable, "===\n")
  cat("¿Qué valor de", variable, "define cada percentil en cada mes?\n\n")
  print(umbrales)
  
  # Graficar evolución de umbrales
  umbrales_long <- melt(umbrales, 
                        id.vars = "foto_mes", 
                        measure.vars = c("percentil_10", "percentil_25", 
                                        "percentil_50", "percentil_75", "percentil_90"),
                        variable.name = "percentil",
                        value.name = "valor_umbral")
  
  p <- ggplot(umbrales_long, aes(x = foto_mes, y = valor_umbral, 
                                  color = percentil, group = percentil)) +
    geom_line(linewidth = 1) +
    geom_point(size = 2) +
    scale_y_continuous(labels = scales::comma) +
    scale_color_manual(values = c("percentil_10" = "#d73027",
                                   "percentil_25" = "#fc8d59",
                                   "percentil_50" = "#fee090",
                                   "percentil_75" = "#91bfdb",
                                   "percentil_90" = "#4575b4"),
                       labels = c("P10", "P25", "P50 (mediana)", "P75", "P90")) +
    labs(title = paste("Evolución de Umbrales de Percentiles:", variable),
         subtitle = "¿Cuánto necesita un cliente para estar en cada percentil?",
         x = "Mes", y = paste("Valor de", variable),
         color = "Percentil") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "right")
  
  print(p)
  
  invisible(list(umbrales = umbrales, plot = p))
}



```

```{r}
# Uso
umbrales_percentiles(dataset, "mcuentas_saldo")

# También ver para comparar qué cliente está en qué percentil en diferentes meses
cat("\n=== EJEMPLO: ¿En qué percentil está un cliente mes a mes? ===\n")
cliente_ej <- dataset[!is.na(mcuentas_saldo_rank)][1]$numero_de_cliente
dataset[numero_de_cliente == cliente_ej, 
        .(foto_mes, 
          mcuentas_saldo, 
          ranking = round(mcuentas_saldo_rank, 3),
          percentil = paste0("P", round(mcuentas_saldo_rank * 100, 1)))]
```

##fwrite competencia_01_fe123_rank


```{r}
setwd(dir_experimento)
getwd()
fwrite(dataset, file = "competencia01_fe123_rank.csv.gz", sep = ",")

```

```{r}
dim(dataset)
```


```{r}
cols_rank <- grep("_rank$", names(dataset), value = TRUE)
print(cols_rank)

```

### levanto fe123_Rank
```{r}

getwd()
# dataset <- fread("competencia01_fe123_rank.csv.gz", stringsAsFactors = TRUE)


```


## AGUINALDO
clientes que tuvieron payroll en 04 y 06, tienen al menos 1 ctrxpayroll en junio y su mpayroll de junio es al menos 40% mayor que el de abril. (me quedan clientes con ctrxpayroll positivo sin asignar a la columna). Por lo tanto lo que identifico no son empleados, sino aproximo aguinaldos.

```{r}
# ==============================================================================
# LIMPIAR COMPLETAMENTE: recibio_aguinaldo, aguinaldo_estimado, mpayroll_normalizado
# Esta celda borra todas las asignaciones para empezar de cero
# ==============================================================================

# Eliminar las columnas si existen
dataset[, recibio_aguinaldo := NULL]
dataset[, aguinaldo_estimado := NULL]
dataset[, mpayroll_normalizado := NULL]

cat("✓ Columnas eliminadas completamente.\n")
cat("✓ Listo para volver a ejecutar desde cero.\n")
```

### recibio_aguinaldo 202106
```{r}

#VERIFICACIÓN DE LAGS
cat("VERIFICACIÓN DE LAGS\n")
cliente_test <- dataset[foto_mes == 202106, numero_de_cliente][1]
verificacion <- dataset[numero_de_cliente == cliente_test & foto_mes %in% c(202104, 202106),
                        .(numero_de_cliente, foto_mes, mpayroll)]
print(verificacion)

lag2_correcto <- dataset[numero_de_cliente == cliente_test & foto_mes == 202106, mpayroll_lag2]
valor_202104 <- dataset[numero_de_cliente == cliente_test & foto_mes == 202104, mpayroll]

cat("\nLag2 en 202106:", lag2_correcto, "\n")
cat("Valor real 202104:", valor_202104, "\n")
cat("Coinciden:", isTRUE(all.equal(lag2_correcto, valor_202104)), "\n\n")

# DETECCIÓN DE QUIEN RECIBIÓ AGUINALDO
dataset[, recibio_aguinaldo := 0L]

dataset[foto_mes == 202106, recibio_aguinaldo := ifelse(
  cpayroll_trx_lag2 > 0 & #sueldo en abril
  cpayroll_trx_lag1 > 0 & #sueldo en abril
  cpayroll_trx > 0 & #sueldo en junio
  mpayroll > 1.4 * mpayroll_lag1 & #sueldo junio 40% mayor que abril
  !is.na(mpayroll) & !is.na(mpayroll_lag2) & #sueldo abril y junio no NA
  mpayroll > 0 & mpayroll_lag1 > 0 & mpayroll_lag2 > 0, #sueldo abril y junio positivo
  1L, 0L
)]

cat("\nDISTRIBUCIÓN DE 'recibio_aguinaldo' en 202106:\n")
print(dataset[foto_mes == 202106, .N, by = recibio_aguinaldo])


cat("\nDISTRIBUCIÓN cpayroll_trx:\n")
print(dataset[foto_mes == 202106 & recibio_aguinaldo == 1, .N, by = cpayroll_trx])

```


```{r}
tabla_aguinaldo <- dataset[foto_mes == 202106, .N, by = recibio_aguinaldo]
tabla_aguinaldo[, proporcion := round(N / sum(N) * 100, 2)]
print(tabla_aguinaldo)

```

```{r}

# ANALISIS DE ESTABILIDAD DE MPAYROLL EN MESES TRAIN
# Filtrar solo clientes que recibieron aguinaldo en junio
clientes_con_aguinaldo <- dataset[foto_mes == 202106 & recibio_aguinaldo == 1, 
                                   unique(numero_de_cliente)]

# Calcular promedio de mpayroll por mes para esos clientes
evolucion_mpayroll <- dataset[numero_de_cliente %in% clientes_con_aguinaldo & 
                               !is.na(mpayroll),
                               .(mpayroll_promedio = mean(mpayroll, na.rm = TRUE),
                                 mpayroll_mediana = median(mpayroll, na.rm = TRUE),
                                 n_clientes = .N),
                               by = foto_mes][order(foto_mes)]

# Gráfico
library(ggplot2)
ggplot(evolucion_mpayroll, aes(x = foto_mes, y = mpayroll_promedio)) +
  geom_line(color = "blue", size = 1.2) +
  geom_point(color = "blue", size = 3) +
  geom_line(aes(y = mpayroll_mediana), color = "red", linetype = "dashed") +
  geom_point(aes(y = mpayroll_mediana), color = "red", size = 2) +
  labs(title = "Evolución de mpayroll - Clientes con aguinaldo en 202106",
       subtitle = "Línea azul: promedio | Línea roja: mediana",
       x = "Mes",
       y = "mpayroll") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Ver los números
print(evolucion_mpayroll)
```


### mpayroll_normalizado 202106

mpayroll normalizado representa cuánto tendrían en sus cuentas si hubieran mantenido el nivel de mayo. Se calcula restando del saldo de junio el delta positivo respecto a mayo (pmax(saldo_junio - saldo_mayo, 0)), asumiendo que ese incremento corresponde al aguinaldo depositado y no gastado inmediatamente. Para clientes sin aguinaldo o en otros meses, los saldos normalizados son idénticos a los originales. Y le sumé 3% inflación.


```{r }


dataset[, mpayroll_normalizado := mpayroll]

dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
        aguinaldo_estimado := pmax(mpayroll_delta1, 0)] #pmax evito negativos

# Se resta el aguinaldo estimado y luego se multiplica el resultado
# por 1.03 para sumar el 3,2% de inflación.
dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
        mpayroll_normalizado := (mpayroll - aguinaldo_estimado) * 1.032]

```



```{r}
# # Ver algunos casos individuales
# dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
#         .(mpayroll,
#           mpayroll_lag1,
#           aguinaldo_estimado,
#           mpayroll_normalizado,
#           diferencia = mpayroll - mpayroll_normalizado)][1:10]
```

```{r}
# 
# library(ggplot2)
# 
# # 1. IDENTIFICAR CLIENTES CON AGUINALDO
# clientes_con_aguinaldo <- dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
#                                    unique(numero_de_cliente)]
# 
# # 2. ASIGNAR LA COLUMNA 'es_empleado' AL DATASET PRINCIPAL
# # Se asigna a todos los registros del cliente (todos los meses)
# dataset[, es_empleado := numero_de_cliente %in% clientes_con_aguinaldo]
# 
# # 3. CALCULAR LA EVOLUCIÓN: Promedios agrupados por mes y por 'es_empleado'
# evolucion <- dataset[,
#                      .(mpayroll_prom = mean(mpayroll, na.rm = TRUE),
#                        mpayroll_norm_prom = mean(mpayroll_normalizado, na.rm = TRUE)
#                        ),
#                      by = .(foto_mes, es_empleado)] # Agrupación por la nueva columna
# 
# # --- GRÁFICO 0: mpayroll (CORRECCIÓN DE COLORES) ---
# p0 <- ggplot(evolucion, aes(x = foto_mes, group = es_empleado)) +
#   geom_line(aes(y = mpayroll_prom, color = factor(es_empleado)), size = 1) +
#   geom_line(aes(y = mpayroll_norm_prom, color = factor(es_empleado)),
#             linetype = "dashed", size = 1) +
#   geom_point(aes(y = mpayroll_prom, color = factor(es_empleado))) +
#   # 🟢 CORRECCIÓN: Se usa "FALSE" y "TRUE" como claves para mapear el factor lógico
#   scale_color_manual(values = c("FALSE" = "#E74C3C", "TRUE" = "#3498DB"),
#                      labels = c("Sin aguinaldo", "Con aguinaldo")) +
#   labs(title = paste0("Evolución mpayroll - EXP ", PARAM$experimento),
#        subtitle = "Línea sólida: original | Línea punteada: normalizada",
#        x = "Período", y = "Saldo Promedio", color = "Grupo") +
#   theme_minimal()
# 
# print(p0)
# 
# # Ver los datos de empleados en junio
# evolucion[es_empleado == TRUE, 
#           .(foto_mes,
#             mpayroll_original = round(mpayroll_prom, 0),
#             mpayroll_normalizado = round(mpayroll_norm_prom, 0),
#             diferencia = round(mpayroll_prom - mpayroll_norm_prom, 0))]
# 
# # --- LIMPIEZA ---
# 
# # Eliminar columna auxiliar 'es_empleado' para mantener el dataset limpio
# dataset[, es_empleado := NULL]

```


### recibio_aguinaldo backwards
```{r}

# PROPAGO RECIBIO AGUINALDO HACIA ATRAS

# Primero: poner NA a todos los que NO están en 202106
clientes_en_202106 <- dataset[foto_mes == 202106, unique(numero_de_cliente)]
dataset[!(numero_de_cliente %in% clientes_en_202106), recibio_aguinaldo := NA_integer_]

# 1. Clientes con 1 en 202106 → poner 1 en todos los meses
clientes_con_1 <- dataset[foto_mes == 202106 & recibio_aguinaldo == 1, numero_de_cliente]
dataset[numero_de_cliente %in% clientes_con_1, recibio_aguinaldo := 1]

# 2. Clientes con 0 en 202106 → poner 0 en todos los meses
clientes_con_0 <- dataset[foto_mes == 202106 & recibio_aguinaldo == 0, numero_de_cliente]
dataset[numero_de_cliente %in% clientes_con_0, recibio_aguinaldo := 0]


```


```{r}
# analisis de recibio aguinaldo Contar únicos y registros por mes
dataset[, .(
  valores_unicos = paste(sort(unique(recibio_aguinaldo)), collapse = ", "),
  n_con_1 = sum(recibio_aguinaldo == 1, na.rm = TRUE),
  n_con_0 = sum(recibio_aguinaldo == 0, na.rm = TRUE),
  n_con_NA = sum(is.na(recibio_aguinaldo)),
  total_registros = .N
), by = foto_mes][order(foto_mes)]

```


### efecto aguinaldo hacia atrás
```{r}
# ==============================================================================
# PARCHE: EXTENDER LÓGICA DE AGUINALDO A MESES DE TRAIN CON AJUSTE POR INFLACIÓN
# Este bloque aplica el cálculo de aguinaldo_estimado y mpayroll_normalizado
# a todos los meses != 202106 para clientes con recibio_aguinaldo == 1
# 
# INFLACIÓN MENSUAL 2021 (ajustable):
# Enero: 4.1%, Febrero: 3.6%, Marzo: 4.8%, Abril: 4.1%, Mayo: 3.3%, Junio: 3.2%
#
# Para deshacer: simplemente comentar o eliminar este bloque completo
# ==============================================================================

# Factores de inflación mensual (ajustables)
inflacion_mensual <- data.table(
  foto_mes = c(202101, 202102, 202103, 202104, 202105, 202106),
  inflacion = c(0.041, 0.036, 0.048, 0.041, 0.033, 0.032)
)

# Calcular inflación acumulada desde cada mes hasta junio
# Enero a Junio: (1+0.036)*(1+0.048)*(1+0.041)*(1+0.033)*(1+0.032) = 1.205
# Febrero a Junio: (1+0.048)*(1+0.041)*(1+0.033)*(1+0.032) = 1.162
# Marzo a Junio: (1+0.041)*(1+0.033)*(1+0.032) = 1.109
# Abril a Junio: (1+0.033)*(1+0.032) = 1.066
# Mayo a Junio: (1+0.032) = 1.032
# Junio: 1.0 (sin ajuste)

inflacion_acumulada <- data.table(
  foto_mes = c(202101, 202102, 202103, 202104, 202105, 202106),
  factor_deflactor = c(1.205, 1.162, 1.109, 1.066, 1.032, 1.0)
)

# Guardar delta1 de 202106 por cliente para usar en otros meses
delta1_junio <- dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
                        .(numero_de_cliente, delta1_junio = mpayroll_delta1)]

# Mergear ese delta a todo el dataset
dataset <- merge(dataset, delta1_junio, by = "numero_de_cliente", all.x = TRUE)

# Mergear factores de inflación
dataset <- merge(dataset, inflacion_acumulada, by = "foto_mes", all.x = TRUE)

# Calcular aguinaldo_estimado en meses != 202106 usando delta de junio DEFLACTADO
dataset[foto_mes != 202106 & recibio_aguinaldo == 1 & !is.na(delta1_junio),
        aguinaldo_estimado := pmax(delta1_junio / factor_deflactor, 0)]

# Calcular mpayroll_normalizado en meses != 202106 (SIN el *1.03)
dataset[foto_mes != 202106 & recibio_aguinaldo == 1 & !is.na(aguinaldo_estimado),
        mpayroll_normalizado := pmax(mpayroll - aguinaldo_estimado, 0)]

# Limpiar columnas temporales
dataset[, c("delta1_junio", "factor_deflactor") := NULL]

cat("\n=== PARCHE CON AJUSTE POR INFLACIÓN APLICADO ===\n")
cat("Factores de deflactación usados:\n")
print(inflacion_acumulada)
cat("\nVerificación de mpayroll_normalizado por mes (clientes con aguinaldo=1):\n")
print(dataset[recibio_aguinaldo == 1, 
              .(n = .N,
                promedio_mpayroll = mean(mpayroll, na.rm = TRUE),
                promedio_normalizado = mean(mpayroll_normalizado, na.rm = TRUE),
                n_ceros = sum(mpayroll_normalizado == 0, na.rm = TRUE)),
              by = foto_mes][order(foto_mes)])

# ==============================================================================
# FIN DEL PARCHE
# ==============================================================================
```


```{r chunk67a}
# + 3 columnas

dim(dataset)
```


## fwrite competencia_01_fe123_rank_agui
dataset con clase ternaria y lags y deltas de orden 1 y aguinaldo columnas normalizadas
```{r chunk23}

# fwrite(dataset,
#        file = "/content/buckets/b1/datasets/competencia_01_fe1yag.csv.gz",
#        sep = ",")
# 
# Rstudio
setwd(dir_experimento)
getwd()

fwrite(dataset,
       file = "competencia_01_fe123_rank_agui.csv.gz",
       sep = ",")

```

# OPTIMIZACION Hiperparámetros

limpio el ambiente de R

```{r chunk28}
format(Sys.time(), "%a %b %d %X %Y")
```

```{r chunk29}
# limpio la memoria
rm(list=ls(all.names=TRUE)) # remove all objects
gc(full=TRUE, verbose=FALSE) # garbage collection
```

##Carga de Librerias

```{r chunk30}
# cargo las librerias que necesito
require("data.table")
require("parallel")

if(!require("R.utils")) install.packages("R.utils")
require("R.utils")

if( !require("primes") ) install.packages("primes")
require("primes")

if( !require("utils") ) install.packages("utils")
require("utils")

if( !require("rlist") ) install.packages("rlist")
require("rlist")

if( !require("yaml")) install.packages("yaml")
require("yaml")

if( !require("lightgbm") ) install.packages("lightgbm")
require("lightgbm")

if( !require("DiceKriging") ) install.packages("DiceKriging")
require("DiceKriging")

if( !require("mlrMBO") ) install.packages("mlrMBO")
require("mlrMBO")
```

# nro EXP y seeds

```{r chunk31}
PARAM <- list()
PARAM$experimento <- 9016

PARAM$semilla_primigenia <- 999199
PARAM$semillas <- c(999199, 999499, 999599, 999959, 999979
#                    , 104729, 523987
#                    , 7919,1299709, 2097593
)

```

```{r chunk32}
# # training y future
# PARAM$train <- c(202102)
# PARAM$train_final <- c(202102)
# PARAM$future <- c(202104)
# PARAM$semilla_kaggle <- 314159
# PARAM$cortes <- seq(6000, 19000, by= 500)
```

# períodos
```{r chunk33}
# training y future
PARAM$train <- c(202101, 202102, 202103)
PARAM$train_final <- c(202102, 202101, 202103)
PARAM$future <- c(202106)
PARAM$false_future <- c(202104)
PARAM$semilla_kaggle <- 314159
PARAM$cortes <- seq(6000, 18000, by= 500)
```

# undersampling
```{r chunk34}
# un undersampling de 0.1  toma solo el 10% de los CONTINUA
# undersampling de 1.0  implica tomar TODOS los datos

PARAM$trainingstrategy$undersampling <- 0.2
```

# HIPERPARAMETROS
```{r chunk35}
# Parametros LightGBM

PARAM$hyperparametertuning$xval_folds <- 5

# parametros fijos del LightGBM que se pisaran con la parte variable de la BO
PARAM$lgbm$param_fijos <-  list(
  boosting= "gbdt", # puede ir  dart  , ni pruebe random_forest
  # boosting= "dart", #ATENCION MODIFIQUE
  objective= "binary",
  metric= "auc",
  first_metric_only= FALSE,
  boost_from_average= TRUE,
  feature_pre_filter= FALSE,
  force_row_wise= TRUE, # para reducir warnings
  verbosity= -100,

  seed= PARAM$semilla_primigenia,

  max_depth= -1L, # -1 significa no limitar,  por ahora lo dejo fijo
  min_gain_to_split= 0, # min_gain_to_split >= 0
  min_sum_hessian_in_leaf= 0.001, #  min_sum_hessian_in_leaf >= 0.0
  lambda_l1= 0.0, # lambda_l1 >= 0.0
  lambda_l2= 0.0, # lambda_l2 >= 0.0
  max_bin= 31L, # lo debo dejar fijo, no participa de la BO

  bagging_fraction= 1.0, # 0.0 < bagging_fraction <= 1.0
  pos_bagging_fraction= 1.0, # 0.0 < pos_bagging_fraction <= 1.0
  neg_bagging_fraction= 1.0, # 0.0 < neg_bagging_fraction <= 1.0
  is_unbalance= FALSE, #
  scale_pos_weight= 1.0, # scale_pos_weight > 0.0

  drop_rate= 0.1, # 0.0 < neg_bagging_fraction <= 1.0
  max_drop= 50, # <=0 means no limit
  skip_drop= 0.5, # 0.0 <= skip_drop <= 1.0

  extra_trees= FALSE,

  num_iterations= 1200,
  learning_rate= 0.02,
  feature_fraction= 0.5,
  num_leaves= 750,
  #min_data_in_leaf= 5000,
  min_data_in_leaf= 3 # TOCADO!!!!
)
```

Aqui se definen los hiperparámetros de LightGBM que participan de la Bayesian Optimization
<br> si es un numero entero debe ir  makeIntegerParam
<br> si es un numero real (con decimales) debe ir  makeNumericParam
<br> es muy importante leer cuales son un lower y upper  permitidos y ademas razonables

```{r chunk36}
# # Aqui se cargan los bordes de los hiperparametros de la BO
# # INICIAL EXP HT4940

# PARAM$hyperparametertuning$hs <- makeParamSet(
#   makeIntegerParam("num_iterations", lower= 8L, upper= 2048L),
#   makeNumericParam("learning_rate", lower= 0.01, upper= 0.3),
#   makeNumericParam("feature_fraction", lower= 0.1, upper= 1.0),
#   makeIntegerParam("num_leaves", lower= 8L, upper= 2048L),
#   makeIntegerParam("min_data_in_leaf", lower= 1L, upper= 8000L)
# )
```

```{r chunk37}
# # EXP 9000

# PARAM$hyperparametertuning$hs <- makeParamSet(
#   makeIntegerParam("num_iterations", lower= 250L, upper= 5000L),
#   # o más con early stopping al entrenar
#   makeNumericParam("learning_rate", lower= 0.005, upper= 0.2),
#   makeNumericParam("feature_fraction", lower= 0.2, upper= 0.99),
#   makeIntegerParam("num_leaves", lower= 64L, upper= 1000L),
#   makeIntegerParam("min_data_in_leaf", lower= 50L, upper= 3000L),

#   makeIntegerParam("max_depth", lower = 3, upper= 26),
#   makeNumericParam("min_gain_to_split", lower = 0.0, upper = 0.4),
#   #makeNumericParam("min_sum_hessian_in_leaf", lower = 0.001, upper = 0.1),
#   makeNumericParam("lambda_l1", lower = 0.0, upper = 5.0),
#   makeNumericParam("lambda_l2", lower = 0.0, upper = 10.0),
#   #max_bin= 31L, # lo debo dejar fijo, no participa de la BO

#   makeNumericParam("bagging_fraction", lower = 0.2, upper = 0.99),
#   makeIntegerParam("bagging_freq", lower = 1L, upper = 7L) #1 para que ocurra
#   #makeNumericParam("pos_bagging_fraction", lower = 0.5, upper = 1.0),
#   #makeNumericParam("neg_bagging_fraction", lower = 0.5, upper = 1.0),
#   #makeNumericParam("scale_pos_weight", lower = 0.5, upper = 1.0),

# #son de dart
#   # makeNumericParam("drop_rate", lower = 0.0, upper = 0.3),
#   # makeIntegerParam("max_drop", lower = 0L, upper = 50L),
#   # makeNumericParam("skip_drop", lower = 0.1, upper = 0.5)
# )
```

```{r chunk38}
# EXP 9015

# PARAM$hyperparametertuning$hs <- makeParamSet(
#   makeIntegerParam("num_iterations", lower= 900L, upper= 3100L), #acorté
#   # o más con early stopping al entrenar
#   makeNumericParam("learning_rate", lower= 0.009, upper= 0.2), #agrandé
#   makeNumericParam("feature_fraction", lower= 0.2, upper= 0.8), #acorté
#   makeIntegerParam("num_leaves", lower= 64L, upper= 1000L),
#   #makeIntegerParam("min_data_in_leaf", lower= 2L, upper= 5L), fijo en 3
# 
#   makeIntegerParam("max_depth", lower = 3, upper= 26),
#   makeNumericParam("min_gain_to_split", lower = 0.0, upper = 0.4),
#   #makeNumericParam("min_sum_hessian_in_leaf", lower = 0.001, upper = 0.1),
#   makeNumericParam("lambda_l1", lower = 0.0, upper = 5.0),
#   makeNumericParam("lambda_l2", lower = 0.0, upper = 10.0),
# 
#   makeNumericParam("bagging_fraction", lower = 0.2, upper = 0.99),
#   makeIntegerParam("bagging_freq", lower = 1L, upper = 7L) #1 para que ocurra
#   #makeNumericParam("pos_bagging_fraction", lower = 0.5, upper = 1.0),
#   #makeNumericParam("neg_bagging_fraction", lower = 0.5, upper = 1.0),
#   #makeNumericParam("scale_pos_weight", lower = 0.5, upper = 1.0),
# )
```


```{r nuevos}
# EXP 9006
# 
# PARAM$hyperparametertuning$hs <- makeParamSet(
#   makeIntegerParam("num_iterations", lower= 250L, upper= 5000L),
#   makeNumericParam("learning_rate", lower= 0.005, upper= 0.1),
#   makeNumericParam("feature_fraction", lower= 0.2, upper= 0.99),
#   makeIntegerParam("num_leaves", lower= 64L, upper= 1000L),
#   makeIntegerParam("min_data_in_leaf", lower= 2L, upper= 5L)
# )
```


```{r chunk39}
# EXP 9002

# PARAM$hyperparametertuning$hs <- makeParamSet(
#   makeIntegerParam("num_iterations", lower= 250L, upper= 5000L),
#   # o más con early stopping al entrenar
#   makeNumericParam("learning_rate", lower= 0.005, upper= 0.1),
#   makeNumericParam("feature_fraction", lower= 0.2, upper= 0.99),
#   makeIntegerParam("num_leaves", lower= 64L, upper= 1000L),
#   #makeIntegerParam("min_data_in_leaf", lower= 2L, upper= 5L), APAGADO
# 
#   makeIntegerParam("max_depth", lower = 3, upper= 26),
#   makeNumericParam("min_gain_to_split", lower = 0.0, upper = 0.4),
#   #makeNumericParam("min_sum_hessian_in_leaf", lower = 0.001, upper = 0.1),
#   makeNumericParam("lambda_l1", lower = 0.0, upper = 5.0),
#   makeNumericParam("lambda_l2", lower = 0.0, upper = 10.0),
#   #max_bin= 31L, # lo debo dejar fijo, no participa de la BO
# 
#   makeNumericParam("bagging_fraction", lower = 0.2, upper = 0.99),
#   makeIntegerParam("bagging_freq", lower = 1L, upper = 7L) #1 para que ocurra
#   #makeNumericParam("pos_bagging_fraction", lower = 0.5, upper = 1.0),
#   #makeNumericParam("neg_bagging_fraction", lower = 0.5, upper = 1.0),
#   #makeNumericParam("scale_pos_weight", lower = 0.5, upper = 1.0),

#son de dart
  # makeNumericParam("drop_rate", lower = 0.0, upper = 0.3),
  # makeIntegerParam("max_drop", lower = 0L, upper = 50L),
  # makeNumericParam("skip_drop", lower = 0.1, upper = 0.5)
)
```

```{r chunk40}
# # EXP HT 4942

# PARAM$hyperparametertuning$hs <- makeParamSet(
#   makeIntegerParam("num_iterations", lower= 100L, upper= 5000L),
#   # o más con early stopping al entrenar
#   makeNumericParam("learning_rate", lower= 0.01, upper= 0.1),
#   makeNumericParam("feature_fraction", lower= 0.3, upper= 1.0),
#   makeIntegerParam("num_leaves", lower= 64L, upper= 2000),
#   makeIntegerParam("min_data_in_leaf", lower= 50L, upper= 3000L),

#   # makeIntegerParam("max_depth", lower = 3, upper= 30),
#   makeNumericParam("min_gain_to_split", lower = 0.0, upper = 0.4),
#   makeNumericParam("min_sum_hessian_in_leaf", lower = 0.001, upper = 0.1),
#   makeNumericParam("lambda_l1", lower = 0.0, upper = 10.0),
#   makeNumericParam("lambda_l2", lower = 0.0, upper = 10.0),
#   #max_bin= 31L, # lo debo dejar fijo, no participa de la BO

#   makeNumericParam("bagging_fraction", lower = 0.5, upper = 0.999),
#   makeIntegerParam("bagging_freq", lower = 1L, upper = 7L),
#   #makeNumericParam("pos_bagging_fraction", lower = 0.5, upper = 1.0),
#   #makeNumericParam("neg_bagging_fraction", lower = 0.5, upper = 1.0),
#   makeNumericParam("scale_pos_weight", lower = 20.0, upper = 40.0),

# #son de dart
#   makeNumericParam("drop_rate", lower = 0.0, upper = 0.3),
#   makeIntegerParam("max_drop", lower = 0L, upper = 50L),
#   makeNumericParam("skip_drop", lower = 0.1, upper = 0.5)
# )
```

```{r}

# EXP 9016
PARAM$hyperparametertuning$hs <- makeParamSet(
  makeIntegerParam("num_iterations", lower= 800, upper= 4500L),
  # o más con early stopping al entrenar
  makeNumericParam("learning_rate", lower= 0.007, upper= 0.1),
  makeNumericParam("feature_fraction", lower= 0.2, upper= 0.99),
  makeIntegerParam("num_leaves", lower= 64L, upper= 1000L),
  #makeIntegerParam("min_data_in_leaf", lower= 2L, upper= 5L), APAGADO

  makeIntegerParam("max_depth", lower = 3, upper= 26),
  makeNumericParam("min_gain_to_split", lower = 0.0, upper = 0.4),
  #makeNumericParam("min_sum_hessian_in_leaf", lower = 0.001, upper = 0.1),
  makeNumericParam("lambda_l1", lower = 0.0, upper = 5.0),
  makeNumericParam("lambda_l2", lower = 0.0, upper = 10.0),
  #max_bin= 31L, # lo debo dejar fijo, no participa de la BO

  makeNumericParam("bagging_fraction", lower = 0.2, upper = 0.99),
  makeIntegerParam("bagging_freq", lower = 1L, upper = 7L) #1 para que ocurra
  #makeNumericParam("pos_bagging_fraction", lower = 0.5, upper = 1.0),
  #makeNumericParam("neg_bagging_fraction", lower = 0.5, upper = 1.0),
  #makeNumericParam("scale_pos_weight", lower = 0.5, upper = 1.0),

#son de dart
  # makeNumericParam("drop_rate", lower = 0.0, upper = 0.3),
  # makeIntegerParam("max_drop", lower = 0L, upper = 50L),
  # makeNumericParam("skip_drop", lower = 0.1, upper = 0.5)
)
```

A mayor cantidad de hiperparámetros, se debe aumentar las iteraciones de la Bayesian Optimization
<br> 30 es un valor muy tacaño, pero corre rápido
<br> deberia partir de 50, alcanzando los 100 si se dispone de tiempo

# ht iteraciones
```{r chunk41}

PARAM$hyperparametertuning$iteraciones <- 35 # iteraciones bayesianas

```

# carpeta HT

```{r chunk42}

# carpeta de trabajo

setwd("/content/buckets/b1/exp")
experimento_folder <- paste0("HT", PARAM$experimento)
dir.create(experimento_folder, showWarnings=FALSE)
setwd( paste0("/content/buckets/b1/exp/", experimento_folder ))
# 
# 
#  # PARA R
#  # Definir directorio base
# dir_base <- "C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia 1/experimentos"
# dir.create(dir_base, showWarnings = FALSE, recursive = TRUE)
# # # Crear directorio exp
# experimento <- paste0("exp", PARAM$experimento)
# dir_experimento <- file.path(dir_base, experimento)
# dir.create(dir_experimento, showWarnings = FALSE)
# # # Crear directorio HT dentro de exp
# HT <- paste0("HT", PARAM$experimento)
# dir_HT <- file.path(dir_experimento, HT)
# dir.create(dir_HT, showWarnings = FALSE)
# # # Cambiar al directorio HT
# setwd(dir_HT)
# # # Verificar
# cat("Directorio de trabajo:", getwd(), "\n")
```


## carga dataset
```{r chunk43}

dataset <- fread("https://storage.googleapis.com/silvanacontreras76_bukito3/datasets/competencia01_fe123_rank.csv.gz", stringsAsFactors= TRUE)
#1143

```

```{r chunk44}

dim(dataset)
```


### dedataset_train foto mes y clase
```{r chunk47}

dataset_train <- dataset[foto_mes %in% PARAM$train]
```

```{r chunk48}

# paso la clase a binaria que tome valores {0,1}  enteros
#  BAJA+1 y BAJA+2  son  1,   CONTINUA es 0
#  a partir de ahora ya NO puedo cortar  por prob(BAJA+2) > 1/40

dataset_train[,
  clase01 := ifelse(clase_ternaria %in% c("BAJA+2","BAJA+1"), 1L, 0L)
]
```

```{r chunk49}

# defino los datos que forma parte del training
# aqui se hace el undersampling de los CONTINUA
# notar que para esto utilizo la SEGUNDA semilla

set.seed(PARAM$semilla_primigenia, kind = "L'Ecuyer-CMRG")
dataset_train[, azar := runif(nrow(dataset_train))]
dataset_train[, training := 0L]

dataset_train[
  foto_mes %in%  PARAM$train &
    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c("BAJA+1", "BAJA+2")),
  training := 1L
]
```

```{r chunk50}

# los campos que se van a utilizar

campos_buenos <- setdiff(
  colnames(dataset_train),
  c("clase_ternaria", "clase01", "azar", "training")
)
```

##dtrain
```{r chunk51}

# dejo los datos en el formato que necesita LightGBM

dtrain <- lgb.Dataset(
  data= data.matrix(dataset_train[training == 1L, campos_buenos, with= FALSE]),
  label= dataset_train[training == 1L, clase01],
  free_raw_data= FALSE
)

nrow(dtrain)
ncol(dtrain)
```

## Configuracion Bayesian Optimization
```{r chunk52}

# En el argumento x llegan los parámetros de la bayesiana
#  devuelve la AUC en cross validation del modelo entrenado

EstimarGanancia_AUC_lightgbm <- function(x) {

  # x pisa (o agrega) a param_fijos
  param_completo <- modifyList(PARAM$lgbm$param_fijos, x)

  # entreno LightGBM
  modelocv <- lgb.cv(
    data= dtrain,
    nfold= PARAM$hyperparametertuning$xval_folds,
    stratified= TRUE,
    param= param_completo
  )

  # obtengo la ganancia
  AUC <- modelocv$best_score

  # hago espacio en la memoria
  rm(modelocv)
  gc(full= TRUE, verbose= FALSE)

  message(format(Sys.time(), "%a %b %d %X %Y"), " AUC ", AUC)

  return(AUC)
}
```


```{r chunk53}

# Aqui comienza la configuracion de la Bayesian Optimization
# en este archivo quedan la evolucion binaria de la BO

#para VM
kbayesiana <- "bayesiana.RDATA"

#para Rstudio
# kbayesiana <- file.path(dir_experimento, "bayesiana.RDATA")


funcion_optimizar <- EstimarGanancia_AUC_lightgbm # la funcion que voy a maximizar

configureMlr(show.learner.output= FALSE)

# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar
# por favor, no desesperarse por lo complejo

obj.fun <- makeSingleObjectiveFunction(
  fn= funcion_optimizar, # la funcion que voy a maximizar
  minimize= FALSE, # estoy Maximizando la ganancia
  noisy= TRUE,
  par.set= PARAM$hyperparametertuning$hs, # definido al comienzo del programa
  has.simple.signature= FALSE # paso los parametros en una lista
)

# cada 600 segundos guardo el resultado intermedio
ctrl <- makeMBOControl(
  save.on.disk.at.time= 600, # se graba cada 600 segundos
  save.file.path= kbayesiana
) # se graba cada 600 segundos

# indico la cantidad de iteraciones que va a tener la Bayesian Optimization
ctrl <- setMBOControlTermination(
  ctrl,
  iters= PARAM$hyperparametertuning$iteraciones
) # cantidad de iteraciones

# defino el método estandar para la creacion de los puntos iniciales,
# los "No Inteligentes"
ctrl <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI())

# establezco la funcion que busca el maximo
surr.km <- makeLearner(
  "regr.km",
  predict.type= "se",
  covtype= "matern3_2",
  control= list(trace= TRUE)
)
```

## Corrida Bayesian Optimization
```{r chunk54}

 # inicio la optimizacion bayesiana, retomando si ya existe
# es la celda mas lenta de todo el notebook

if (!file.exists(kbayesiana)) {
  bayesiana_salida <- mbo(obj.fun, learner= surr.km, control= ctrl)
} else {
  bayesiana_salida <- mboContinue(kbayesiana) # retomo en caso que ya exista
}
```

```{r chunk55}

print(bayesiana_salida$x)  # Mejores hiperparámetros
print(bayesiana_salida$y)  # Mejor AUC
```

## levanto proceso hasta donde llegó (si lo corté)
```{r chunk56}

# # levantar proceso interrumpido sin continuar

# load(kbayesiana)  # carga 'opt.state' en el entorno

# # armo un contenedor con el mismo campo que usaba
# bayesiana_salida <- list(opt.path = opt.state$opt.path)

# print(bayesiana_salida$x)  # Mejores hiperparámetros
# print(bayesiana_salida$y)  # Mejor AUC
```

## bayesiana_salida guarda
```{r chunk57}

tb_bayesiana <- as.data.table(bayesiana_salida$opt.path)
colnames( tb_bayesiana)

```

```{r chunk58}

# almaceno los resultados de la Bayesian Optimization
# y capturo los mejores hiperparametros encontrados

tb_bayesiana <- as.data.table(bayesiana_salida$opt.path) #(ya está en la celda anterior)

tb_bayesiana[, iter := .I]

# ordeno en forma descendente por AUC = y
setorder(tb_bayesiana, -y)

# grabo para eventualmente poder utilizarlos en OTRA corrida

fwrite( tb_bayesiana,
  file= "BO_log.txt",
  sep= "\t"
)
# 
# #grabo para Rstudio
# ruta_bo <- file.path(dir_experimento, "BO_log.txt")
# fwrite(tb_bayesiana,
#   file = ruta_bo,
#   sep = "\t"
# )

# los mejores hiperparámetros son los que quedaron en el registro 1 de la tabla
PARAM$out$lgbm$mejores_hiperparametros <- tb_bayesiana[
  1, # el primero es el de mejor AUC
  setdiff(colnames(tb_bayesiana),
    c("y","dob","eol","error.message","exec.time","ei","error.model",
      "train.time","prop.type","propose.time","se","mean","iter")),
  with= FALSE
]


PARAM$out$lgbm$y <- tb_bayesiana[1, y]
```

```{r chunk59}

write_yaml( PARAM, file="PARAM.yml")
```

```{r chunk60}

print(PARAM$out$lgbm$mejores_hiperparametros)
print(PARAM$out$lgbm$y)
```

# PRODUCCION
Construyo el modelo final, que es uno solo, no hace ningun tipo de particion < training, validation, testing>]

## carpeta EXP 
```{r chunk64}

setwd("/content/buckets/b1/exp")
experimento <- paste0("exp", PARAM$experimento)
dir.create(experimento, showWarnings= FALSE)
setwd( paste0("/content/buckets/b1/exp/", experimento ))

# # RStudio
# dir_base <- "C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia 1/experimentos"
# 
# # Crear directorio base si no existe
# dir.create(dir_base, showWarnings = FALSE, recursive = TRUE)
# 
# # Crear directorio del experimento
# experimento <- paste0("exp", PARAM$experimento)
# dir_experimento <- file.path(dir_base, experimento)
# dir.create(dir_experimento, showWarnings = FALSE)
# 
# # Cambiar al directorio del experimento
# setwd(dir_experimento)
# 
# # Verificar
# cat("Directorio de trabajo:", getwd(), "\n")
```

## Final Training Dataset

Aqui esta la gran decision de en qué meses hago el Final Training
<br> debo utilizar los mejores hiperparámetros que encontré en la  optimización bayesiana
```{r chunk65}


dataset <- fread("https://storage.googleapis.com/silvanacontreras76_bukito3/datasets/competencia_01_fe123_rank_agui.csv.gz", stringsAsFactors= TRUE)


```


```{r}
cat("VERIFICACIÓN POST-CARGA\n\n")

#columnas 459 (fe1)
#columnas 462 (fe1yag)

cat("Dimensiones totales:", dim(dataset), "\n\n")

cat("Distribución por foto_mes:\n")
print(dataset[, .N, by = foto_mes][order(foto_mes)])

cat("\nPERÍODO 202106\n")
cat("Filas en 202106:", nrow(dataset[foto_mes == 202106]), "\n")
cat("Esperado: 164313\n")

if(nrow(dataset[foto_mes == 202106]) != 164313) {
  cat("\nPROBLEMA AL CARGAR\n")
  cat("Diferencia:", 164313 - nrow(dataset[foto_mes == 202106]), "registros faltantes\n")
  cat("El archivo competencia_01_fe.csv ya tiene el problema\n")
  cat("Revisar cómo se generó el feature engineering\n")
} else {
  cat("\nCarga correcta - 202106 OK\n")
}
```

```{r chunk67}
dim(dataset) #1143
```
### períodos
```{r chunk33}

PARAM$train_final <- c(202102, 202101, 202103)
PARAM$future <- c(202106)
PARAM$false_future <- c(202104)
PARAM$semilla_kaggle <- 314159
PARAM$cortes <- seq(6000, 18000, by= 500)
```

### undersampling
```{r chunk34}
# un undersampling de 0.1  toma solo el 10% de los CONTINUA
# undersampling de 1.0  implica tomar TODOS los datos

PARAM$trainingstrategy$undersampling <- 0.2

```

## clase01
```{r chunk68}
# clase01
dataset[, clase01 := ifelse(clase_ternaria %in% c("BAJA+1", "BAJA+2"), 1L, 0L)]
```

```{r}
dataset[, unique(foto_mes)]
print(PARAM$train_final)

```

##dataset_train final
```{r chunk69}

dataset_train <- dataset[foto_mes %in% PARAM$train_final]
dataset_train[,.N,clase_ternaria]

```

```{r chunk70}
# los campos que se van a utilizar

campos_buenos <- setdiff(
  colnames(dataset_train),
  c("clase_ternaria", "clase01", "azar", "training")
)
```

##dtrain_final
```{r chunk71}
# dejo los datos en el formato que necesita LightGBM

dtrain_final <- lgb.Dataset(
  data= data.matrix(dataset_train[, campos_buenos, with= FALSE]),
  label= dataset_train[, clase01]
)
```

##Final Training Hyperparameters
```{r chunk72}
# Parametros LightGBM

PARAM$hyperparametertuning$xval_folds <- 5

# parametros fijos del LightGBM que se pisaran con la parte variable de la BO
PARAM$lgbm$param_fijos <-  list(
  boosting= "gbdt", # puede ir  dart  , ni pruebe random_forest
  # boosting= "dart", #ATENCION MODIFIQUE
  objective= "binary",
  metric= "auc",
  first_metric_only= FALSE,
  boost_from_average= TRUE,
  feature_pre_filter= FALSE,
  force_row_wise= TRUE, # para reducir warnings
  verbosity= -100,

  seed= PARAM$semilla_primigenia,

  max_depth= -1L, # -1 significa no limitar,  por ahora lo dejo fijo
  min_gain_to_split= 0, # min_gain_to_split >= 0
  min_sum_hessian_in_leaf= 0.001, #  min_sum_hessian_in_leaf >= 0.0
  lambda_l1= 0.0, # lambda_l1 >= 0.0
  lambda_l2= 0.0, # lambda_l2 >= 0.0
  max_bin= 31L, # lo debo dejar fijo, no participa de la BO

  bagging_fraction= 1.0, # 0.0 < bagging_fraction <= 1.0
  pos_bagging_fraction= 1.0, # 0.0 < pos_bagging_fraction <= 1.0
  neg_bagging_fraction= 1.0, # 0.0 < neg_bagging_fraction <= 1.0
  is_unbalance= FALSE, #
  scale_pos_weight= 1.0, # scale_pos_weight > 0.0

  drop_rate= 0.1, # 0.0 < neg_bagging_fraction <= 1.0
  max_drop= 50, # <=0 means no limit
  skip_drop= 0.5, # 0.0 <= skip_drop <= 1.0

  extra_trees= FALSE,

  num_iterations= 1200,
  learning_rate= 0.02,
  feature_fraction= 0.5,
  num_leaves= 750,
  min_data_in_leaf= 5000
)
```


```{r chunk61}
# EXP HT 4940
# Envios=9000	 TOTAL=353600000  Public=335333333 Private=361428571

# EXP 4940

# mejores_hiperparametros_ht4940 <- list(
#   num_iterations = 1085,
#   learning_rate = 0.0100625,
#   feature_fraction = 0.5160196,
#   num_leaves = 1838,
#   min_data_in_leaf = 1309
# )

# param_final <- modifyList(PARAM$lgbm$param_fijos, mejores_hiperparametros_ht4940)

```

```{r chunk62}
# EXP HT 4941

# mejores_hiperparametros:
#       num_iterations: 962
#       learning_rate: 0.0144182
#       feature_fraction: 0.6585625
#       num_leaves: 365
#       min_data_in_leaf: 726
#       max_depth: 23
#       min_gain_to_split: 0.0570174
#       lambda_l1: 3.9986413
#       lambda_l2: 1.7045828
```

```{r chunk63}
# EXP HT 4942

# num_iterations 4593
# learning_rate 0.0427741
# feature_fraction 0.3755552
# num_leaves 1910
# min_data_in_leaf 2195
# min_gain_to_split 0.08282231
# min_sum_hessian_in_leaf 0.08637694
# lambda_l1 0.8836508
# lambda_l2 4.757157
# bagging_fraction 0.9806646
# bagging_freq 1
# scale_pos_weight 34.63562
# drop_rate 0.08215384
# max_drop 1
# skip_drop 0.3466582
# AUC 0.925877
```


```{r chunk73}

# # 9002 (fe123) bayesiana frenada en #30. AUC =  0.9441 

# PARAM$out$lgbm$mejores_hiperparametros <- list(
#   num_iterations = 4710,
#   learning_rate = 0.00775,
#   feature_fraction = 0.853,
#   num_leaves = 264,
#   min_data_in_leaf = 3,
#   max_depth = 15,
#   min_gain_to_split = 6.27e-05,
#   lambda_l1 = 0.0249,
#   lambda_l2 = 1.73,
#   bagging_fraction = 0.809,
#   bagging_freq = 2
# )
```

```{r}
# 9002 bayesiana completa #80. almost best AUC = 0.9448

# PARAM$out$lgbm$mejores_hiperparametros <- list(
#    num_iterations = 2757,
#    learning_rate = 0.0126,
#    feature_fraction = 0.758,
#    num_leaves = 282,
#    min_data_in_leaf = 3,
#    max_depth = 16,
#    min_gain_to_split = 0.00226,
#    lambda_l1 = 0.0381,
#    lambda_l2 = 0.000909,
#    bagging_fraction = 0.69,
#    bagging_freq = 1
# )

```

```{r}

# 9002 bayesiana completa #80. BEST AUC = 0.9450

# PARAM$out$lgbm$mejores_hiperparametros <- list(
#    num_iterations = 3910,                  ### ATENCION!!!!
#    learning_rate = 0.006639251,
#    feature_fraction = 0.7971601,
#    num_leaves = 248,
#    min_data_in_leaf = 4,
#    max_depth = 16,
#    min_gain_to_split = 0.01307809,
#    lambda_l1 = 0.02541107,
#    lambda_l2 = 0.02196564,
#    bagging_fraction = 0.6892582,
#    bagging_freq = 4
# )

```


```{r}
# EXP HT 9006 (en colab). es FE1. AUC = 0.9472
# 
# PARAM$out$lgbm$mejores_hiperparametros <- list(
#   num_iterations = 2610,
#   learning_rate = 0.0219,
#   feature_fraction = 0.61,
#   num_leaves = 455,
#   min_data_in_leaf = 4
# )
  
```

```{r}

# EXP 9012 (1,2,3, ranking y agui) provisorio hasta iter #36
# AUC= 0,9447

# PARAM$out$lgbm$mejores_hiperparametros <- list(
#    num_iterations = 3158,
#    learning_rate = 0.0171,
#    feature_fraction = 0.904,
#    num_leaves = 372,
#    min_data_in_leaf = 3,  # Asumido, ya que no se especificó en el último set
#    max_depth = 12,
#    min_gain_to_split = 0.000113,
#    lambda_l1 = 0.015,
#    lambda_l2 = 7.11,
#    bagging_fraction = 0.949,
#    bagging_freq = 4
# )

```

```{r chunk74}
#EXP 9015

```

```{r chunk75}
param_final <- modifyList(PARAM$lgbm$param_fijos,
  PARAM$out$lgbm$mejores_hiperparametros)

param_final
```

## semillas para ensamble
```{r chunk76}
#Solo si uso ensamble creo que no sería necesario luego de haber corregido el codigo. revisar

param_final$seed <- PARAM$semillas
```

### escalo por undersampling

```{r chunk77}
# este punto es muy SUTIL  y será revisado en la Clase 05
# acá retoma esto PARAM$trainingstrategy$undersampling ojo no haber borrado

param_normalizado <- copy(param_final)
param_normalizado$min_data_in_leaf <-  round(param_final$min_data_in_leaf / PARAM$trainingstrategy$undersampling)
```

### escalo para 202104
```{r}

# # Ajuste adicional por cambio en cantidad de registros reales
# registros_optimizacion <- dataset[foto_mes %in% c(202101, 202102, 202103), .N]
# registros_training <- dataset[foto_mes %in% c(202101, 202102, 202103, 202104), .N]
# factor_registros <- registros_training / registros_optimizacion
# 
# param_normalizado$min_data_in_leaf <- round(param_normalizado$min_data_in_leaf * factor_registros)

```

```{r chunk78}
param_normalizado
```
# verifico dtrain_final

```{r}

cat("1. DIMENSIONES:\n")
cat("   Filas:", nrow(dtrain_final), "\n")
cat("   Columnas:", ncol(dtrain_final), "\n\n")

cat("2. PERÍODOS DE ENTRENAMIENTO:\n")
cat("   Configurados:", paste(PARAM$train_final, collapse = ", "), "\n")
dataset_train_check <- dataset[foto_mes %in% PARAM$train_final]
cat("   Filas en dataset para esos períodos:", nrow(dataset_train_check), "\n")
cat("   ¿Coincide con dtrain_final?", nrow(dataset_train_check) == nrow(dtrain_final), "\n\n")

cat("3. DISTRIBUCIÓN POR PERÍODO:\n")
print(dataset_train_check[, .N, by = foto_mes])
cat("\n")

cat("4. BALANCE DE CLASES:\n")
dataset_train_check[, clase01 := ifelse(clase_ternaria %in% c("BAJA+1", "BAJA+2"), 1L, 0L)]
print(dataset_train_check[, .N, by = clase01])
cat("   Proporción positivos:", 
    round(sum(dataset_train_check$clase01) / nrow(dataset_train_check) * 100, 2), "%\n\n")

cat("6. FUTURO A PREDECIR:\n")
dfuture <- dataset[foto_mes == 202106]
cat("   Período:", PARAM$future, "\n")
cat("   Filas en 202106:", nrow(dfuture), "\n")
cat("   ESPERADO: 164313\n")
cat("   DIFERENCIA:", 164313 - nrow(dfuture), "\n\n")

```

# TRAINING
Genero el modelo final, siempre sobre TODOS los datos de  final_train, sin hacer ningun tipo de undersampling de la clase mayoritaria y mucho menos cross validation.
```{r chunk79}
# # entreno LightGBM

# modelo_final <- lgb.train(
#   data= dtrain_final,
#   param= param_normalizado
# )
```

```{r}
# ENTRENO RENOVADO CON DIRECTORIO Y GUARDANDO DE A 1

# setwd( paste0("/content/buckets/b1/exp/", experimento ))
# setwd(dir_experimento)

# Entreno ensamble mis semillas
cat("Entrenando", length(PARAM$semillas), "modelos con diferentes semillas...\n")

# Lista para guardar los modelos
modelos_ensemble <- list()

# Entrenar un modelo por cada semilla
for(i in 1:length(PARAM$semillas)) {
  cat("\n=== Entrenando modelo", i, "de", length(PARAM$semillas), "con semilla", PARAM$semillas[i], "===\n")
  
  set.seed(PARAM$semillas[i])
  param_normalizado$seed <- PARAM$semillas[i]
  
  # Entrenar modelo
  modelo <- lgb.train(
    data = dtrain_final,
    param = param_normalizado
  )
  
  # Guardar modelo INMEDIATAMENTE
  nombre_archivo <- paste0("modelo_seed_", PARAM$semillas[i], ".txt")
  lgb.save(modelo, nombre_archivo)
  cat("Modelo", i, "guardado como:", nombre_archivo, "\n")
  
  # Guardar en lista
  modelos_ensemble[[i]] <- modelo
  
  cat("Modelo", i, "entrenado y guardado exitosamente\n")
}

cat("\n=== Ensamble de", length(PARAM$semillas), "modelos completado ===\n")


```

####retomo train si murió
```{r}
# # REANUDACIÓN DEL ENTRENAMIENTO DE SEMILLAS FALTANTES
# 
# # Asegurarse de estar en el directorio correcto
# setwd(dir_experimento)
# 
# # 1. IDENTIFICAR SEMILLAS YA ENTRENADAS
# # Obtener los nombres de los archivos de modelos ya guardados
# archivos_modelos <- list.files(pattern = "^modelo_seed_.*\\.txt$")
# 
# # Extraer las semillas de los nombres de archivo.
# # Esto convierte, por ejemplo, "modelo_seed_271828.txt" a "271828" y luego a un número.
# semillas_entrenadas <- as.numeric(gsub("modelo_seed_(.*)\\.txt", "\\1", archivos_modelos))
# 
# # 2. ENCONTRAR SEMILLAS FALTANTES
# # Usar setdiff para encontrar las semillas que están en PARAM$semillas pero NO en semillas_entrenadas
# semillas_faltantes <- setdiff(PARAM$semillas, semillas_entrenadas)
# 
# cat("Total de semillas a entrenar (original):", length(PARAM$semillas), "\n")
# cat("Semillas ya entrenadas:", length(semillas_entrenadas), "\n")
# cat("Semillas faltantes por entrenar:", length(semillas_faltantes), "\n")
# if (length(semillas_faltantes) > 0) {
#     cat("Semillas faltantes:", paste(semillas_faltantes, collapse = ", "), "\n")
# } else {
#     cat("¡Todo el ensamble ya fue entrenado y guardado!\n")
# }
# 
# 
# # 3. ENTRENAR SOLO LAS SEMILLAS FALTANTES
# 
# # Crear una lista auxiliar para el bucle de reanudación
# semillas_a_procesar <- semillas_faltantes
# 
# # Entrenar un modelo solo por cada semilla faltante
# for(semilla_actual in semillas_a_procesar) {
#     cat("\n=== Retomando entrenamiento con semilla faltante:", semilla_actual, "===\n")
# 
#     set.seed(semilla_actual)
#     param_normalizado$seed <- semilla_actual
# 
#     # Entrenar modelo (usando la semilla actual)
#     modelo <- lgb.train(
#         data = dtrain_final,
#         param = param_normalizado
#     )
# 
#     # Guardar modelo INMEDIATAMENTE
#     nombre_archivo <- paste0("modelo_seed_", semilla_actual, ".txt")
#     lgb.save(modelo, nombre_archivo)
#     cat("Modelo guardado como:", nombre_archivo, "\n")
# 
#     # Nota: No lo guardamos en 'modelos_ensemble' porque esa lista suele ser para
#     # usar en memoria. Aquí nos enfocamos solo en guardar el archivo faltante.
# 
#     cat("Modelo entrenado y guardado exitosamente\n")
# }
# 
# cat("\n=== Proceso de reanudación completado ===\n")
```


## importancia variables
```{r chunk82}
# ahora imprimo la importancia de variables

# tb_importancia <- as.data.table(lgb.importance(modelo_final))

tb_importancia <- as.data.table(lgb.importance(modelos_ensemble[[1]]))


# # para VM
# setwd(dir_experimento)

fwrite(tb_importancia,
  file = "impo.txt",
  sep = "\t"
)


```

```{r chunk83}
library(ggplot2)
library(scales)

tb_importancia_top <- head(tb_importancia[order(-Gain)], 20)

p <- ggplot(tb_importancia_top, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "#2C7BB6", alpha = 0.8) +
  geom_text(aes(label = round(Gain, 0)), hjust = -0.1, size = 3, color = "gray30") +
  coord_flip() +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.1))) +
  labs(
    title = paste0("Top 20 Feature Importance - EXP ", PARAM$experimento),
    subtitle = paste0("Modelo LightGBM - ", length(PARAM$semillas), " semillas"),
    x = NULL,
    y = "Gain",
    caption = paste0("Total features: ", nrow(tb_importancia))
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12, color = "gray40"),
    plot.caption = element_text(size = 9, color = "gray50"),
    axis.text = element_text(size = 10),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank()
  )

# Mostrar gráfico
print(p)

# para VM

# setwd(dir_experimento)

nombre_archivo <- paste0("feature_importance_ggplot_exp_", PARAM$experimento, ".png")
ggsave(nombre_archivo, p, width = 12, height = 8, dpi = 300)


```

```{r chunk84}
# Gráfico nativo de lightgbm

lgb.plot.importance(tb_importancia, top_n = 20, measure = "Gain")
title(main = paste0("Feature Importance - EXP ", PARAM$experimento), line = 0.5)

#guardar
# setwd(dir_experimento)

png(paste0("feature_importance_lgb_exp_", PARAM$experimento, ".png"), width = 800, height = 600)
lgb.plot.importance(tb_importancia, top_n = 20, measure = "Gain")
title(main = paste0("Feature Importance - EXP ", PARAM$experimento), line = 0.5)
dev.off()


```

## Scoring
Aplico el modelo final a los datos del futuro
```{r chunk85}
# # aplico el modelo a los datos sin clase

# dfuture <- dataset[foto_mes %in% PARAM$future]

# # aplico el modelo a los datos nuevos
# prediccion <- predict(
#   modelo_final,
#   data.matrix(dfuture[, campos_buenos, with= FALSE])
# )
```

###cargo modelos desde otro lado
```{r}
# # Cargar modelos guardados desde el directorio de experimento
# dir_experimento <- "C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia 1/experimentos/exp9012"
# modelos_ensemble <- list()
# 
# cat("\n=== Cargando modelos del ensamble ===\n")
# 
# for(i in 1:length(PARAM$semillas)) {
#   # Construir el nombre del archivo según tu nomenclatura
#   archivo_modelo <- file.path(dir_experimento, paste0("modelo_seed_", PARAM$semillas[i], ".txt"))
#   
#   # Verificar si el archivo existe
#   if(file.exists(archivo_modelo)) {
#     cat("Cargando modelo", i, "- Semilla:", PARAM$semillas[i], "\n")
#     
#     # Cargar modelo LightGBM desde archivo .txt
#     modelos_ensemble[[i]] <- lgb.load(archivo_modelo)
#     
#   } else {
#     cat("ADVERTENCIA: No se encontró el archivo:", archivo_modelo, "\n")
#   }
# }
# 
# cat("\nTotal de modelos cargados:", length(modelos_ensemble), "\n")
```

```{r chunk86}
# # aplico el modelo a los datos sin clase

# setwd(dir_experimento)

# Definir dfuture
dfuture <- dataset[foto_mes %in% PARAM$future]

# Aplicar cada modelo y guardar predicciones
predicciones_ensemble <- list()

cat("\n=== Generando predicciones con el ensamble ===\n")

for(i in 1:length(PARAM$semillas)) {

  cat("Prediciendo con modelo", i, "\n")

  prediccion_temp <- predict(
    modelos_ensemble[[i]],
    data.matrix(dfuture[, campos_buenos, with = FALSE])
  )

  predicciones_ensemble[[i]] <- prediccion_temp
}

# Promediar las predicciones
prediccion <- rowMeans(do.call(cbind, predicciones_ensemble))

cat("Predicción final generada como promedio de",
length(PARAM$semillas), "modelos\n")
# length(semillas), "modelos\n")

```

## Tabla Prediccion
```{r chunk87}
tb_prediccion <- dfuture[, list(numero_de_cliente, foto_mes)]
tb_prediccion[, prob := prediccion]

# guardar
# setwd(dir_experimento)

# para VM
fwrite(tb_prediccion,
  file = "prediccion.txt",
  sep = "\t"
)

```

```{r chunk88}
# CONTROL FILAS
# Se deben entregar 164876 predicciones, paquete premium de 202106
# La primer linea del archivo tiene los títulos, con lo cual 164876 + 1 lineas
# error dice que debe tener: Submission must have 164313 rows

filas_prediccion <- nrow(tb_prediccion)
filas202106 <- nrow(dataset[foto_mes == 202106])

print(paste("'tb_prediccion' tiene", filas_prediccion, "filas."))
print(paste("'202106' tiene", filas202106, "filas."))
```

## submissions y envíos
```{r chunk89}
# genero archivos con los  "envios" mejores



setorder(tb_prediccion, -prob)

# setwd(dir_experimento) #para local R

# para VM
dir.create("kaggle", showWarnings = FALSE)

for (envios in PARAM$cortes) {
  tb_prediccion[, Predicted := 0L]
  tb_prediccion[1:envios, Predicted := 1L]

  archivo_kaggle <- paste0("./kaggle/KA", PARAM$experimento, "_", envios, ".csv")

  fwrite(tb_prediccion[, list(numero_de_cliente, Predicted)],
    file = archivo_kaggle,
    sep = ",",
    col.names = TRUE
  )#
  cat("Generado:", basename(archivo_kaggle), "\n")
}

```

# TESTEO
drealidad es el período a predecir con las columnas de interés y fold publico privado.  

Particionar arma este formato  fold = 1  es el 30% del dataset que representaría el "público" de la competencia y fold = 2 es el 70% de ldataset que representaría el "privado" de la competencia? y ambos folds mantienen la representación de los niveles de clase ternaria (continua, baja+1 y baja+2).

 Realidad_inicializar: ejecuta todo esto, arma el kaggle.  

 Realidad evaluar: junta predicciones con realidad en prealidad (cliente, clase ternaria, fold publico o privado y predicción 1 enviar 0 no enviar), calcula ganancias. tbl es tabla resumen de prealidad



```{r chunk90}

# particionar agrega una columna llamada fold a un dataset
#   que consiste en una particion estratificada segun agrupa
# particionar( data=dataset, division=c(70,30),
#  agrupa=clase_ternaria, seed=semilla)   crea una particion 70, 30

particionar <- function(data, division, agrupa= "", campo= "fold", start= 1, seed= NA) {
  if (!is.na(seed)) set.seed(seed, "L'Ecuyer-CMRG")

  bloque <- unlist(mapply(
    function(x, y) {rep(y, x)},division, seq(from= start, length.out= length(division))))

  data[, (campo) := sample(rep(bloque,ceiling(.N / length(bloque))))[1:.N],by= agrupa]
}
```

```{r chunk91}

# iniciliazo el dataset de realidad, para medir ganancia
realidad_inicializar <- function( pfuture, pparam) {

  # datos para verificar la ganancia
  drealidad <- pfuture[, list(numero_de_cliente, foto_mes, clase_ternaria)]

  particionar(drealidad,
    division= c(3, 7),
    agrupa= "clase_ternaria",
    seed= PARAM$semilla_kaggle
  )

  return( drealidad )
}
```

```{r chunk92}

# evaluo ganancia en los datos de la realidad

realidad_evaluar <- function( prealidad, pprediccion) {

  prealidad[ pprediccion,
    on= c("numero_de_cliente", "foto_mes"),
    predicted:= i.Predicted
  ]

  tbl <- prealidad[, list("qty"=.N), list(fold, predicted, clase_ternaria)]

  res <- list()
  res$public  <- tbl[fold==1 & predicted==1L, sum(qty*ifelse(clase_ternaria=="BAJA+2", 780000, -20000))]/0.3
  res$private <- tbl[fold==2 & predicted==1L, sum(qty*ifelse(clase_ternaria=="BAJA+2", 780000, -20000))]/0.7
  res$total <- tbl[predicted==1L, sum(qty*ifelse(clase_ternaria=="BAJA+2", 780000, -20000))]

  prealidad[, predicted:=NULL]
  return( res )
}
```


```{r chunk97}

# Definir dfuture
false_future <- dataset[foto_mes %in% PARAM$false_future]

# Aplicar cada modelo y guardar predicciones
predicciones_ensemble_false_future <- list()

cat("\n=== Generando predicciones para testeo ===\n")

for(i in 1:length(PARAM$semillas)) {
  cat("Prediciendo con modelo para test", i, "\n")

  prediccion_temp_false_future <- predict(
    modelos_ensemble[[i]],
    data.matrix(false_future[, campos_buenos, with = FALSE])
  )

  predicciones_ensemble_false_future[[i]] <- prediccion_temp_false_future
}

# Promediar las predicciones
prediccion_false_future <- rowMeans(do.call(cbind, predicciones_ensemble_false_future))

cat("Predicción final false_future generada como promedio de",
length(PARAM$semillas), "modelos \n")
# length(semillas), "modelos\n")

```

```{r chunk98}

tb_prediccion_false_future <- false_future[, list(numero_de_cliente, foto_mes)]
tb_prediccion_false_future[, prob := prediccion_false_future]

# setwd(dir_experimento)
# para VM
fwrite(tb_prediccion_false_future,
  file = "prediccion_false_future.txt",
  sep = "\t"
)


```

```{r chunk99}

# inicilizo el dataset  drealidad SOLO PARA BACKTESTING

drealidad <- realidad_inicializar( false_future, PARAM)
```


```{r chunk100}

PARAM$cortes
```

```{r}

# setwd(dir_experimento)  #PARA R LOCAL

# Crear/abrir archivo para guardar
test_ganancia_envios <- paste0("test_ganancia_envios_", PARAM$experimento, ".txt")
sink(test_ganancia_envios, split = TRUE)  # split = TRUE imprime en pantalla Y guarda

cat("EXPERIMENTO:", PARAM$experimento, "\n")
cat("Período Train Final:", paste(PARAM$train_final, collapse = ", "), "\n")
cat("Período False Future:", paste(PARAM$false_future, collapse = ", "), "\n")
cat("Cortes:", paste(range(PARAM$cortes), collapse = " a "), 
    " (step=", unique(diff(PARAM$cortes)), ")\n", sep = "")

setorder(tb_prediccion_false_future, -prob)
for (envios in PARAM$cortes) {
  tb_prediccion_false_future[, Predicted := 0L]
  tb_prediccion_false_future[1:envios, Predicted := 1L]
  res <- realidad_evaluar(drealidad, tb_prediccion_false_future)
  options(scipen = 999)
  cat("Envios=", envios, "\t",
      " TOTAL=", res$total,
      "  Public=", res$public,
      " Private=", res$private,
      "\n",
      sep = ""
  )
}

# Cerrar archivo
sink()
cat("\n✓ Resultados guardados en:", test_ganancia_envios, "\n")


```

```{r chunk102}

# Guardar resultados de ganancia por cada corte (solo para graficar)
resultados_ganancia <- data.table()

setorder(tb_prediccion_false_future, -prob)

for (envios in PARAM$cortes) {
  tb_prediccion_false_future[, Predicted := 0L]
  tb_prediccion_false_future[1:envios, Predicted := 1L]
  
  res <- realidad_evaluar(drealidad, tb_prediccion_false_future)
  
  options(scipen = 999)
  cat("Envios=", envios, "\t",
    " TOTAL=", res$total,
    "  Public=", res$public,
    " Private=", res$private,
    "\n",
    sep = ""
  )
  
  resultados_ganancia <- rbind(resultados_ganancia,
    data.table(envios = envios, ganancia_total = res$total))
}

ptest <- ggplot(resultados_ganancia, aes(x = envios, y = ganancia_total)) +
  geom_line(color = "#2C7BB6", size = 1) +
  geom_point(color = "#2C7BB6", size = 2) +
  geom_hline(yintercept = 0, linetype = "solid", color = "gray50", alpha = 0.3) +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = paste0("Curva de Ganancia - EXP ", PARAM$experimento),
    x = "Cantidad de Envios",
    y = "Ganancia Total ($)"
  ) +
  theme_minimal()

print(ptest)

# setwd(dir_experimento) #para Rstudio

# para VM

nombre_archivo <- paste0("curva_ganancia_exp_", PARAM$experimento, ".png")
ggsave(nombre_archivo, p, width = 10, height = 6, dpi = 300)


```

```{r}
# chunk102b - Gráfico de ganancia acumulada (referencia sofi)

# Ordenar por probabilidad descendente
setorder(tb_prediccion_false_future, -prob)

# Calcular ganancia acumulada
tb_prediccion_false_future[, indice := 1:.N]

# Unir con drealidad para tener clase_ternaria
tb_ganancia <- merge(
  tb_prediccion_false_future[, .(numero_de_cliente, foto_mes, prob, indice)],
  drealidad[, .(numero_de_cliente, foto_mes, clase_ternaria)],
  by = c("numero_de_cliente", "foto_mes")
)

# Ordenar por indice para mantener el orden
setorder(tb_ganancia, indice)

# Calcular ganancia individual y acumulada
tb_ganancia[, ganancia_individual := ifelse(clase_ternaria == "BAJA+2", 780000, -20000)]
tb_ganancia[, ganancia_acumulada := cumsum(ganancia_individual)]

# Encontrar ganancia máxima
ganancia_maxima <- max(tb_ganancia$ganancia_acumulada)
indice_maximo <- tb_ganancia[ganancia_acumulada == ganancia_maxima, indice][1]

# Filtrar datos (umbral 66% de ganancia máxima)
umbral_ganancia <- ganancia_maxima * 0.66
tb_filtrada <- tb_ganancia[ganancia_acumulada >= umbral_ganancia]

# Crear gráfico
p_acumulada <- ggplot(tb_filtrada, aes(x = indice, y = ganancia_acumulada)) +
  geom_line(color = "blue", size = 1.2) +
  geom_point(data = tb_ganancia[indice == indice_maximo], 
             aes(x = indice, y = ganancia_acumulada),
             color = "red", size = 3) +
  annotate("label", 
           x = indice_maximo, 
           y = ganancia_maxima * 1.05,
           label = paste0("Ganancia Máxima\n", format(ganancia_maxima, big.mark = ",", scientific = FALSE)),
           color = "red", 
           fontface = "bold",
           fill = "white",
           label.size = 0.5) +
  geom_segment(aes(x = indice_maximo, y = ganancia_maxima,
                   xend = indice_maximo, yend = ganancia_maxima * 1.04),
               arrow = arrow(length = unit(0.3, "cm")),
               color = "red") +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = paste0("Ganancia acumulada por orden de predicción (filtrada) - EXP ", PARAM$experimento),
    x = "Clientes ordenados por probabilidad",
    y = "Ganancia Acumulada"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    panel.grid.minor = element_line(alpha = 0.3)
  )

print(p_acumulada)

# Guardar
# setwd(dir_experimento) #para R
nombre_archivo_acum <- paste0("curva_ganancia_acumulada_exp_", PARAM$experimento, ".png")
ggsave(nombre_archivo_acum, p_acumulada, width = 14, height = 8, dpi = 300)

cat("Ganancia máxima:", format(ganancia_maxima, big.mark = ","), "\n")
cat("Corte ideal por cliente:", indice_maximo, "\n")

```


```{r chunk104}

# setwd(dir_experimento) #para R

#para VM
write_yaml(PARAM, file = "PARAM.yml")

```

```{r chunk105}


# CONTROL FILAS
# : Submission must have 164313 rows

filas_prediccion_ff <- nrow(tb_prediccion_false_future)
filas202104 <- nrow(dataset[foto_mes == 202104])

print(paste("'tb_prediccion_ff' tiene", filas_prediccion_ff, "filas."))
print(paste("'202104' tiene", filas202104, "filas."))
```

```{r chunk106}
format(Sys.time(), "%a %b %d %X %Y")
```

