
title: 'Competencia 02'
output: html_document


9100

HT
dataset: 
hiperparámetros: reducido
meses train HT: 
undersamling: 0.2

FINAL TRAIN: 
dataset:
meses final train:
seeds:

FUTURE: 
FALSE_FUTURE:


#INICIO


```{r chunk29}
# limpio la memoria
rm(list=ls(all.names=TRUE)) # remove all objects
gc(full=TRUE, verbose=FALSE) # garbage collection
```

### nro exp seeds
```{r}

PARAM <- list()
PARAM$experimento <- 9100

PARAM$semilla_primigenia <- 999199
PARAM$semillas <- c(999199, 999499, 999599, 999959, 999979
                    , 104729, 523987, 7919,1299709, 2097593
)

```

```{r chunk22}
getwd()
```

### carpeta exp para R
```{r}

# PARA R
# Definir directorio base
dir_base <- "C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia_02"
dir.create(dir_base, showWarnings = FALSE, recursive = TRUE)
# # Crear directorio exp
experimento <- paste0("exp", PARAM$experimento)
dir_experimento <- file.path(dir_base, experimento)
dir.create(dir_experimento, showWarnings = FALSE)

setwd(dir_experimento)
getwd()

```

```{r chunk22}
getwd()
```


### librerías
```{r chunk4}
# cargo las librerias que necesito
require("data.table")
require("parallel")

if(!require("R.utils")) install.packages("R.utils")
require("R.utils")

if( !require("primes") ) install.packages("primes")
require("primes")

if( !require("utils") ) install.packages("utils")
require("utils")

if( !require("rlist") ) install.packages("rlist")
require("rlist")

if( !require("yaml")) install.packages("yaml")
require("yaml")

if( !require("lightgbm") ) install.packages("lightgbm")
require("lightgbm")

if( !require("DiceKriging") ) install.packages("DiceKriging")
require("DiceKriging")

if( !require("mlrMBO") ) install.packages("mlrMBO")
require("mlrMBO")
```

Esta parte se debe correr con el runtime en lenguaje **R** Ir al menu, Runtime -> Change Runtime Tipe -> Runtime type -> R

#DATASET CRUDO
```{r}

require( "data.table" )

#VM
# dataset <- fread("https://storage.googleapis.com/open-courses/dmeyf2025-e4a2/competencia_02_crudo.csv.gz" )

#Rstudio
dataset <- fread("C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia_02/competencia_02_crudo.csv.gz" )


```

## Clase ternaria
```{r chunk1}

#CLASE TERNARIA
# calculo el periodo0 consecutivo
dsimple <- dataset[, list(
    "pos" = .I,
    numero_de_cliente,
    periodo0 = as.integer(foto_mes/100)*12 +  foto_mes%%100 ) ]


# ordeno
setorder( dsimple, numero_de_cliente, periodo0 )

# calculo topes
periodo_ultimo <- dsimple[, max(periodo0) ]
periodo_anteultimo <- periodo_ultimo - 1


# calculo orden 1 y 2
dsimple[, c("periodo1", "periodo2") :=
    shift(periodo0, n=1:2, fill=NA, type="lead"),  numero_de_cliente ]

# assign most common class values = "CONTINUA"
dsimple[ periodo0 < periodo_anteultimo, clase_ternaria := "CONTINUA" ]

# calculo BAJA+1
dsimple[ periodo0 < periodo_ultimo &
    ( is.na(periodo1) | periodo0 + 1 < periodo1 ),
    clase_ternaria := "BAJA+1" ]

# calculo BAJA+2
dsimple[ periodo0 < periodo_anteultimo & (periodo0+1 == periodo1 )
    & ( is.na(periodo2) | periodo0 + 2 < periodo2 ),
    clase_ternaria := "BAJA+2" ]


# pego el resultado en el dataset original y grabo
setorder( dsimple, pos )
dataset[, clase_ternaria := dsimple$clase_ternaria ]

```

```{r chunk3}
setorder( dataset, foto_mes, clase_ternaria, numero_de_cliente)
dataset[, .N, list(foto_mes, clase_ternaria)]
```


```{r chunk2}
dim(dataset)
```
####fwrite
```{r}
# VM
# fwrite( dataset,
#     file =  "/content/datasets/competencia_01.csv.gz",
#     sep = ","
# )


# fwrite(dataset,
#        file = "competencia_02.csv.gz",
#        sep = ",")

```
```{r}
# dataset <-fread("competencia_02.csv.gz", stringsAsFactors = TRUE)
```

# DATA ENGINEERING

## TIPOS datos
```{r chunk7}
# #TIPOS DE DATOS POR COLUMNA
# cat("=== TIPOS DE DATOS POR COLUMNA ===\n")
# tipos_datos <- sapply(dataset, class)
# print(tipos_datos)

```

```{r chunk8}
# resumen_tipos <- data.table(
#   columna = names(dataset),
#   tipo = sapply(dataset, function(x) class(x)[1]),
#   tipo_r = sapply(dataset, typeof),
#   valores_unicos = sapply(dataset, function(x) length(unique(x))),
#   valores_na = sapply(dataset, function(x) sum(is.na(x))),
#   porcentaje_na = sapply(dataset, function(x) round(sum(is.na(x))/length(x)*100, 2))
# )
# 
# print(resumen_tipos)
```

### v excluir
```{r chunk9}
# variables_excluir
# variables_excluir <- c("numero_de_cliente", "foto_mes", "clase_ternaria")
```

### v continuas
```{r chunk10}
# variables continuas

variables_continuas <- c(
  # Variables de rentabilidad y comisiones
  "mrentabilidad", "mrentabilidad_annual", "mcomisiones",
  "mactivos_margen", "mpasivos_margen",

  # Cuentas y saldos principales
  "mcuenta_corriente", "mcaja_ahorro", "mcuentas_saldo",

  # Tarjetas - consumos
  "mtarjeta_visa_consumo", "mtarjeta_master_consumo",

  # Préstamos - montos
  "mprestamos_personales", "mprestamos_prendarios", "mprestamos_hipotecarios",

  # Inversiones - montos
  "mplazo_fijo_pesos", "mplazo_fijo_dolares",
  "minversion1_pesos", "minversion1_dolares", "minversion2",

  # Payroll
  "mpayroll", "mpayroll2",

  # Débitos automáticos
  "mcuenta_debitos_automaticos",

  # Servicios y pagos
  "mpagodeservicios", "mpagomiscuentas",

  # Comisiones
  "mcomisiones_mantenimiento", "mcomisiones_otras",

  # Forex
  "mforex_buy", "mforex_sell",

  # Transferencias
  "mtransferencias_recibidas", "mtransferencias_emitidas",

  # Extracciones y ATM
  "mextraccion_autoservicio", "matm", "matm_other",

  # Cheques
  "mcheques_depositados", "mcheques_emitidos",

  # Tarjetas Master y Visa - saldos y pagos
  "Master_msaldototal", "Master_msaldopesos", "Master_msaldodolares",
  "Master_mconsumospesos", "Master_mconsumosdolares",
  "Master_mlimitecompra", "Master_madelantopesos", "Master_madelantodolares",
  "Master_mpagado", "Master_mpagospesos", "Master_mpagosdolares",
  "Master_mconsumototal", "Master_mpagominimo",

  "Visa_msaldototal", "Visa_msaldopesos", "Visa_msaldodolares",
  "Visa_mconsumospesos", "Visa_mconsumosdolares",
  "Visa_mlimitecompra", "Visa_madelantopesos", "Visa_madelantodolares",
  "Visa_mpagado", "Visa_mpagospesos", "Visa_mpagosdolares",
  "Visa_mconsumototal", "Visa_mpagominimo",

  "mcuenta_corriente_adicional", "Master_mfinanciacion_limite", "Visa_mfinanciacion_limite",
"mcheques_emitidos_rechazados", "mcheques_depositados_rechazados", "mtarjeta_master_descuentos",
"mcaja_ahorro_adicional", "mtarjeta_visa_descuentos", "mttarjeta_master_debitos_automaticos",
"mcaja_ahorro_dolares", "mttarjeta_visa_debitos_automaticos", "mautoservicio", "mcajeros_propios_descuentos"
 )

cat("Variables continuas:", length(variables_continuas), "\n")
cat("Variables:\n")
print(variables_continuas)

# Verificar que todas las variables existen en el dataset
variables_existentes <- intersect(variables_continuas, names(dataset))
variables_faltantes <- setdiff(variables_continuas, names(dataset))

if(length(variables_faltantes) > 0) {
  cat("\n️ ADVERTENCIA: Variables no encontradas en dataset:\n")
  print(variables_faltantes)
}

variables_continuas <- variables_existentes
cat("\nVariables confirmadas en dataset:", length(variables_continuas), "\n")
```


```{r chunk13}
# Variables a explorar = todas - (continuas (con sus lags y deltas), excluir)
# 
# variables_explorar <- setdiff(names(dataset),
#                               c(variables_continuas, variables_excluir,
#                                ))
# 
# cat("Variables continuas:", length(variables_continuas), "\n")
# cat("Variables excluir:", length(variables_excluir), "\n")
# cat("Variables a explorar:", length(variables_explorar), "\n")


```

### v binarias
```{r chunk14}

# # Variables binarias
# variables_binarias <- c(
#   "active_quarter", "cliente_vip", "cdescubierto_preacordado", "tcallcenter", "thomebanking", "tmobile_app", "Master_delinquency", "Visa_delinquency"
# )
# 
# #tcuentas no va porque tiene tres valores 0, 1, 2
# #ccajas_transacciones  cantidad
# #cmobile_app_trx  cantidad
# 
# cat("Total variables binarias:", length(variables_binarias), "\n\n")
# 
# # Análisis detallado de cada binaria
# for(var in variables_binarias) {
#   cat("Variable:", var, "\n")
# 
#   # Valores únicos y frecuencias
#   valores <- sort(unique(dataset[[var]][!is.na(dataset[[var]])]))
#   freq_table <- table(dataset[[var]], useNA = "ifany")
# 
#   cat("  Valores:", paste(valores, collapse = ", "), "\n")
#   cat("  Distribución:", paste(names(freq_table), "=", as.numeric(freq_table), collapse = " | "), "\n")
# 
#   # Recomendación para LightGBM
#   if(all(valores %in% c(0, 1))) {
#     cat(" LightGBM: Mantener como INTEGER (0/1)\n")
#   } else {
#     cat("LightGBM: Considerar recodificar a 0/1\n")
#   }
# 
#   
# }

```


### v discretas
```{r chunk17}
# # Variables discretas = todas - (continuas (con sus deltas y lags), binarias (con sus deltas y lags), excluir)
# 
# 
# total_lags <- length(grep("_lag[0-9]+$", names(dataset)))
# total_deltas <- length(grep("_delta[0-9]+$", names(dataset)))
# 
# variables_discretas <- setdiff(names(dataset),
#                               c(variables_continuas, variables_excluir,
#                                 variables_binarias, total_lags, total_deltas
#                                 ))
# 
# cat("Variables continuas:", length(variables_continuas), "\n")
# cat("Variables excluir:", length(variables_excluir), "\n")
# cat("Variables binarias:", length(variables_binarias), "\n")
# cat("Variables discretas:", length(variables_discretas), "\n")
# 

```


## PRESTAMOS personales

```{r chunk66a}

# Identificar columnas a eliminar
columnas_eliminar <- grep("prestamos_personales", names(dataset), value = TRUE)

# Visualizar las columnas que se van a eliminar
print(paste("Total de columnas a eliminar:", length(columnas_eliminar)))
print(columnas_eliminar)


```

```{r}
# # Eliminar las columnas
dataset[, (columnas_eliminar) := NULL]

variables_continuas <- intersect(variables_continuas, names(dataset))
print(length(variables_continuas))
dim(dataset)
#153 (con clase ternaria, sin prestamos personales m y c)
```

## NAN si batch total 0

```{r}
#EXPLORO

res_wide <- dataset[
  , lapply(.SD, function(v) {
      # condición estricta: sin NAs y todos exactamente 0
      if (!all(!is.na(v))) return(FALSE)
      all(v == 0)
    }),
  by = foto_mes
]

cero_por_batch <- melt(
  res_wide,
  id.vars = "foto_mes",
  variable.name = "variable",
  value.name = "todo_cero"
)[todo_cero == TRUE, .(variable, foto_mes)]

cero_por_batch


```

```{r}

# APLICO
NA_of <- function(v){
  if (inherits(v, "integer64")) {
    if (requireNamespace("bit64", quietly = TRUE)) return(bit64::NA_integer64_)
    return(NA)  # fallback si no está bit64
  }
  if (is.integer(v))   return(NA_integer_)
  if (is.numeric(v))   return(NA_real_)
  if (is.logical(v))   return(NA)
  if (is.character(v)) return(NA_character_)
  # factor, Date, etc. → NA genérico
  NA
}

vars <- unique(cero_por_batch$variable)

for (var in vars) {
  meses <- cero_por_batch[variable == var, foto_mes]
  na_val <- NA_of(dataset[[var]])
  
  # comparación a 0 según tipo (para evitar coerciones raras)
  if (is.character(dataset[[var]])) {
    dataset[foto_mes %in% meses & get(var) == "0", (var) := na_val]
  } else {
    dataset[foto_mes %in% meses & get(var) == 0,  (var) := na_val]
  }
}

```

```{r}
dim(dataset)
```

## NAN monto si cant 0

```{r}

# Diccionario 1: (c -> m)  aplicar NA a m cuando c == 0/"0"

dict_c0_mNA <- list(
  c("ccaja_ahorro","mcaja_ahorro"),
  c("ccaja_ahorro","mcaja_ahorro_dolares"),
  c("ccajeros_propios_descuentos","mcajeros_propios_descuentos"),
  c("ccheques_depositados_rechazados","mcheques_depositados_rechazados"),
  c("ccheques_emitidos_rechazados","mcheques_emitidos_rechazados"),
  c("ccomisiones_mantenimiento","mcomisiones_mantenimiento"),
  c("ccuenta_corriente","mcuenta_corriente"),
  c("cextraccion_autoservicio","mextraccion_autoservicio"),
  c("cforex_buy","mforex_buy"),
  c("cforex_sell","mforex_sell"),
  c("cinversion1","minversion1_dolares"),
  c("cinversion1","minversion1_pesos"),
  c("cinversion2","minversion2"),
  c("cpagodeservicios","mpagodeservicios"),
  c("cpagomiscuentas","mpagomiscuentas"),
  c("cplazo_fijo","mplazo_fijo_dolares"),
  c("cplazo_fijo","mplazo_fijo_pesos"),
  c("cprestamos_hipotecarios","mprestamos_hipotecarios"),
  c("cprestamos_prendarios","mprestamos_prendarios"),
  c("ctarjeta_visa", "Visa_mlimitecompra"),
  c("ctarjeta_master", "Master_mlimitecompra")
)

# Diccionario 2: (c -> m) aplicar NA a m cuando (c == 0/"0"  y  m == 0/"0")
dict_c0_m0_mNA <- list(
  c("ccheques_depositados","mcheques_depositados"),
  c("ccheques_emitidos","mcheques_emitidos")
)

# Diccionario 3: ([c1,c2] -> [m1,m2]) aplicar NA a ambos m cuando (c1 == 0/"0"  y  c2 == 0/"0")
dict_c0c0_mNAmNA <- list(
  list(c_cols = c("cpayroll_trx","cpayroll2_trx"),
       m_cols = c("mpayroll","mpayroll2"))
)

# Helpers
NA_of <- function(v){
  if (inherits(v, "integer64")) return(if (requireNamespace("bit64", quietly=TRUE)) bit64::NA_integer64_ else NA)
  if (is.integer(v))   return(NA_integer_)
  if (is.numeric(v))   return(NA_real_)
  if (is.logical(v))   return(NA)
  if (is.character(v)) return(NA_character_)
  NA
}

is_zero <- function(x){
  if (is.character(x) || is.factor(x)) x == "0" else x == 0
}

```

```{r}

# Regla 1: si c == 0 -> m := NA
for (p in dict_c0_mNA){
  ccol <- p[1]; mcol <- p[2]
  if (!all(c(ccol, mcol) %in% names(dataset))) next
  cond <- is_zero(dataset[[ccol]])
  na_m  <- NA_of(dataset[[mcol]])
  dataset[cond, (mcol) := na_m]
}

# Regla 2: si c == 0  y  m == 0 -> m := NA
for (p in dict_c0_m0_mNA){
  ccol <- p[1]; mcol <- p[2]
  if (!all(c(ccol, mcol) %in% names(dataset))) next
  cond <- is_zero(dataset[[ccol]]) & is_zero(dataset[[mcol]])
  na_m  <- NA_of(dataset[[mcol]])
  dataset[cond, (mcol) := na_m]
}

# Regla 3: si c1 == 0  y  c2 == 0 -> m1 := NA  y  m2 := NA
for (item in dict_c0c0_mNAmNA){
  c1 <- item$c_cols[1]; c2 <- item$c_cols[2]
  m1 <- item$m_cols[1]; m2 <- item$m_cols[2]
  if (!all(c(c1,c2,m1,m2) %in% names(dataset))) next
  cond <- is_zero(dataset[[c1]]) & is_zero(dataset[[c2]])
  na1  <- NA_of(dataset[[m1]])
  na2  <- NA_of(dataset[[m2]])
  dataset[cond, c((m1),(m2)) := .(na1, na2)]
}

```


```{r}
dim(dataset)
```

## RANKINGS

```{r}
# Borrar todas las columnas que terminen en _rank
cols_rank <- grep("_rank$", names(dataset), value = TRUE)
print(cols_rank)


```

```{r}
dataset[, (cols_rank) := NULL]
cat("Columnas borradas:", length(cols_rank), "\n")
```

```{r}
cat("Variables continuas:", length(variables_continuas), "\n")

```

```{r}
# Rankings con cero fijo: positivos → [0,1], negativos → [-1,0], cero fijo
# Calcula percentiles dentro de cada foto_mes preservando el cero
ranking_cero_fijo <- function(x) {
  positivos <- x > 0 & !is.na(x)
  negativos <- x < 0 & !is.na(x)
  ceros <- x == 0 & !is.na(x)
  nas <- is.na(x)
  
  n_pos <- sum(positivos)
  n_neg <- sum(negativos)
  
  resultado <- numeric(length(x))
  resultado[nas] <- NA
  resultado[ceros] <- 0
  
  if (n_pos > 0) {
    ranks_pos <- frank(x[positivos], ties.method = "average", na.last = "keep")
    resultado[positivos] <- (ranks_pos - 1) / n_pos
  }
  
  if (n_neg > 0) {
    ranks_neg <- frank(x[negativos], ties.method = "average", na.last = "keep")
    resultado[negativos] <- -1 + (ranks_neg - 1) / n_neg
  }
  
  return(resultado)
}

```


```{r}
# Variables para rankear top 20 features solo montos

#  vars_ranking <- c(
#   "mcuentas_saldo",
#   "mcaja_ahorro",
#   "mcuenta_corriente",
#   "mpasivos_margen",
#   "mactivos_margen",
#   "mtarjeta_visa_consumo",
#   "mtarjeta_master_consumo",
#   "mprestamos_personales",
#   "mpayroll",
#   "mrentabilidad",
#   "mrentabilidad_annual"
# )

vars_ranking <- variables_continuas
  
# Aplicar ranking por foto_mes
for (var in vars_ranking) {
  if (var %in% names(dataset)) {
    col_rank <- paste0(var, "_rank")
    dataset[, (col_rank) := ranking_cero_fijo(get(var)), by = foto_mes]
    cat("Ranking creado:", col_rank, "\n")
  }
}

# Validación de rangos
vars_rank <- paste0(vars_ranking[vars_ranking %in% names(dataset)], "_rank")
cat("\n=== VALIDACIÓN DE RANGOS ===\n")
dataset[, lapply(.SD, function(x) {
  c(min = round(min(x, na.rm = TRUE), 6), 
    max = round(max(x, na.rm = TRUE), 6),
    ceros = sum(x == 0, na.rm = TRUE))
}), .SDcols = vars_rank]

cat("\nRankings aplicados correctamente\n")


```


```{r}
variables_continuas_ranking <- grep("_rank$", names(dataset), value = TRUE)
print(variables_continuas_ranking)

```


```{r}
# library(ggplot2)
# library(gridExtra)
# 
# visualizar_ranking <- function(dataset, variable, mes_ejemplo = 202101) {
#   
#   var_rank <- paste0(variable, "_rank")
#   
#   if (!variable %in% names(dataset)) {
#     stop(paste("Variable", variable, "no existe en el dataset"))
#   }
#   
#   if (!var_rank %in% names(dataset)) {
#     stop(paste("Ranking", var_rank, "no existe. Ejecutar ranking_cero_fijo primero"))
#   }
#   
#   dt_plot <- dataset[foto_mes == mes_ejemplo]
#   
#   cat("\n=== COMPOSICIÓN DE", variable, "===\n")
#   stats <- dt_plot[, .(
#     n_total = .N,
#     n_nas = sum(is.na(get(variable))),
#     pct_nas = round(100 * sum(is.na(get(variable))) / .N, 2),
#     n_ceros = sum(get(variable) == 0, na.rm = TRUE),
#     pct_ceros = round(100 * sum(get(variable) == 0, na.rm = TRUE) / .N, 2),
#     n_positivos = sum(get(variable) > 0, na.rm = TRUE),
#     pct_positivos = round(100 * sum(get(variable) > 0, na.rm = TRUE) / .N, 2),
#     n_negativos = sum(get(variable) < 0, na.rm = TRUE),
#     pct_negativos = round(100 * sum(get(variable) < 0, na.rm = TRUE) / .N, 2)
#   )]
#   print(stats)
#   
#   tiene_negativos <- stats$n_negativos > 0
#   tiene_ceros <- stats$n_ceros > 0
#   tiene_nas <- stats$n_nas > 0
#   
#   # Gráfico 1: Densidad original (sin ceros ni NAs)
#   if (tiene_negativos) {
#     p1 <- ggplot(dt_plot[get(variable) != 0 & !is.na(get(variable))], 
#                  aes(x = abs(get(variable)), fill = get(variable) > 0)) +
#       geom_density(alpha = 0.6) +
#       scale_x_log10(labels = scales::comma) +
#       scale_fill_manual(values = c("TRUE" = "darkgreen", "FALSE" = "darkred"),
#                         labels = c("TRUE" = "Positivos", "FALSE" = "Negativos")) +
#       labs(title = paste("Distribución Original:", variable),
#            subtitle = paste0("foto_mes = ", mes_ejemplo, " | Escala log (sin ceros/NAs)"),
#            x = "Valor Absoluto (log10)", y = "Densidad", fill = "") +
#       theme_minimal() +
#       theme(legend.position = "top")
#   } else {
#     p1 <- ggplot(dt_plot[get(variable) > 0 & !is.na(get(variable))], 
#                  aes(x = get(variable))) +
#       geom_density(fill = "darkgreen", alpha = 0.6) +
#       scale_x_log10(labels = scales::comma) +
#       labs(title = paste("Distribución Original:", variable),
#            subtitle = paste0("foto_mes = ", mes_ejemplo, " | Escala log"),
#            x = "Valor (log10)", y = "Densidad") +
#       theme_minimal()
#   }
#   
#   # Gráfico 2: Histograma del ranking (más informativo que densidad)
#   p2 <- ggplot(dt_plot[!is.na(get(var_rank))], aes(x = get(var_rank))) +
#     geom_histogram(bins = 50, fill = "darkgreen", alpha = 0.7, color = "white") +
#     geom_vline(xintercept = 0, color = "red", linetype = "dashed", linewidth = 1) +
#     geom_vline(xintercept = c(-1, 1), color = "blue", linetype = "dotted") +
#     scale_x_continuous(limits = c(-1.05, 1.05), breaks = seq(-1, 1, 0.25)) +
#     labs(title = "Histograma del Ranking",
#          subtitle = "Línea roja = cero | Líneas azules = extremos [-1, 1]",
#          x = "Ranking [-1, 1]", y = "Frecuencia") +
#     theme_minimal()
#   
#   # Gráfico 3: Boxplot
#   if (tiene_negativos) {
#     p3 <- ggplot(dt_plot[get(variable) != 0 & !is.na(get(variable))], 
#                  aes(x = factor(sign(get(variable))), y = abs(get(variable)))) +
#       geom_boxplot(fill = c("darkred", "darkgreen"), alpha = 0.6) +
#       scale_y_log10(labels = scales::comma) +
#       scale_x_discrete(labels = c("-1" = "Negativos", "1" = "Positivos")) +
#       labs(title = "Boxplot por signo (escala log)",
#            x = "", y = "Valor Absoluto (log10)") +
#       theme_minimal()
#   } else {
#     p3 <- ggplot(dt_plot[get(variable) > 0 & !is.na(get(variable))], 
#                  aes(x = "", y = get(variable))) +
#       geom_boxplot(fill = "darkgreen", alpha = 0.6) +
#       scale_y_log10(labels = scales::comma) +
#       labs(title = "Boxplot (escala log)",
#            x = "", y = "Valor (log10)") +
#       theme_minimal()
#   }
#   
#   # Gráfico 4: Scatter
#   set.seed(123)
#   muestra <- dt_plot[sample(.N, min(.N, 5000))]
#   
#   p4 <- ggplot(muestra[get(variable) != 0 & !is.na(get(variable))], 
#                aes(x = get(variable), y = get(var_rank))) +
#     geom_point(alpha = 0.3, color = "darkgreen", size = 1) +
#     geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
#     geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
#     scale_x_continuous(labels = scales::comma) +
#     labs(title = "Transformación: Original → Ranking",
#          x = "Valor Original", y = "Ranking") +
#     theme_minimal()
#   
#   print(p1)
#   print(p2)
#   grid.arrange(p3, p4, ncol = 2)
#   
#   cat("\n=== ESTADÍSTICAS DETALLADAS ===\n")
#   if (tiene_nas) {
#     cat("\nNAs:", stats$n_nas, "(", stats$pct_nas, "%)\n")
#   }
#   if (tiene_negativos) {
#     cat("\nValores negativos:\n")
#     print(summary(dt_plot[get(variable) < 0][[variable]]))
#   }
#   if (tiene_ceros) {
#     cat("\nValores cero:", stats$n_ceros, "\n")
#   }
#   cat("\nValores positivos:\n")
#   print(summary(dt_plot[get(variable) > 0][[variable]]))
#   
#   cat("\nRanking:\n")
#   print(summary(dt_plot[[var_rank]]))
#   cat("\nRanking NAs:", sum(is.na(dt_plot[[var_rank]])), "\n")
#   
#   invisible(list(stats = stats))
# }
```

```{r}
# visualizar_ranking(dataset, "mcuentas_saldo", mes_ejemplo = 202101) 

```

```{r}
# analizar_ranking <- function(dataset, variable, mes_ejemplo = 202101) {
#   
#   var_rank <- paste0(variable, "_rank")
#   
#   if (!var_rank %in% names(dataset)) {
#     stop(paste("Ranking", var_rank, "no existe en el dataset"))
#   }
#   
#   cat("\n=== ESTADÍSTICAS DEL RANKING:", var_rank, "===\n")
#   cat("Mes:", mes_ejemplo, "\n\n")
#   
#   # Estadísticas completas
#   stats <- dataset[foto_mes == mes_ejemplo, .(
#     min_ranking = min(get(var_rank), na.rm = TRUE),
#     max_ranking = max(get(var_rank), na.rm = TRUE),
#     cuantos_negativos = sum(get(var_rank) < 0, na.rm = TRUE),
#     cuantos_igual_cero = sum(get(var_rank) == 0, na.rm = TRUE),
#     cuantos_positivos = sum(get(var_rank) > 0, na.rm = TRUE),
#     cuantos_na = sum(is.na(get(var_rank)))
#   )]
#   print(stats)
#   
#   # Los 10 peores (más negativos)
#   cat("\n=== 10 PEORES (ranking más negativo) ===\n")
#   peores <- dataset[foto_mes == mes_ejemplo, 
#                     c(variable, var_rank), 
#                     with = FALSE][order(get(var_rank))][1:10]
#   print(peores)
#   
#   # Los 10 cercanos a cero
#   cat("\n=== 10 CLIENTES CERCA DEL CERO ===\n")
#   cercanos_cero <- dataset[foto_mes == mes_ejemplo, 
#                            c(variable, var_rank), 
#                            with = FALSE][order(abs(get(var_rank)))][1:10]
#   print(cercanos_cero)
#   
#   # Los 10 mejores (más positivos)
#   cat("\n=== 10 MEJORES (ranking más positivo) ===\n")
#   mejores <- dataset[foto_mes == mes_ejemplo, 
#                      c(variable, var_rank), 
#                      with = FALSE][order(-get(var_rank))][1:10]
#   print(mejores)
#   
#   invisible(list(stats = stats, peores = peores, 
#                  cercanos_cero = cercanos_cero, mejores = mejores))
# }


```

```{r}

# analizar_ranking(dataset, "mcuentas_saldo", mes_ejemplo = 202101)

```

```{r}

# histograma_ranking_temporal <- function(dataset, variable, periodos = NULL) {
#   
#   var_rank <- paste0(variable, "_rank")
#   
#   if (!var_rank %in% names(dataset)) {
#     stop(paste("Ranking", var_rank, "no existe en el dataset"))
#   }
#   
#   # Si no se especifican períodos, usar todos
#   if (is.null(periodos)) {
#     periodos <- unique(dataset$foto_mes)
#   }
#   
#   # Detectar si hay valores negativos en el ranking
#   tiene_negativos <- dataset[foto_mes %in% periodos, 
#                              any(get(var_rank) < 0, na.rm = TRUE)]
#   
#   # Ajustar límites y breaks según si hay negativos
#   if (tiene_negativos) {
#     limites <- c(-1.05, 1.05)
#     breaks_seq <- seq(-1, 1, 0.5)
#     rango_label <- "[-1, 1]"
#   } else {
#     limites <- c(-0.05, 1.05)
#     breaks_seq <- seq(0, 1, 0.25)
#     rango_label <- "[0, 1]"
#   }
#   
#   # Crear gráfico
#   p <- ggplot(dataset[foto_mes %in% periodos & !is.na(get(var_rank))], 
#               aes(x = get(var_rank))) +
#     geom_histogram(bins = 50, fill = "darkgreen", alpha = 0.7) +
#     facet_wrap(~foto_mes, scales = "free_y") +
#     geom_vline(xintercept = 0, color = "red", linetype = "dashed") +
#     geom_vline(xintercept = c(-1, 1), color = "blue", linetype = "dotted", alpha = 0.5) +
#     scale_x_continuous(limits = limites, breaks = breaks_seq) +
#     labs(title = paste("Evolución del Ranking:", variable),
#          x = paste("Ranking", rango_label), 
#          y = "Frecuencia") +
#     theme_minimal() +
#     theme(axis.text.x = element_text(angle = 45, hjust = 1))
#   
#   print(p)
#   invisible(p)
# }


```


```{r}

# evolucion_cliente_ranking <- function(dataset, variable, min_meses = 6, cliente_especifico = NULL) {
#   
#   var_rank <- paste0(variable, "_rank")
#   
#   if (!var_rank %in% names(dataset)) {
#     stop(paste("Ranking", var_rank, "no existe en el dataset"))
#   }
#   
#   # Si no se especifica cliente, elegir uno al azar
#   if (is.null(cliente_especifico)) {
#     set.seed(NULL)
#     clientes_validos <- dataset[!is.na(get(var_rank)), 
#                                 .(n_meses = .N), 
#                                 by = numero_de_cliente][n_meses >= min_meses]
#     
#     if (nrow(clientes_validos) == 0) {
#       stop(paste("No hay clientes con al menos", min_meses, "meses de datos válidos"))
#     }
#     
#     cliente_ejemplo <- clientes_validos[sample(.N, 1)]$numero_de_cliente
#   } else {
#     cliente_ejemplo <- cliente_especifico
#   }
#   
#   cat("Cliente seleccionado:", cliente_ejemplo, "\n")
#   
#   # Extraer evolución
#   evolucion <- dataset[numero_de_cliente == cliente_ejemplo, 
#                        c("foto_mes", variable, var_rank), 
#                        with = FALSE]
#   
#   print(evolucion)
#   
#   # Detectar si hay valores negativos
#   tiene_negativos <- any(evolucion[[var_rank]] < 0, na.rm = TRUE)
#   
#   # Ajustar escala Y según si hay negativos
#   if (tiene_negativos) {
#     limites_y <- c(-100, 100)
#     breaks_y <- seq(-100, 100, 20)
#   } else {
#     limites_y <- c(0, 100)
#     breaks_y <- seq(0, 100, 20)
#   }
#   
#   # Crear subtítulo con información de la variable
#   valor_actual <- evolucion[.N][[variable]]
#   if (!is.na(valor_actual)) {
#     subtitulo <- paste(variable, "actual:", round(valor_actual, 2))
#   } else {
#     subtitulo <- paste(variable, ": ver tabla")
#   }
#   
#   # Graficar
#   p <- ggplot(evolucion[!is.na(get(var_rank))], 
#               aes(x = foto_mes, y = get(var_rank) * 100)) +
#     geom_line(color = "darkgreen", linewidth = 1) +
#     geom_point(color = "darkgreen", size = 3) +
#     geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
#     scale_y_continuous(name = "Ranking (percentil)", 
#                        breaks = breaks_y,
#                        limits = limites_y) +
#     labs(title = paste("Evolución Ranking", variable, "- Cliente", cliente_ejemplo),
#          subtitle = subtitulo,
#          x = "Mes") +
#     theme_minimal() +
#     theme(axis.text.x = element_text(angle = 45, hjust = 1))
#   
#   print(p)
#   
#   invisible(list(cliente = cliente_ejemplo, evolucion = evolucion, plot = p))
# }
```



```{r}

# Cliente aleatorio
# evolucion_cliente_ranking(dataset, "mcuentas_saldo")

# # Cliente específico
# evolucion_cliente_ranking(dataset, "mcuentas_saldo", cliente_especifico = 123456)
# 
# # Cambiar mínimo de meses
# evolucion_cliente_ranking(dataset, "mcuentas_saldo", min_meses = 10)

```


```{r}

# # Uso:
# # Cliente aleatorio
# evolucion_cliente_ranking(dataset, "mpayroll")
#
# # Verificar que el ranking se calculó por mes
# cat("\n=== VERIFICACIÓN: ¿Se calculó el ranking por mes? ===\n")
# 
# # Ver algunos valores para diferentes meses
# dataset[numero_de_cliente == dataset[1]$numero_de_cliente, 
#         .(foto_mes, mcuentas_saldo, mcuentas_saldo_rank)]
# 
# # Verificar que los rankings son diferentes entre meses
# cat("\n=== Estadísticas del ranking por mes ===\n")
# dataset[!is.na(mcuentas_saldo_rank), 
#         .(min_rank = min(mcuentas_saldo_rank),
#           max_rank = max(mcuentas_saldo_rank),
#           media_rank = mean(mcuentas_saldo_rank),
#           n_obs = .N), 
#         by = foto_mes]
# 
# # Ver si mcuentas_saldo varía entre meses
# cat("\n=== mcuentas_saldo varía entre meses? ===\n")
# dataset[, .(min_saldo = min(mcuentas_saldo, na.rm = TRUE),
#             max_saldo = max(mcuentas_saldo, na.rm = TRUE),
#             media_saldo = mean(mcuentas_saldo, na.rm = TRUE)), 
#         by = foto_mes]
```

```{r}
# # Función para ver los umbrales de percentiles por mes
# umbrales_percentiles <- function(dataset, variable, percentiles = c(0.1, 0.25, 0.5, 0.75, 0.9)) {
#   
#   var_rank <- paste0(variable, "_rank")
#   
#   if (!var_rank %in% names(dataset)) {
#     stop(paste("Ranking", var_rank, "no existe en el dataset"))
#   }
#   
#   # Calcular umbrales para cada mes
#   umbrales <- dataset[!is.na(get(var_rank)), 
#                       .(percentil_10 = quantile(get(variable)[get(var_rank) > 0], 0.1, na.rm = TRUE),
#                         percentil_25 = quantile(get(variable)[get(var_rank) > 0], 0.25, na.rm = TRUE),
#                         percentil_50 = quantile(get(variable)[get(var_rank) > 0], 0.5, na.rm = TRUE),
#                         percentil_75 = quantile(get(variable)[get(var_rank) > 0], 0.75, na.rm = TRUE),
#                         percentil_90 = quantile(get(variable)[get(var_rank) > 0], 0.9, na.rm = TRUE),
#                         n_negativos = sum(get(variable) < 0),
#                         n_ceros = sum(get(variable) == 0),
#                         n_positivos = sum(get(variable) > 0)), 
#                       by = foto_mes]
#   
#   cat("\n=== UMBRALES DE PERCENTILES PARA", variable, "===\n")
#   cat("¿Qué valor de", variable, "define cada percentil en cada mes?\n\n")
#   print(umbrales)
#   
#   # Graficar evolución de umbrales
#   umbrales_long <- melt(umbrales, 
#                         id.vars = "foto_mes", 
#                         measure.vars = c("percentil_10", "percentil_25", 
#                                         "percentil_50", "percentil_75", "percentil_90"),
#                         variable.name = "percentil",
#                         value.name = "valor_umbral")
#   
#   p <- ggplot(umbrales_long, aes(x = foto_mes, y = valor_umbral, 
#                                   color = percentil, group = percentil)) +
#     geom_line(linewidth = 1) +
#     geom_point(size = 2) +
#     scale_y_continuous(labels = scales::comma) +
#     scale_color_manual(values = c("percentil_10" = "#d73027",
#                                    "percentil_25" = "#fc8d59",
#                                    "percentil_50" = "#fee090",
#                                    "percentil_75" = "#91bfdb",
#                                    "percentil_90" = "#4575b4"),
#                        labels = c("P10", "P25", "P50 (mediana)", "P75", "P90")) +
#     labs(title = paste("Evolución de Umbrales de Percentiles:", variable),
#          subtitle = "¿Cuánto necesita un cliente para estar en cada percentil?",
#          x = "Mes", y = paste("Valor de", variable),
#          color = "Percentil") +
#     theme_minimal() +
#     theme(axis.text.x = element_text(angle = 45, hjust = 1),
#           legend.position = "right")
#   
#   print(p)
#   
#   invisible(list(umbrales = umbrales, plot = p))
# }
# 


```

```{r}
# # Uso
# umbrales_percentiles(dataset, "mcuentas_saldo")
# 
# # También ver para comparar qué cliente está en qué percentil en diferentes meses
# cat("\n=== EJEMPLO: ¿En qué percentil está un cliente mes a mes? ===\n")
# cliente_ej <- dataset[!is.na(mcuentas_saldo_rank)][1]$numero_de_cliente
# dataset[numero_de_cliente == cliente_ej, 
#         .(foto_mes, 
#           mcuentas_saldo, 
#           ranking = round(mcuentas_saldo_rank, 3),
#           percentil = paste0("P", round(mcuentas_saldo_rank * 100, 1)))]
```


## (off) AGUINALDO
clientes que tuvieron payroll en 04 y 06, tienen al menos 1 ctrxpayroll en junio y su mpayroll de junio es al menos 40% mayor que el de abril. (me quedan clientes con ctrxpayroll positivo sin asignar a la columna). Por lo tanto lo que identifico no son empleados, sino aproximo aguinaldos.

```{r}

# # Eliminar las columnas si existen
# dataset[, recibio_aguinaldo := NULL]
# dataset[, aguinaldo_estimado := NULL]
# dataset[, mpayroll_normalizado := NULL]
# 
# cat("✓ Columnas eliminadas completamente.\n")
# cat("✓ Listo para volver a ejecutar desde cero.\n")
```

### recibio_aguinaldo 202106
```{r}

# #VERIFICACIÓN DE LAGS
# cat("VERIFICACIÓN DE LAGS\n")
# cliente_test <- dataset[foto_mes == 202106, numero_de_cliente][1]
# verificacion <- dataset[numero_de_cliente == cliente_test & foto_mes %in% c(202104, 202106),
#                         .(numero_de_cliente, foto_mes, mpayroll)]
# print(verificacion)
# 
# lag2_correcto <- dataset[numero_de_cliente == cliente_test & foto_mes == 202106, mpayroll_lag2]
# valor_202104 <- dataset[numero_de_cliente == cliente_test & foto_mes == 202104, mpayroll]
# 
# cat("\nLag2 en 202106:", lag2_correcto, "\n")
# cat("Valor real 202104:", valor_202104, "\n")
# cat("Coinciden:", isTRUE(all.equal(lag2_correcto, valor_202104)), "\n\n")
# 
# # DETECCIÓN DE QUIEN RECIBIÓ AGUINALDO
# dataset[, recibio_aguinaldo := 0L]
# 
# dataset[foto_mes == 202106, recibio_aguinaldo := ifelse(
#   cpayroll_trx_lag2 > 0 & #sueldo en abril
#   cpayroll_trx_lag1 > 0 & #sueldo en abril
#   cpayroll_trx > 0 & #sueldo en junio
#   mpayroll > 1.4 * mpayroll_lag1 & #sueldo junio 40% mayor que abril
#   !is.na(mpayroll) & !is.na(mpayroll_lag2) & #sueldo abril y junio no NA
#   mpayroll > 0 & mpayroll_lag1 > 0 & mpayroll_lag2 > 0, #sueldo abril y junio positivo
#   1L, 0L
# )]
# 
# cat("\nDISTRIBUCIÓN DE 'recibio_aguinaldo' en 202106:\n")
# print(dataset[foto_mes == 202106, .N, by = recibio_aguinaldo])
# 
# 
# cat("\nDISTRIBUCIÓN cpayroll_trx:\n")
# print(dataset[foto_mes == 202106 & recibio_aguinaldo == 1, .N, by = cpayroll_trx])

```


```{r}
# tabla_aguinaldo <- dataset[foto_mes == 202106, .N, by = recibio_aguinaldo]
# tabla_aguinaldo[, proporcion := round(N / sum(N) * 100, 2)]
# print(tabla_aguinaldo)

```

```{r}

# # ANALISIS DE ESTABILIDAD DE MPAYROLL EN MESES TRAIN
# # Filtrar solo clientes que recibieron aguinaldo en junio
# clientes_con_aguinaldo <- dataset[foto_mes == 202106 & recibio_aguinaldo == 1, 
#                                    unique(numero_de_cliente)]
# 
# # Calcular promedio de mpayroll por mes para esos clientes
# evolucion_mpayroll <- dataset[numero_de_cliente %in% clientes_con_aguinaldo & 
#                                !is.na(mpayroll),
#                                .(mpayroll_promedio = mean(mpayroll, na.rm = TRUE),
#                                  mpayroll_mediana = median(mpayroll, na.rm = TRUE),
#                                  n_clientes = .N),
#                                by = foto_mes][order(foto_mes)]
# 
# # Gráfico
# library(ggplot2)
# ggplot(evolucion_mpayroll, aes(x = foto_mes, y = mpayroll_promedio)) +
#   geom_line(color = "blue", size = 1.2) +
#   geom_point(color = "blue", size = 3) +
#   geom_line(aes(y = mpayroll_mediana), color = "red", linetype = "dashed") +
#   geom_point(aes(y = mpayroll_mediana), color = "red", size = 2) +
#   labs(title = "Evolución de mpayroll - Clientes con aguinaldo en 202106",
#        subtitle = "Línea azul: promedio | Línea roja: mediana",
#        x = "Mes",
#        y = "mpayroll") +
#   theme_minimal() +
#   theme(axis.text.x = element_text(angle = 45, hjust = 1))
# 
# # Ver los números
# print(evolucion_mpayroll)
```


### mpayroll_normalizado 202106

mpayroll normalizado representa cuánto tendrían en sus cuentas si hubieran mantenido el nivel de mayo. Se calcula restando del saldo de junio el delta positivo respecto a mayo (pmax(saldo_junio - saldo_mayo, 0)), asumiendo que ese incremento corresponde al aguinaldo depositado y no gastado inmediatamente. Para clientes sin aguinaldo o en otros meses, los saldos normalizados son idénticos a los originales. Y le sumé 3% inflación.


```{r }
# 
# 
# dataset[, mpayroll_normalizado := mpayroll]
# 
# dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
#         aguinaldo_estimado := pmax(mpayroll_delta1, 0)] #pmax evito negativos
# 
# # Se resta el aguinaldo estimado y luego se multiplica el resultado
# # por 1.03 para sumar el 3,2% de inflación.
# dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
#         mpayroll_normalizado := (mpayroll - aguinaldo_estimado) * 1.032]

```



```{r}
# # Ver algunos casos individuales
# dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
#         .(mpayroll,
#           mpayroll_lag1,
#           aguinaldo_estimado,
#           mpayroll_normalizado,
#           diferencia = mpayroll - mpayroll_normalizado)][1:10]
```

```{r}
# 
# library(ggplot2)
# 
# # 1. IDENTIFICAR CLIENTES CON AGUINALDO
# clientes_con_aguinaldo <- dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
#                                    unique(numero_de_cliente)]
# 
# # 2. ASIGNAR LA COLUMNA 'es_empleado' AL DATASET PRINCIPAL
# # Se asigna a todos los registros del cliente (todos los meses)
# dataset[, es_empleado := numero_de_cliente %in% clientes_con_aguinaldo]
# 
# # 3. CALCULAR LA EVOLUCIÓN: Promedios agrupados por mes y por 'es_empleado'
# evolucion <- dataset[,
#                      .(mpayroll_prom = mean(mpayroll, na.rm = TRUE),
#                        mpayroll_norm_prom = mean(mpayroll_normalizado, na.rm = TRUE)
#                        ),
#                      by = .(foto_mes, es_empleado)] # Agrupación por la nueva columna
# 
# # --- GRÁFICO 0: mpayroll (CORRECCIÓN DE COLORES) ---
# p0 <- ggplot(evolucion, aes(x = foto_mes, group = es_empleado)) +
#   geom_line(aes(y = mpayroll_prom, color = factor(es_empleado)), size = 1) +
#   geom_line(aes(y = mpayroll_norm_prom, color = factor(es_empleado)),
#             linetype = "dashed", size = 1) +
#   geom_point(aes(y = mpayroll_prom, color = factor(es_empleado))) +
#   # 🟢 CORRECCIÓN: Se usa "FALSE" y "TRUE" como claves para mapear el factor lógico
#   scale_color_manual(values = c("FALSE" = "#E74C3C", "TRUE" = "#3498DB"),
#                      labels = c("Sin aguinaldo", "Con aguinaldo")) +
#   labs(title = paste0("Evolución mpayroll - EXP ", PARAM$experimento),
#        subtitle = "Línea sólida: original | Línea punteada: normalizada",
#        x = "Período", y = "Saldo Promedio", color = "Grupo") +
#   theme_minimal()
# 
# print(p0)
# 
# # Ver los datos de empleados en junio
# evolucion[es_empleado == TRUE, 
#           .(foto_mes,
#             mpayroll_original = round(mpayroll_prom, 0),
#             mpayroll_normalizado = round(mpayroll_norm_prom, 0),
#             diferencia = round(mpayroll_prom - mpayroll_norm_prom, 0))]
# 
# # --- LIMPIEZA ---
# 
# # Eliminar columna auxiliar 'es_empleado' para mantener el dataset limpio
# dataset[, es_empleado := NULL]

```


### recibio_aguinaldo backwards
```{r}
# 
# # PROPAGO RECIBIO AGUINALDO HACIA ATRAS CORREGIDO
# 
# # PROPAGO RECIBIO AGUINALDO HACIA ATRAS
# clientes_en_202106 <- dataset[foto_mes == 202106, unique(numero_de_cliente)]
# 
# # Clientes NO en 202106 → poner 0 en todos los meses
# dataset[!(numero_de_cliente %in% clientes_en_202106), recibio_aguinaldo := 0L]
# 
# # Clientes con 1 en 202106 → poner 1 en todos los meses
# clientes_con_1 <- dataset[foto_mes == 202106 & recibio_aguinaldo == 1, numero_de_cliente]
# dataset[numero_de_cliente %in% clientes_con_1, recibio_aguinaldo := 1]
# 
# # Clientes con 0 en 202106 → poner 0 en todos los meses
# clientes_con_0 <- dataset[foto_mes == 202106 & recibio_aguinaldo == 0, numero_de_cliente]
# dataset[numero_de_cliente %in% clientes_con_0, recibio_aguinaldo := 0]


```


```{r}
# # analisis de recibio aguinaldo Contar únicos y registros por mes
# dataset[, .(
#   valores_unicos = paste(sort(unique(recibio_aguinaldo)), collapse = ", "),
#   n_con_1 = sum(recibio_aguinaldo == 1, na.rm = TRUE),
#   n_con_0 = sum(recibio_aguinaldo == 0, na.rm = TRUE),
#   n_con_NA = sum(is.na(recibio_aguinaldo)),
#   total_registros = .N
# ), by = foto_mes][order(foto_mes)]

```


### mpayroll normalizado backwards
```{r}
# # ==============================================================================
# # PARCHE: EXTENDER LÓGICA DE AGUINALDO A MESES DE TRAIN CON AJUSTE POR INFLACIÓN
# # Este bloque aplica el cálculo de aguinaldo_estimado y mpayroll_normalizado
# # a todos los meses != 202106 para clientes con recibio_aguinaldo == 1
# # 
# # INFLACIÓN MENSUAL 2021 (ajustable):
# # Enero: 4.1%, Febrero: 3.6%, Marzo: 4.8%, Abril: 4.1%, Mayo: 3.3%, Junio: 3.2%
# #
# # Para deshacer: simplemente comentar o eliminar este bloque completo
# # ==============================================================================
# 
# # Factores de inflación mensual (ajustables)
# inflacion_mensual <- data.table(
#   foto_mes = c(202101, 202102, 202103, 202104, 202105, 202106),
#   inflacion = c(0.041, 0.036, 0.048, 0.041, 0.033, 0.032)
# )
# 
# # Calcular inflación acumulada desde cada mes hasta junio
# # Enero a Junio: (1+0.036)*(1+0.048)*(1+0.041)*(1+0.033)*(1+0.032) = 1.205
# # Febrero a Junio: (1+0.048)*(1+0.041)*(1+0.033)*(1+0.032) = 1.162
# # Marzo a Junio: (1+0.041)*(1+0.033)*(1+0.032) = 1.109
# # Abril a Junio: (1+0.033)*(1+0.032) = 1.066
# # Mayo a Junio: (1+0.032) = 1.032
# # Junio: 1.0 (sin ajuste)
# 
# inflacion_acumulada <- data.table(
#   foto_mes = c(202101, 202102, 202103, 202104, 202105, 202106),
#   factor_deflactor = c(1.205, 1.162, 1.109, 1.066, 1.032, 1.0)
# )
# 
# # Guardar delta1 de 202106 por cliente para usar en otros meses
# delta1_junio <- dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
#                         .(numero_de_cliente, delta1_junio = mpayroll_delta1)]
# 
# # Mergear ese delta a todo el dataset
# dataset <- merge(dataset, delta1_junio, by = "numero_de_cliente", all.x = TRUE)
# 
# # Mergear factores de inflación
# dataset <- merge(dataset, inflacion_acumulada, by = "foto_mes", all.x = TRUE)
# 
# # Calcular aguinaldo_estimado en meses != 202106 usando delta de junio DEFLACTADO
# dataset[foto_mes != 202106 & recibio_aguinaldo == 1 & !is.na(delta1_junio),
#         aguinaldo_estimado := pmax(delta1_junio / factor_deflactor, 0)]
# 
# # Calcular mpayroll_normalizado en meses != 202106 (SIN el *1.03)
# dataset[foto_mes != 202106 & recibio_aguinaldo == 1 & !is.na(aguinaldo_estimado),
#         mpayroll_normalizado := pmax(mpayroll - aguinaldo_estimado, 0)]
# 
# # Limpiar columnas temporales
# dataset[, c("delta1_junio", "factor_deflactor") := NULL]
# 
# cat("\n=== PARCHE CON AJUSTE POR INFLACIÓN APLICADO ===\n")
# cat("Factores de deflactación usados:\n")
# print(inflacion_acumulada)
# cat("\nVerificación de mpayroll_normalizado por mes (clientes con aguinaldo=1):\n")
# print(dataset[recibio_aguinaldo == 1, 
#               .(n = .N,
#                 promedio_mpayroll = mean(mpayroll, na.rm = TRUE),
#                 promedio_normalizado = mean(mpayroll_normalizado, na.rm = TRUE),
#                 n_ceros = sum(mpayroll_normalizado == 0, na.rm = TRUE)),
#               by = foto_mes][order(foto_mes)])
# 
# # ==============================================================================
# # FIN DEL PARCHE
# # ==============================================================================
```


#### fwrite

```{r chunk23}

# fwrite(dataset,
#        file = "/content/buckets/b1/datasets/competencia_01_fe1yag.csv.gz",
#        sep = ",")
# 
fwrite(dataset,
       file = "competencia_02_nan_rankcont_concont.csv.gz",
       sep = ",")
# 
# dim(dataset)

# 225 (sin prestpers, con continuas, con rankings)


```

```{r}
#rstudio local
# dataset <- fread("C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia_02/competencia_02_nan_rankcont_concont.csv.gz", stringsAsFactors= TRUE)

```

```{r}
dim(dataset)
```


## DROP CONTINUAS

```{r}

dataset[, (variables_continuas) := NULL]
dim(dataset)

```


```{r}
colnames(dataset)
```

## LAGS Y DELTAS

```{r chunk6}

# Creacion de LAGs
setorder(dataset, numero_de_cliente, foto_mes)

# todo es lagueable, menos la primary key y la clase
cols_lagueables <- copy( setdiff(
    colnames(dataset),
    c("numero_de_cliente", "foto_mes", "clase_ternaria")
) )

# https://rdrr.io/cran/data.table/man/shift.html

# lags de orden 1
dataset[,
    paste0(cols_lagueables, "_lag1") := shift(.SD, 1, NA, "lag"),
    by = numero_de_cliente,
    .SDcols = cols_lagueables
]

# lags de orden 2
dataset[,
    paste0(cols_lagueables, "_lag2") := shift(.SD, 2, NA, "lag"),
    by = numero_de_cliente,
    .SDcols = cols_lagueables
]

# agrego los delta lags
for (vcol in cols_lagueables)
{
    dataset[, paste0(vcol, "_delta1") := get(vcol) - get(paste0(vcol, "_lag1"))]
    dataset[, paste0(vcol, "_delta2") := get(vcol) - get(paste0(vcol, "_lag2"))]
}



```

```{r chunk20}
total_lags <- length(grep("_lag[0-9]+$", names(dataset)))
total_deltas <- length(grep("_delta[0-9]+$", names(dataset)))
total_lags
total_deltas

```

#### fwrite
```{r}
# fwrite(dataset,
#        file = "competencia_02_nan_rankcont_sincont_ld12.csv.gz",
#        sep = ",")

# 753 (sin prestpers, sin continuas, con rankings, con lags y deltas 1 y 2)

```

```{r chunk21}
dim(dataset)
# 753 (sin prestpers, sin continuas, con rankings, con lags y deltas 1 y 2)
```

### levanto dataset
```{r}

# dataset <- fread("~/datasets/competencia_01_fe1.csv.gz", stringsAsFactors= TRUE)

dataset <- fread("C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia_02/competencia_02_nan_rankcont_sincont_ld12.csv.gz", stringsAsFactors= TRUE)

# dataset <- fread("https://storage.googleapis.com/silvanacontreras76_bukito3/datasets/competencia_02_nan_rankcont_sincont_ld12.csv.gz", stringsAsFactors = TRUE)

```


```{r chunk25}
dataset[, .N, foto_mes]
# dataset[foto_mes == 202101, .N, clase_ternaria]
```

```{r chunk26}
# colnames(dataset)
```


# OPTIMIZACION Hiperparámetros

limpio el ambiente de R

```{r chunk28}
format(Sys.time(), "%a %b %d %X %Y")
```

```{r chunk29}
# limpio la memoria
rm(list=ls(all.names=TRUE)) # remove all objects
gc(full=TRUE, verbose=FALSE) # garbage collection
```

##Carga de Librerias

```{r chunk30}
# cargo las librerias que necesito
require("data.table")
require("parallel")

if(!require("R.utils")) install.packages("R.utils")
require("R.utils")

if( !require("primes") ) install.packages("primes")
require("primes")

if( !require("utils") ) install.packages("utils")
require("utils")

if( !require("rlist") ) install.packages("rlist")
require("rlist")

if( !require("yaml")) install.packages("yaml")
require("yaml")

if( !require("lightgbm") ) install.packages("lightgbm")
require("lightgbm")

if( !require("DiceKriging") ) install.packages("DiceKriging")
require("DiceKriging")

if( !require("mlrMBO") ) install.packages("mlrMBO")
require("mlrMBO")
```

# nro EXP y seeds

```{r chunk31}
PARAM <- list()
PARAM$experimento <- 9100

PARAM$semilla_primigenia <- 999199
PARAM$semillas <- c(999199, 999499, 999599, 999959, 999979
                 , 104729, 523987
                    , 7919,1299709, 2097593
)

```

# períodos
```{r chunk33}
# training y future
PARAM$train <- c(202103)
PARAM$train_final <- c(202101, 202102, 202103, 202104)
PARAM$future <- c(202108)
PARAM$false_future <- c(202104)
PARAM$semilla_kaggle <- 314159
PARAM$cortes <- seq(7000, 16000, by= 500)
```

# undersampling
```{r chunk34}
# un undersampling de 0.1  toma solo el 10% de los CONTINUA
# undersampling de 1.0  implica tomar TODOS los datos

PARAM$trainingstrategy$undersampling <- 0.2
```

# HIPERPARAMETROS
```{r chunk35}
# Parametros LightGBM

PARAM$hyperparametertuning$xval_folds <- 5

# parametros fijos del LightGBM que se pisaran con la parte variable de la BO
PARAM$lgbm$param_fijos <-  list(
  boosting= "gbdt", # puede ir  dart  , ni pruebe random_forest
  # boosting= "dart", #ATENCION MODIFIQUE
  objective= "binary",
  metric= "auc",
  first_metric_only= FALSE,
  boost_from_average= TRUE,
  feature_pre_filter= FALSE,
  force_row_wise= TRUE, # para reducir warnings
  verbosity= -100,

  seed= PARAM$semilla_primigenia,

  max_depth= -1L, # -1 significa no limitar,  por ahora lo dejo fijo
  min_gain_to_split= 0, # min_gain_to_split >= 0
  min_sum_hessian_in_leaf= 0.001, #  min_sum_hessian_in_leaf >= 0.0
  lambda_l1= 0.0, # lambda_l1 >= 0.0
  lambda_l2= 0.0, # lambda_l2 >= 0.0
  max_bin= 31L, # lo debo dejar fijo, no participa de la BO

  bagging_fraction= 1.0, # 0.0 < bagging_fraction <= 1.0
  pos_bagging_fraction= 1.0, # 0.0 < pos_bagging_fraction <= 1.0
  neg_bagging_fraction= 1.0, # 0.0 < neg_bagging_fraction <= 1.0
  is_unbalance= FALSE, #
  scale_pos_weight= 1.0, # scale_pos_weight > 0.0

  drop_rate= 0.1, # 0.0 < neg_bagging_fraction <= 1.0
  max_drop= 50, # <=0 means no limit
  skip_drop= 0.5, # 0.0 <= skip_drop <= 1.0

  extra_trees= FALSE,

  num_iterations= 1200,
  learning_rate= 0.02,
  feature_fraction= 0.5,
  num_leaves= 750,
  min_data_in_leaf= 5000
)
```

Aqui se definen los hiperparámetros de LightGBM que participan de la Bayesian Optimization
<br> si es un numero entero debe ir  makeIntegerParam
<br> si es un numero real (con decimales) debe ir  makeNumericParam
<br> es muy importante leer cuales son un lower y upper  permitidos y ademas razonables
```{r}
# Nuevos basados en notebook

PARAM$hyperparametertuning$hs <- makeParamSet(
  makeIntegerParam("num_iterations", lower= 50L, upper= 2048L),
  makeNumericParam("learning_rate", lower= 0.008, upper= 0.2),
  makeNumericParam("feature_fraction", lower= 0.2, upper= 1.0),
  makeIntegerParam("num_leaves", lower= 8L, upper= 2048L),
  makeIntegerParam("min_data_in_leaf", lower= 3L, upper= 8000L)
)
```


```{r}
#EXP 9022

# PARAM$hyperparametertuning$hs <- makeParamSet(
#   makeIntegerParam("num_iterations", lower= 250L, upper= 5000L),
#   # o más con early stopping al entrenar
#   makeNumericParam("learning_rate", lower= 0.005, upper= 0.1),
#   makeNumericParam("feature_fraction", lower= 0.2, upper= 0.99),
#   makeIntegerParam("num_leaves", lower= 64L, upper= 1000L),
#   makeIntegerParam("min_data_in_leaf", lower= 2L, upper= 5L), #ATENCION
# 
#   makeIntegerParam("max_depth", lower = 3, upper= 26),
#   makeNumericParam("min_gain_to_split", lower = 0.0, upper = 0.4),
#   #makeNumericParam("min_sum_hessian_in_leaf", lower = 0.001, upper = 0.1),
#   makeNumericParam("lambda_l1", lower = 0.0, upper = 5.0),
#   makeNumericParam("lambda_l2", lower = 0.0, upper = 10.0),
#   #max_bin= 31L, # lo debo dejar fijo, no participa de la BO
# 
#   makeNumericParam("bagging_fraction", lower = 0.2, upper = 0.99),
#   makeIntegerParam("bagging_freq", lower = 1L, upper = 7L) #1 para que ocurra
#   #makeNumericParam("pos_bagging_fraction", lower = 0.5, upper = 1.0),
#   #makeNumericParam("neg_bagging_fraction", lower = 0.5, upper = 1.0),
#   #makeNumericParam("scale_pos_weight", lower = 0.5, upper = 1.0),
# 
# #son de dart
#   # makeNumericParam("drop_rate", lower = 0.0, upper = 0.3),
#   # makeIntegerParam("max_drop", lower = 0L, upper = 50L),
#   # makeNumericParam("skip_drop", lower = 0.1, upper = 0.5)
# )

```


A mayor cantidad de hiperparámetros, se debe aumentar las iteraciones de la Bayesian Optimization
<br> 30 es un valor muy tacaño, pero corre rápido
<br> deberia partir de 50, alcanzando los 100 si se dispone de tiempo

# ht iteraciones
```{r chunk41}

PARAM$hyperparametertuning$iteraciones <- 45# iteraciones bayesianas

```

# carpeta HT

```{r chunk42}

# carpeta de trabajo

# setwd("/content/buckets/b1/exp")
# experimento_folder <- paste0("HT", PARAM$experimento)
# dir.create(experimento_folder, showWarnings=FALSE)
# setwd( paste0("/content/buckets/b1/exp/", experimento_folder ))

#PARA R
# Definir directorio base
dir_base <- "C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia_02"
dir.create(dir_base, showWarnings = FALSE, recursive = TRUE)
# # Crear directorio exp
experimento <- paste0("exp", PARAM$experimento)
dir_experimento <- file.path(dir_base, experimento)
dir.create(dir_experimento, showWarnings = FALSE)
# # Crear directorio HT dentro de exp
HT <- paste0("HT", PARAM$experimento)
dir_HT <- file.path(dir_experimento, HT)
dir.create(dir_HT, showWarnings = FALSE)
setwd(dir_HT)
getwd()

```


## carga dataset
```{r chunk43}

# competencia_02_nan_rankcont_sincont_ld12

dataset <- fread("https://storage.googleapis.com/silvanacontreras76_bukito3/datasets/competencia_02_nan_rankcont_sincont_ld12.csv.gz", stringsAsFactors= TRUE)
#1067

```

```{r chunk44}

dim(dataset)
```


### dataset_train foto mes y clase
```{r chunk47}

dataset_train <- dataset[foto_mes %in% PARAM$train]
```

```{r chunk48}

# paso la clase a binaria que tome valores {0,1}  enteros
#  BAJA+1 y BAJA+2  son  1,   CONTINUA es 0
#  a partir de ahora ya NO puedo cortar  por prob(BAJA+2) > 1/40

dataset_train[,
  clase01 := ifelse(clase_ternaria %in% c("BAJA+2","BAJA+1"), 1L, 0L)
]
```

```{r chunk49}

# defino los datos que forma parte del training
# aqui se hace el undersampling de los CONTINUA
# notar que para esto utilizo la SEGUNDA semilla

set.seed(PARAM$semilla_primigenia, kind = "L'Ecuyer-CMRG")
dataset_train[, azar := runif(nrow(dataset_train))]
dataset_train[, training := 0L]

dataset_train[
  foto_mes %in%  PARAM$train &
    (azar <= PARAM$trainingstrategy$undersampling | clase_ternaria %in% c("BAJA+1", "BAJA+2")),
  training := 1L
]
```

```{r chunk50}

# los campos que se van a utilizar

campos_buenos <- setdiff(
  colnames(dataset_train),
  c("clase_ternaria", "clase01", "azar", "training")
)
```

##dtrain
```{r chunk51}

# dejo los datos en el formato que necesita LightGBM

dtrain <- lgb.Dataset(
  data= data.matrix(dataset_train[training == 1L, campos_buenos, with= FALSE]),
  label= dataset_train[training == 1L, clase01],
  free_raw_data= FALSE
)

nrow(dtrain)
ncol(dtrain)
```

## Configuracion Bayesian Optimization
```{r chunk52}

# En el argumento x llegan los parámetros de la bayesiana
#  devuelve la AUC en cross validation del modelo entrenado

EstimarGanancia_AUC_lightgbm <- function(x) {

  # x pisa (o agrega) a param_fijos
  param_completo <- modifyList(PARAM$lgbm$param_fijos, x)

  # entreno LightGBM
  modelocv <- lgb.cv(
    data= dtrain,
    nfold= PARAM$hyperparametertuning$xval_folds,
    stratified= TRUE,
    param= param_completo
  )

  # obtengo la ganancia
  AUC <- modelocv$best_score

  # hago espacio en la memoria
  rm(modelocv)
  gc(full= TRUE, verbose= FALSE)

  message(format(Sys.time(), "%a %b %d %X %Y"), " AUC ", AUC)

  return(AUC)
}
```


```{r chunk53}

# Aqui comienza la configuracion de la Bayesian Optimization
# en este archivo quedan la evolucion binaria de la BO

#para Rstudio
setwd(dir_HT)

#para VM
kbayesiana <- "bayesiana.RDATA"

funcion_optimizar <- EstimarGanancia_AUC_lightgbm # la funcion que voy a maximizar

configureMlr(show.learner.output= FALSE)

# configuro la busqueda bayesiana,  los hiperparametros que se van a optimizar
# por favor, no desesperarse por lo complejo

obj.fun <- makeSingleObjectiveFunction(
  fn= funcion_optimizar, # la funcion que voy a maximizar
  minimize= FALSE, # estoy Maximizando la ganancia
  noisy= TRUE,
  par.set= PARAM$hyperparametertuning$hs, # definido al comienzo del programa
  has.simple.signature= FALSE # paso los parametros en una lista
)

# cada 600 segundos guardo el resultado intermedio
ctrl <- makeMBOControl(
  save.on.disk.at.time= 600, # se graba cada 600 segundos
  save.file.path= kbayesiana
) # se graba cada 600 segundos

# indico la cantidad de iteraciones que va a tener la Bayesian Optimization
ctrl <- setMBOControlTermination(
  ctrl,
  iters= PARAM$hyperparametertuning$iteraciones
) # cantidad de iteraciones

# defino el método estandar para la creacion de los puntos iniciales,
# los "No Inteligentes"
ctrl <- setMBOControlInfill(ctrl, crit= makeMBOInfillCritEI())

# establezco la funcion que busca el maximo
surr.km <- makeLearner(
  "regr.km",
  predict.type= "se",
  covtype= "matern3_2",
  control= list(trace= TRUE)
)
```

## Corrida Bayesian Optimization
```{r chunk54}

 # inicio la optimizacion bayesiana, retomando si ya existe
# es la celda mas lenta de todo el notebook

if (!file.exists(kbayesiana)) {
  bayesiana_salida <- mbo(obj.fun, learner= surr.km, control= ctrl)
} else {
  bayesiana_salida <- mboContinue(kbayesiana) # retomo en caso que ya exista
}
```

```{r chunk55}

print(bayesiana_salida$x)  # Mejores hiperparámetros
print(bayesiana_salida$y)  # Mejor AUC
```

## levanto proceso hasta donde llegó (si lo corté)
```{r chunk56}

# # levantar proceso interrumpido sin continuar

# load(kbayesiana)  # carga 'opt.state' en el entorno

# # armo un contenedor con el mismo campo que usaba
# bayesiana_salida <- list(opt.path = opt.state$opt.path)

# print(bayesiana_salida$x)  # Mejores hiperparámetros
# print(bayesiana_salida$y)  # Mejor AUC
```

## bayesiana_salida guarda
```{r chunk57}

tb_bayesiana <- as.data.table(bayesiana_salida$opt.path)
colnames( tb_bayesiana)

```

```{r chunk58}

setwd(dir_experimento)

# almaceno los resultados de la Bayesian Optimization
# y capturo los mejores hiperparametros encontrados

tb_bayesiana <- as.data.table(bayesiana_salida$opt.path) #(ya está en la celda anterior)

tb_bayesiana[, iter := .I]

# ordeno en forma descendente por AUC = y
setorder(tb_bayesiana, -y)

# grabo para eventualmente poder utilizarlos en OTRA corrida



fwrite( tb_bayesiana,
  file= paste0("BO_log_", PARAM$experimento, ".txt"),
  sep= "\t"
)


# los mejores hiperparámetros son los que quedaron en el registro 1 de la tabla
PARAM$out$lgbm$mejores_hiperparametros <- tb_bayesiana[
  1, # el primero es el de mejor AUC
  setdiff(colnames(tb_bayesiana),
    c("y","dob","eol","error.message","exec.time","ei","error.model",
      "train.time","prop.type","propose.time","se","mean","iter")),
  with= FALSE
]


PARAM$out$lgbm$y <- tb_bayesiana[1, y]
```

```{r chunk59}

write_yaml( PARAM, file="PARAM.yml")
```

```{r chunk60}

print(PARAM$out$lgbm$mejores_hiperparametros)
print(PARAM$out$lgbm$y)
```

# PRODUCCION
Construyo el modelo final, que es uno solo, no hace ningun tipo de particion < training, validation, testing>]


limpio el ambiente de R

```{r chunk28}
format(Sys.time(), "%a %b %d %X %Y")
```

```{r chunk29}
# limpio la memoria
rm(list=ls(all.names=TRUE)) # remove all objects
gc(full=TRUE, verbose=FALSE) # garbage collection
```

##Carga de Librerias

```{r chunk30}
# cargo las librerias que necesito
require("data.table")
require("parallel")

if(!require("R.utils")) install.packages("R.utils")
require("R.utils")

if( !require("primes") ) install.packages("primes")
require("primes")

if( !require("utils") ) install.packages("utils")
require("utils")

if( !require("rlist") ) install.packages("rlist")
require("rlist")

if( !require("yaml")) install.packages("yaml")
require("yaml")

if( !require("lightgbm") ) install.packages("lightgbm")
require("lightgbm")

if( !require("DiceKriging") ) install.packages("DiceKriging")
require("DiceKriging")

if( !require("mlrMBO") ) install.packages("mlrMBO")
require("mlrMBO")
```

# nro EXP y seeds

```{r chunk31}
PARAM <- list()
PARAM$experimento <- 9100

PARAM$semilla_primigenia <- 999199
PARAM$semillas <- c(999199, 999499, 999599, 999959, 999979
                    , 104729, 523987
                    , 7919,1299709, 2097593
)

```


# períodos
```{r chunk33}
# training y future
PARAM$train <- c(202103)
PARAM$train_final <- c(202101, 202102, 202103, 202104)
PARAM$future <- c(202108)
PARAM$false_future <- c(202104)
PARAM$semilla_kaggle <- 314159
PARAM$cortes <- seq(7000, 16000, by= 500)
```

## carpeta EXP 
```{r chunk64}
# 
# setwd("/content/buckets/b1/exp")
# experimento <- paste0("exp", PARAM$experimento)
# dir.create(experimento, showWarnings= FALSE)
# setwd( paste0("/content/buckets/b1/exp/", experimento ))

#PARA R
# Definir directorio base
dir_base <- "C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia_02"
dir.create(dir_base, showWarnings = FALSE, recursive = TRUE)
# # Crear directorio exp
experimento <- paste0("exp", PARAM$experimento)
dir_experimento <- file.path(dir_base, experimento)
dir.create(dir_experimento, showWarnings = FALSE)

```

## Final Training Dataset

Aqui esta la gran decision de en qué meses hago el Final Training
<br> debo utilizar los mejores hiperparámetros que encontré en la  optimización bayesiana
```{r chunk65}

options(timeout = 500)
dataset <- fread("https://storage.googleapis.com/silvanacontreras76_bukito3/datasets/competencia_02_nan_rankcont_sincont_ld12.csv.gz", stringsAsFactors= TRUE)


```


```{r}
cat("VERIFICACIÓN POST-CARGA\n\n")

#fe123 = 1067
#fe123_rankshort = 1078
#fe123_rankshort_agui = 1081
#fe123_rank = 1140
#fe123_rank_agui = 1143

cat("Dimensiones totales:", dim(dataset), "\n\n")

cat("Distribución por foto_mes:\n")
print(dataset[, .N, by = foto_mes][order(foto_mes)])

cat("\nPERÍODO 202106\n")
cat("Filas en 202106:", nrow(dataset[foto_mes == 202106]), "\n")
cat("Esperado: 164313\n")

if(nrow(dataset[foto_mes == 202106]) != 164313) {
  cat("\nPROBLEMA AL CARGAR\n")
  cat("Diferencia:", 164313 - nrow(dataset[foto_mes == 202106]), "registros faltantes\n")
  cat("El archivo competencia_01_fe.csv ya tiene el problema\n")
  cat("Revisar cómo se generó el feature engineering\n")
} else {
  cat("\nCarga correcta - 202106 OK\n")
}
```

```{r chunk67}
dim(dataset) #1067
```

##  mpayroll 202106


```{r}
# DETECCIÓN DE QUIEN RECIBIÓ AGUINALDO
# dataset[, recibio_aguinaldo := 0L]
# 
# dataset[foto_mes == 202106, recibio_aguinaldo := ifelse(
#   cpayroll_trx_lag2 > 0 & #sueldo en abril
#   cpayroll_trx_lag1 > 0 & #sueldo en abril
#   cpayroll_trx > 0 & #sueldo en junio
#   mpayroll > 1.4 * mpayroll_lag1 & #sueldo junio 40% mayor que abril
#   !is.na(mpayroll) & !is.na(mpayroll_lag2) & #sueldo abril y junio no NA
#   mpayroll > 0 & mpayroll_lag1 > 0 & mpayroll_lag2 > 0, #sueldo abril y junio positivo
#   1L, 0L
# )]
# 
# cat("\nDISTRIBUCIÓN DE 'recibio_aguinaldo' en 202106:\n")
# print(dataset[foto_mes == 202106, .N, by = recibio_aguinaldo])
# 
# 
# cat("\nDISTRIBUCIÓN cpayroll_trx:\n")
# print(dataset[foto_mes == 202106 & recibio_aguinaldo == 1, .N, by = cpayroll_trx])

```



```{r }


# dataset[, mpayroll_normalizado := mpayroll]
# 
# dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
#         aguinaldo_estimado := pmax(mpayroll_delta1, 0)] #pmax evito negativos
# 
# # Se resta el aguinaldo estimado y luego se multiplica el resultado
# # por 1.03 para sumar el 3,2% de inflación.
# dataset[foto_mes == 202106 & recibio_aguinaldo == 1,
#         mpayroll_normalizado := (mpayroll - aguinaldo_estimado) * 1.032]

```

```{r}
# # Asignar el valor normalizado a mpayroll solo para clientes de 202106
# dataset[foto_mes == 202106, mpayroll := mpayroll_normalizado]
# 
# # Eliminar las columnas auxiliares
# dataset[, c("recibio_aguinaldo", "aguinaldo_estimado", "mpayroll_normalizado") := NULL]
```

```{r}
dim(dataset)
```


## clase01
```{r chunk68}
# clase01
dataset[, clase01 := ifelse(clase_ternaria %in% c("BAJA+1", "BAJA+2"), 1L, 0L)]
```

```{r}
dataset[, unique(foto_mes)]
print(PARAM$train_final)

```

##dataset_train final
```{r chunk69}

dataset_train <- dataset[foto_mes %in% PARAM$train_final]
dataset_train[,.N,clase_ternaria]

```

```{r chunk70}
# los campos que se van a utilizar

campos_buenos <- setdiff(
  colnames(dataset_train),
  c("clase_ternaria", "clase01", "azar", "training")
)
```

##dtrain_final
```{r chunk71}
# dejo los datos en el formato que necesita LightGBM

dtrain_final <- lgb.Dataset(
  data= data.matrix(dataset_train[, campos_buenos, with= FALSE]),
  label= dataset_train[, clase01]
)
```

##Final Training Hyperparameters


### undersampling
```{r chunk34}
# un undersampling de 0.1  toma solo el 10% de los CONTINUA
# undersampling de 1.0  implica tomar TODOS los datos

PARAM$trainingstrategy$undersampling <- 0.2

```

```{r chunk72}
# Parametros LightGBM

PARAM$hyperparametertuning$xval_folds <- 5

# parametros fijos del LightGBM que se pisaran con la parte variable de la BO
PARAM$lgbm$param_fijos <-  list(
  boosting= "gbdt", # puede ir  dart  , ni pruebe random_forest
  # boosting= "dart", #ATENCION MODIFIQUE
  objective= "binary",
  metric= "auc",
  first_metric_only= FALSE,
  boost_from_average= TRUE,
  feature_pre_filter= FALSE,
  force_row_wise= TRUE, # para reducir warnings
  verbosity= -100,

  seed= PARAM$semilla_primigenia,

  max_depth= -1L, # -1 significa no limitar,  por ahora lo dejo fijo
  min_gain_to_split= 0, # min_gain_to_split >= 0
  min_sum_hessian_in_leaf= 0.001, #  min_sum_hessian_in_leaf >= 0.0
  lambda_l1= 0.0, # lambda_l1 >= 0.0
  lambda_l2= 0.0, # lambda_l2 >= 0.0
  max_bin= 31L, # lo debo dejar fijo, no participa de la BO

  bagging_fraction= 1.0, # 0.0 < bagging_fraction <= 1.0
  pos_bagging_fraction= 1.0, # 0.0 < pos_bagging_fraction <= 1.0
  neg_bagging_fraction= 1.0, # 0.0 < neg_bagging_fraction <= 1.0
  is_unbalance= FALSE, #
  scale_pos_weight= 1.0, # scale_pos_weight > 0.0

  drop_rate= 0.1, # 0.0 < neg_bagging_fraction <= 1.0
  max_drop= 50, # <=0 means no limit
  skip_drop= 0.5, # 0.0 <= skip_drop <= 1.0

  extra_trees= FALSE,

  num_iterations= 1200,
  learning_rate= 0.02,
  feature_fraction= 0.5,
  num_leaves= 750,
  min_data_in_leaf= 5000
)
```

```{r}

#EXP 9023. AUC = 0,9293
 PARAM$out$lgbm$mejores_hiperparametros <- list(
  num_iterations = 3153,
  learning_rate = 0.02368302,
  feature_fraction = 0.3827037,
  num_leaves = 705,
  min_data_in_leaf = 4,
  max_depth = 6,
  min_gain_to_split = 0.2781798,
  lambda_l1 = 2.116463,
  lambda_l2 = 7.013903,
  bagging_fraction = 0.9107902,
  bagging_freq = 5
)
```

```{r}

# # EXP 9022 AUC 0.9293
# 
# PARAM$out$lgbm$mejores_hiperparametros <- list(
#   num_iterations = 3625,
#   learning_rate = 0.005911834,
#   feature_fraction = 0.2175392,
#   num_leaves = 774,
#   min_data_in_leaf = 3,
#   max_depth = 15,
#   min_gain_to_split = 0.2578446,
#   lambda_l1 = 1.472948,
#   lambda_l2 = 4.598443,
#   bagging_fraction = 0.8635728,
#   bagging_freq = 1
# )
```

```{r}

# # 9015 bayesiana #16 so far AUC = 0.9482
# 
# PARAM$out$lgbm$mejores_hiperparametros <- list(
#    num_iterations = 3091,
#    learning_rate = 0.0293,
#    feature_fraction = 0.553,
#    num_leaves = 209,
#    min_data_in_leaf = 3,
#    max_depth = 20,
#    min_gain_to_split = 0.00304,
#    lambda_l1 = 0.00961,
#    lambda_l2 = 0.215,
#    bagging_fraction = 0.867,
#    bagging_freq = 3
# )
```

```{r chunk61}
# EXP HT 4940
# Envios=9000	 TOTAL=353600000  Public=335333333 Private=361428571

# EXP 4940

# mejores_hiperparametros_ht4940 <- list(
#   num_iterations = 1085,
#   learning_rate = 0.0100625,
#   feature_fraction = 0.5160196,
#   num_leaves = 1838,
#   min_data_in_leaf = 1309
# )

# param_final <- modifyList(PARAM$lgbm$param_fijos, mejores_hiperparametros_ht4940)

```

```{r}

# EXP 9016 #16 so far auc = 0.9417
# PARAM$out$lgbm$mejores_hiperparametros <- list(
#   num_iterations = 2019,
#   learning_rate = 0.00825,
#   feature_fraction = 0.674,
#   num_leaves = 200,
#   min_data_in_leaf = 3,
#   max_depth = 19,
#   min_gain_to_split = 0.118,
#   lambda_l1 = 0.441,
#   lambda_l2 = 0.307,
#   bagging_fraction = 0.597,
#   bagging_freq = 3
# )
```

```{r chunk62}
# EXP HT 4941

# mejores_hiperparametros:
#       num_iterations: 962
#       learning_rate: 0.0144182
#       feature_fraction: 0.6585625
#       num_leaves: 365
#       min_data_in_leaf: 726
#       max_depth: 23
#       min_gain_to_split: 0.0570174
#       lambda_l1: 3.9986413
#       lambda_l2: 1.7045828
```

```{r chunk63}
# EXP HT 4942

# num_iterations 4593
# learning_rate 0.0427741
# feature_fraction 0.3755552
# num_leaves 1910
# min_data_in_leaf 2195
# min_gain_to_split 0.08282231
# min_sum_hessian_in_leaf 0.08637694
# lambda_l1 0.8836508
# lambda_l2 4.757157
# bagging_fraction 0.9806646
# bagging_freq 1
# scale_pos_weight 34.63562
# drop_rate 0.08215384
# max_drop 1
# skip_drop 0.3466582
# AUC 0.925877
```


```{r chunk73}

# # 9002 (fe123) bayesiana frenada en #30. AUC =  0.9441 

# PARAM$out$lgbm$mejores_hiperparametros <- list(
#   num_iterations = 4710,
#   learning_rate = 0.00775,
#   feature_fraction = 0.853,
#   num_leaves = 264,
#   min_data_in_leaf = 3,
#   max_depth = 15,
#   min_gain_to_split = 6.27e-05,
#   lambda_l1 = 0.0249,
#   lambda_l2 = 1.73,
#   bagging_fraction = 0.809,
#   bagging_freq = 2
# )
```

```{r}
# 9002 bayesiana completa #80. almost best AUC = 0.9448

# PARAM$out$lgbm$mejores_hiperparametros <- list(
#    num_iterations = 2757,
#    learning_rate = 0.0126,
#    feature_fraction = 0.758,
#    num_leaves = 282,
#    min_data_in_leaf = 3,
#    max_depth = 16,
#    min_gain_to_split = 0.00226,
#    lambda_l1 = 0.0381,
#    lambda_l2 = 0.000909,
#    bagging_fraction = 0.69,
#    bagging_freq = 1
# )

```

```{r}

# 9002 bayesiana completa #80. BEST AUC = 0.9450

# PARAM$out$lgbm$mejores_hiperparametros <- list(
#    num_iterations = 3910,                  ### ATENCION!!!!
#    learning_rate = 0.006639251,
#    feature_fraction = 0.7971601,
#    num_leaves = 248,
#    min_data_in_leaf = 4,
#    max_depth = 16,
#    min_gain_to_split = 0.01307809,
#    lambda_l1 = 0.02541107,
#    lambda_l2 = 0.02196564,
#    bagging_fraction = 0.6892582,
#    bagging_freq = 4
# )

```


```{r}
# EXP HT 9006 (en colab). es FE1. AUC = 0.9472
# 
# PARAM$out$lgbm$mejores_hiperparametros <- list(
#   num_iterations = 2610,
#   learning_rate = 0.0219,
#   feature_fraction = 0.61,
#   num_leaves = 455,
#   min_data_in_leaf = 4
# )
  
```

```{r}

# EXP 9012 (1,2,3, ranking y agui) provisorio hasta iter #36
# AUC= 0,9447
# 
# PARAM$out$lgbm$mejores_hiperparametros <- list(
#    num_iterations = 3158,
#    learning_rate = 0.0171,
#    feature_fraction = 0.904,
#    num_leaves = 372,
#    min_data_in_leaf = 3,  # Asumido, ya que no se especificó en el último set
#    max_depth = 12,
#    min_gain_to_split = 0.000113,
#    lambda_l1 = 0.015,
#    lambda_l2 = 7.11,
#    bagging_fraction = 0.949,
#    bagging_freq = 4
# )

```


```{r chunk75}
param_final <- modifyList(PARAM$lgbm$param_fijos,
  PARAM$out$lgbm$mejores_hiperparametros)

param_final
```

## semillas para ensamble
```{r chunk76}
#Solo si uso ensamble creo que no sería necesario luego de haber corregido el codigo. revisar

param_final$seed <- PARAM$semillas
```

### escalo por undersampling

```{r chunk77}
# este punto es muy SUTIL  y será revisado en la Clase 05
# acá retoma esto PARAM$trainingstrategy$undersampling ojo no haber borrado

param_normalizado <- copy(param_final)
param_normalizado$min_data_in_leaf <-  round(param_final$min_data_in_leaf / PARAM$trainingstrategy$undersampling)
```

### escalo por train reducido
```{r}

# Ajuste adicional por cambio en cantidad de registros reales
registros_optimizacion <- dataset[foto_mes %in% c(202103), .N]
registros_training <- dataset[foto_mes %in% c(202101, 202102, 202103, 202104), .N]
factor_registros <- registros_training / registros_optimizacion

param_normalizado$min_data_in_leaf <- round(param_normalizado$min_data_in_leaf * factor_registros)


```

```{r chunk78}
param_normalizado
```
# verifico dtrain_final

```{r}

cat("1. DIMENSIONES:\n")
cat("   Filas:", nrow(dtrain_final), "\n")
cat("   Columnas:", ncol(dtrain_final), "\n\n")

cat("2. PERÍODOS DE ENTRENAMIENTO:\n")
cat("   Configurados:", paste(PARAM$train_final, collapse = ", "), "\n")
dataset_train_check <- dataset[foto_mes %in% PARAM$train_final]
cat("   Filas en dataset para esos períodos:", nrow(dataset_train_check), "\n")
cat("   ¿Coincide con dtrain_final?", nrow(dataset_train_check) == nrow(dtrain_final), "\n\n")

cat("3. DISTRIBUCIÓN POR PERÍODO:\n")
print(dataset_train_check[, .N, by = foto_mes])
cat("\n")

cat("4. BALANCE DE CLASES:\n")
dataset_train_check[, clase01 := ifelse(clase_ternaria %in% c("BAJA+1", "BAJA+2"), 1L, 0L)]
print(dataset_train_check[, .N, by = clase01])
cat("   Proporción positivos:", 
    round(sum(dataset_train_check$clase01) / nrow(dataset_train_check) * 100, 2), "%\n\n")

cat("6. FUTURO A PREDECIR:\n")
dfuture <- dataset[foto_mes == 202106]
cat("   Período:", PARAM$future, "\n")
cat("   Filas en 202106:", nrow(dfuture), "\n")
cat("   ESPERADO: 164313\n")
cat("   DIFERENCIA:", 164313 - nrow(dfuture), "\n\n")

```

# TRAINING
Genero el modelo final, siempre sobre TODOS los datos de  final_train, sin hacer ningun tipo de undersampling de la clase mayoritaria y mucho menos cross validation.
```{r chunk79}
# # entreno LightGBM

# modelo_final <- lgb.train(
#   data= dtrain_final,
#   param= param_normalizado
# )
```

```{r}
# ENTRENO GUARDANDO DE A 1

# setwd( paste0("/content/buckets/b1/exp/", experimento ))
setwd(dir_experimento)

# Entreno ensamble mis semillas
cat("Entrenando", length(PARAM$semillas), "modelos con diferentes semillas...\n")

# Lista para guardar los modelos
modelos_ensemble <- list()

# Entrenar un modelo por cada semilla
for(i in 1:length(PARAM$semillas)) {
  cat("\n=== Entrenando modelo", i, "de", length(PARAM$semillas), "con semilla", PARAM$semillas[i], "===\n")
  
  set.seed(PARAM$semillas[i])
  param_normalizado$seed <- PARAM$semillas[i]
  
  # Entrenar modelo
  modelo <- lgb.train(
    data = dtrain_final,
    param = param_normalizado
  )
  
  # Guardar modelo INMEDIATAMENTE
  nombre_archivo <- paste0("modelo_", PARAM$experimento, "_", PARAM$semillas[i], ".txt")
  lgb.save(modelo, nombre_archivo)
  cat("Modelo", i, "guardado como:", nombre_archivo, "\n")
  
  # Guardar en lista
  modelos_ensemble[[i]] <- modelo
  
  cat("Modelo", i, "entrenado y guardado exitosamente\n")
}

cat("\n=== Ensamble de", length(PARAM$semillas), "modelos completado ===\n")


```

####retomo train si murió
```{r}
# # REANUDACIÓN DEL ENTRENAMIENTO DE SEMILLAS FALTANTES
# 
# # Asegurarse de estar en el directorio correcto
# setwd(dir_experimento)
# 
# # 1. IDENTIFICAR SEMILLAS YA ENTRENADAS
# # Obtener los nombres de los archivos de modelos ya guardados
# archivos_modelos <- list.files(pattern = "^modelo_seed_.*\\.txt$")
# 
# # Extraer las semillas de los nombres de archivo.
# # Esto convierte, por ejemplo, "modelo_seed_271828.txt" a "271828" y luego a un número.
# semillas_entrenadas <- as.numeric(gsub("modelo_seed_(.*)\\.txt", "\\1", archivos_modelos))
# 
# # 2. ENCONTRAR SEMILLAS FALTANTES
# # Usar setdiff para encontrar las semillas que están en PARAM$semillas pero NO en semillas_entrenadas
# semillas_faltantes <- setdiff(PARAM$semillas, semillas_entrenadas)
# 
# cat("Total de semillas a entrenar (original):", length(PARAM$semillas), "\n")
# cat("Semillas ya entrenadas:", length(semillas_entrenadas), "\n")
# cat("Semillas faltantes por entrenar:", length(semillas_faltantes), "\n")
# if (length(semillas_faltantes) > 0) {
#     cat("Semillas faltantes:", paste(semillas_faltantes, collapse = ", "), "\n")
# } else {
#     cat("¡Todo el ensamble ya fue entrenado y guardado!\n")
# }
# 
# 
# # 3. ENTRENAR SOLO LAS SEMILLAS FALTANTES
# 
# # Crear una lista auxiliar para el bucle de reanudación
# semillas_a_procesar <- semillas_faltantes
# 
# # Entrenar un modelo solo por cada semilla faltante
# for(semilla_actual in semillas_a_procesar) {
#     cat("\n=== Retomando entrenamiento con semilla faltante:", semilla_actual, "===\n")
# 
#     set.seed(semilla_actual)
#     param_normalizado$seed <- semilla_actual
# 
#     # Entrenar modelo (usando la semilla actual)
#     modelo <- lgb.train(
#         data = dtrain_final,
#         param = param_normalizado
#     )
# 
#     # Guardar modelo INMEDIATAMENTE
#     nombre_archivo <- paste0("modelo_seed_", semilla_actual, ".txt")
#     lgb.save(modelo, nombre_archivo)
#     cat("Modelo guardado como:", nombre_archivo, "\n")
# 
#     # Nota: No lo guardamos en 'modelos_ensemble' porque esa lista suele ser para
#     # usar en memoria. Aquí nos enfocamos solo en guardar el archivo faltante.
# 
#     cat("Modelo entrenado y guardado exitosamente\n")
# }
# 
# cat("\n=== Proceso de reanudación completado ===\n")
```


## importancia variables
```{r chunk82}
# ahora imprimo la importancia de variables

# tb_importancia <- as.data.table(lgb.importance(modelo_final))

tb_importancia <- as.data.table(lgb.importance(modelos_ensemble[[1]]))


# # para VM
setwd(dir_experimento)

fwrite(tb_importancia,
  file = paste0("impo_", PARAM$experimento,".txt"),
  sep = "\t"
)


```

```{r chunk83}
library(ggplot2)
library(scales)

tb_importancia_top <- head(tb_importancia[order(-Gain)], 20)

p <- ggplot(tb_importancia_top, aes(x = reorder(Feature, Gain), y = Gain)) +
  geom_bar(stat = "identity", fill = "#2C7BB6", alpha = 0.8) +
  geom_text(aes(label = round(Gain, 0)), hjust = -0.1, size = 3, color = "gray30") +
  coord_flip() +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.1))) +
  labs(
    title = paste0("Top 20 Feature Importance - EXP ", PARAM$experimento),
    subtitle = paste0("Modelo LightGBM - ", length(PARAM$semillas), " semillas"),
    x = NULL,
    y = "Gain",
    caption = paste0("Total features: ", nrow(tb_importancia))
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12, color = "gray40"),
    plot.caption = element_text(size = 9, color = "gray50"),
    axis.text = element_text(size = 10),
    panel.grid.major.y = element_blank(),
    panel.grid.minor = element_blank()
  )

# Mostrar gráfico
print(p)

# para VM

setwd(dir_experimento)

nombre_archivo <- paste0("feature_importance_ggplot_exp_", PARAM$experimento, ".png")
ggsave(nombre_archivo, p, width = 12, height = 8, dpi = 300)


```

```{r chunk84}
# Gráfico nativo de lightgbm

lgb.plot.importance(tb_importancia, top_n = 20, measure = "Gain")
title(main = paste0("Feature Importance - EXP ", PARAM$experimento), line = 0.5)

#guardar
setwd(dir_experimento)

png(paste0("feature_importance_lgb_exp_", PARAM$experimento, ".png"), width = 800, height = 600)
lgb.plot.importance(tb_importancia, top_n = 20, measure = "Gain")
title(main = paste0("Feature Importance - EXP ", PARAM$experimento), line = 0.5)
dev.off()


```

## Predicciones future
Aplico el modelo final a los datos del futuro
```{r chunk85}
# # aplico el modelo a los datos sin clase

# dfuture <- dataset[foto_mes %in% PARAM$future]

# # aplico el modelo a los datos nuevos
# prediccion <- predict(
#   modelo_final,
#   data.matrix(dfuture[, campos_buenos, with= FALSE])
# )
```

###cargo modelos desde otro lado
```{r}


### Cargo modelos desde otro lado

# OPCION 1: Local (comentar/descomentar según necesidad)
# dir_experimento <- "C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia 1/experimentos/exp9013"
# modelos_ensemble <- list()
# 
# cat("\n=== Cargando modelos locales ===\n")
# 
# for(i in 1:length(PARAM$semillas)) {
#   archivo_modelo <- file.path(dir_experimento, paste0("modelo_seed_", PARAM$semillas[i], ".txt"))
#   
#   if(file.exists(archivo_modelo)) {
#     cat("Cargando modelo", i, "- Semilla:", PARAM$semillas[i], "\n")
#     modelos_ensemble[[i]] <- lgb.load(archivo_modelo)
#   } else {
#     cat("ADVERTENCIA: No se encontró el archivo:", archivo_modelo, "\n")
#   }
# }
# 
# cat("\nTotal de modelos cargados:", length(modelos_ensemble), "\n")

# OPCION 2: Google Storage (comentar/descomentar según necesidad)
bucket_name <- "silvanacontreras76_bukito3"
bucket_path <- "exp/exp9013"

modelos_ensemble <- list()

cat("\n=== Cargando modelos desde Google Storage ===\n")

for(i in 1:length(PARAM$semillas)) {
  nombre_modelo <- paste0("modelo_seed_", PARAM$semillas[i], ".txt")
  url_modelo <- paste0("https://storage.googleapis.com/", bucket_name, "/", bucket_path, "/", nombre_modelo)
  
  cat("Descargando modelo", i, "- Semilla:", PARAM$semillas[i], "\n")
  
  archivo_temp <- tempfile(fileext = ".txt")
  download.file(url_modelo, archivo_temp, mode = "wb", quiet = TRUE)
  
  modelos_ensemble[[i]] <- lgb.load(archivo_temp)
  
  cat("✓ Modelo", i, "cargado\n")
}

cat("\nTotal de modelos cargados:", length(modelos_ensemble), "\n")


```

```{r chunk86}
# # aplico el modelo a los datos sin clase

setwd(dir_experimento)

# Definir dfuture
dfuture <- dataset[foto_mes %in% PARAM$future]

# Aplicar cada modelo y guardar predicciones
predicciones_ensemble <- list()

cat("\n=== Generando predicciones con el ensamble ===\n")

for(i in 1:length(PARAM$semillas)) {

  cat("Prediciendo con modelo", i, "\n")

  prediccion_temp <- predict(
    modelos_ensemble[[i]],
    data.matrix(dfuture[, campos_buenos, with = FALSE])
  )

  predicciones_ensemble[[i]] <- prediccion_temp
  
 # guardar predicciones individuales por semilla/modelo
  pred_seed_dt <- data.table(
    numero_de_cliente = dfuture$numero_de_cliente,
    foto_mes          = dfuture$foto_mes,
    seed              = PARAM$semillas[i],
    prob              = prediccion_temp
  )
  fwrite(
    pred_seed_dt,
    file = paste0("predicciones_indiv_",
                PARAM$experimento, "_", PARAM$semillas[i], ".txt"),
  sep = "\t"  )
  }

# Promediar las predicciones
prediccion <- rowMeans(do.call(cbind, predicciones_ensemble))

cat("Predicción final generada como promedio de",
length(PARAM$semillas), "modelos\n")

```


```{r chunk87}
# guarda la predicción promedio como una tabla

tb_prediccion <- dfuture[, list(numero_de_cliente, foto_mes)]
tb_prediccion[, prob := prediccion]

# guardar
setwd(dir_experimento)

# para VM
fwrite(tb_prediccion,
  file = paste0("prediccion_prom",
                PARAM$experimento, ".txt"),
  sep = "\t"
)

```

```{r chunk88}
# CONTROL FILAS
# Se deben entregar 164876 predicciones, paquete premium de 202106
# La primer linea del archivo tiene los títulos, con lo cual 164876 + 1 lineas
# error dice que debe tener: Submission must have 164313 rows

filas_prediccion <- nrow(tb_prediccion)
filas202106 <- nrow(dataset[foto_mes == 202106])

print(paste("'tb_prediccion' tiene", filas_prediccion, "filas."))
print(paste("'202106' tiene", filas202106, "filas."))
```

## submissions y envíos future
```{r chunk89}
# genero archivos con los  "envios" mejores



setorder(tb_prediccion, -prob)

setwd(dir_experimento) #para local R

# para VM
dir.create("kaggle", showWarnings = FALSE)

for (envios in PARAM$cortes) {
  tb_prediccion[, Predicted := 0L]
  tb_prediccion[1:envios, Predicted := 1L]

  archivo_kaggle <- paste0("./kaggle/KA", PARAM$experimento, "_", envios, ".csv")

  fwrite(tb_prediccion[, list(numero_de_cliente, Predicted)],
    file = archivo_kaggle,
    sep = ",",
    col.names = TRUE
  )#
  cat("Generado:", basename(archivo_kaggle), "\n")
}

```

# TESTEO
drealidad es el período a predecir con las columnas de interés y fold publico privado.  

Particionar arma este formato  fold = 1  es el 30% del dataset que representaría el "público" de la competencia y fold = 2 es el 70% de ldataset que representaría el "privado" de la competencia? y ambos folds mantienen la representación de los niveles de clase ternaria (continua, baja+1 y baja+2).

 Realidad_inicializar: ejecuta todo esto, arma el kaggle.  

 Realidad evaluar: junta predicciones con realidad en prealidad (cliente, clase ternaria, fold publico o privado y predicción 1 enviar 0 no enviar), calcula ganancias. tbl es tabla resumen de prealidad



```{r chunk90}

# particionar agrega una columna llamada fold a un dataset
#   que consiste en una particion estratificada segun agrupa
# particionar( data=dataset, division=c(70,30),
#  agrupa=clase_ternaria, seed=semilla)   crea una particion 70, 30

particionar <- function(data, division, agrupa= "", campo= "fold", start= 1, seed= NA) {
  if (!is.na(seed)) set.seed(seed, "L'Ecuyer-CMRG")

  bloque <- unlist(mapply(
    function(x, y) {rep(y, x)},division, seq(from= start, length.out= length(division))))

  data[, (campo) := sample(rep(bloque,ceiling(.N / length(bloque))))[1:.N],by= agrupa]
}
```

```{r chunk91}

# iniciliazo el dataset de realidad, para medir ganancia
realidad_inicializar <- function( pfuture, pparam) {

  # datos para verificar la ganancia
  drealidad <- pfuture[, list(numero_de_cliente, foto_mes, clase_ternaria)]

  particionar(drealidad,
    division= c(3, 7),
    agrupa= "clase_ternaria",
    seed= PARAM$semilla_kaggle
  )

  return( drealidad )
}
```

```{r chunk92}

# evaluo ganancia en los datos de la realidad

realidad_evaluar <- function( prealidad, pprediccion) {

  prealidad[ pprediccion,
    on= c("numero_de_cliente", "foto_mes"),
    predicted:= i.Predicted
  ]

  tbl <- prealidad[, list("qty"=.N), list(fold, predicted, clase_ternaria)]

  res <- list()
  res$public  <- tbl[fold==1 & predicted==1L, sum(qty*ifelse(clase_ternaria=="BAJA+2", 780000, -20000))]/0.3
  res$private <- tbl[fold==2 & predicted==1L, sum(qty*ifelse(clase_ternaria=="BAJA+2", 780000, -20000))]/0.7
  res$total <- tbl[predicted==1L, sum(qty*ifelse(clase_ternaria=="BAJA+2", 780000, -20000))]

  prealidad[, predicted:=NULL]
  return( res )
}
```

## predicciones false_future
```{r chunk97}

#PREDICCIONES POR SEMILLA Y PROMEDIO PREDICCIONES
setwd(dir_experimento)

false_future <- dataset[foto_mes %in% PARAM$false_future]

predicciones_ensemble_false_future <- list()

cat("\n=== Generando predicciones para testeo ===\n")

for(i in 1:length(PARAM$semillas)) {
  cat("Prediciendo con modelo", i, "- Semilla:", PARAM$semillas[i], "\n")

  prediccion_temp_false_future <- predict(
    modelos_ensemble[[i]],
    data.matrix(false_future[, campos_buenos, with = FALSE])
  )

  predicciones_ensemble_false_future[[i]] <- prediccion_temp_false_future
  
  tb_pred_individual <- false_future[, list(numero_de_cliente, foto_mes)]
  tb_pred_individual[, prob := prediccion_temp_false_future]
  
  nombre_pred <- paste0("prediccion_false_future_seed_", PARAM$experimento, "_",   PARAM$semillas[i], ".txt")
  fwrite(tb_pred_individual, file = nombre_pred, sep = "\t")
  cat("Guardada:", nombre_pred, "\n")
}

prediccion_false_future <- rowMeans(do.call(cbind, predicciones_ensemble_false_future))

cat("\nPredicción final false_future generada como promedio de",
    length(PARAM$semillas), "modelos\n")

```

```{r chunk98}

setwd(dir_experimento)

tb_prediccion_false_future <- false_future[, list(numero_de_cliente, foto_mes)]
tb_prediccion_false_future[, prob := prediccion_false_future]


# para VM
fwrite(tb_prediccion_false_future,
  file = paste0("prediccion_false_future_", PARAM$experimento, ".txt"),
  sep = "\t"
)


```

## drealidad backtesting
```{r chunk99}

# inicilizo el dataset  drealidad SOLO PARA BACKTESTING

drealidad <- realidad_inicializar( false_future, PARAM)
```


```{r chunk100}

PARAM$cortes
```

## ganancias de testing
```{r}

setwd(dir_experimento)  #PARA R LOCAL

# Crear/abrir archivo para guardar
test_ganancia_envios <- paste0("test_ganancia_envios_", PARAM$experimento, ".txt")
sink(test_ganancia_envios, split = TRUE)  # split = TRUE imprime en pantalla Y guarda

cat("EXPERIMENTO:", PARAM$experimento, "\n")
cat("Período Train Final:", paste(PARAM$train_final, collapse = ", "), "\n")
cat("Período False Future:", paste(PARAM$false_future, collapse = ", "), "\n")
cat("Cortes:", paste(range(PARAM$cortes), collapse = " a "), 
    " (step=", unique(diff(PARAM$cortes)), ")\n", sep = "")

setorder(tb_prediccion_false_future, -prob)
for (envios in PARAM$cortes) {
  tb_prediccion_false_future[, Predicted := 0L]
  tb_prediccion_false_future[1:envios, Predicted := 1L]
  res <- realidad_evaluar(drealidad, tb_prediccion_false_future)
  options(scipen = 999)
  cat("Envios=", envios, "\t",
      " TOTAL=", res$total,
      "  Public=", res$public,
      " Private=", res$private,
      "\n",
      sep = ""
  )
}

# Cerrar archivo
sink()
cat("\n Resultados guardados en:", test_ganancia_envios, "\n")


```

##ganancias por corte para curva
```{r chunk102}

# Guardar resultados de ganancia por cada corte (solo para graficar)
resultados_ganancia <- data.table()

setorder(tb_prediccion_false_future, -prob)

for (envios in PARAM$cortes) {
  tb_prediccion_false_future[, Predicted := 0L]
  tb_prediccion_false_future[1:envios, Predicted := 1L]
  
  res <- realidad_evaluar(drealidad, tb_prediccion_false_future)
  
  options(scipen = 999)
  cat("Envios=", envios, "\t",
    " TOTAL=", res$total,
    "  Public=", res$public,
    " Private=", res$private,
    "\n",
    sep = ""
  )
  
  resultados_ganancia <- rbind(resultados_ganancia,
    data.table(envios = envios, ganancia_total = res$total))
}

ptest <- ggplot(resultados_ganancia, aes(x = envios, y = ganancia_total)) +
  geom_line(color = "#2C7BB6", size = 1) +
  geom_point(color = "#2C7BB6", size = 2) +
  geom_hline(yintercept = 0, linetype = "solid", color = "gray50", alpha = 0.3) +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = paste0("Curva de Ganancia - EXP ", PARAM$experimento),
    x = "Cantidad de Envios",
    y = "Ganancia Total ($)"
  ) +
  theme_minimal()

print(ptest)

setwd(dir_experimento) #para Rstudio

# para VM

nombre_archivo <- paste0("curva_ganancia_exp_", PARAM$experimento, ".png")
ggsave(nombre_archivo, p, width = 10, height = 6, dpi = 300)


```

## grafico acumulado
```{r}
# chunk102b - Gráfico de ganancia acumulada (referencia sofi)

# Ordenar por probabilidad descendente
setorder(tb_prediccion_false_future, -prob)

# Calcular ganancia acumulada
tb_prediccion_false_future[, indice := 1:.N]

# Unir con drealidad para tener clase_ternaria
tb_ganancia <- merge(
  tb_prediccion_false_future[, .(numero_de_cliente, foto_mes, prob, indice)],
  drealidad[, .(numero_de_cliente, foto_mes, clase_ternaria)],
  by = c("numero_de_cliente", "foto_mes")
)

# Ordenar por indice para mantener el orden
setorder(tb_ganancia, indice)

# Calcular ganancia individual y acumulada
tb_ganancia[, ganancia_individual := ifelse(clase_ternaria == "BAJA+2", 780000, -20000)]
tb_ganancia[, ganancia_acumulada := cumsum(ganancia_individual)]

# Encontrar ganancia máxima
ganancia_maxima <- max(tb_ganancia$ganancia_acumulada)
indice_maximo <- tb_ganancia[ganancia_acumulada == ganancia_maxima, indice][1]

# Filtrar datos (umbral 66% de ganancia máxima)
umbral_ganancia <- ganancia_maxima * 0.66
tb_filtrada <- tb_ganancia[ganancia_acumulada >= umbral_ganancia]

# Crear gráfico
p_acumulada <- ggplot(tb_filtrada, aes(x = indice, y = ganancia_acumulada)) +
  geom_line(color = "blue", size = 1.2) +
  geom_point(data = tb_ganancia[indice == indice_maximo], 
             aes(x = indice, y = ganancia_acumulada),
             color = "red", size = 3) +
  annotate("label", 
           x = indice_maximo, 
           y = ganancia_maxima * 1.05,
           label = paste0("Ganancia Máxima\n", format(ganancia_maxima, big.mark = ",", scientific = FALSE)),
           color = "red", 
           fontface = "bold",
           fill = "white",
           label.size = 0.5) +
  geom_segment(aes(x = indice_maximo, y = ganancia_maxima,
                   xend = indice_maximo, yend = ganancia_maxima * 1.04),
               arrow = arrow(length = unit(0.3, "cm")),
               color = "red") +
  scale_x_continuous(labels = scales::comma) +
  scale_y_continuous(labels = scales::comma) +
  labs(
    title = paste0("Ganancia acumulada por orden de predicción (filtrada) - EXP ", PARAM$experimento),
    x = "Clientes ordenados por probabilidad",
    y = "Ganancia Acumulada"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    panel.grid.minor = element_line(alpha = 0.3)
  )

print(p_acumulada)

# Guardar
setwd(dir_experimento) #para R
nombre_archivo_acum <- paste0("curva_ganancia_acumulada_exp_", PARAM$experimento, ".png")
ggsave(nombre_archivo_acum, p_acumulada, width = 14, height = 8, dpi = 300)

cat("Ganancia máxima:", format(ganancia_maxima, big.mark = ","), "\n")
cat("Corte ideal por cliente:", indice_maximo, "\n")

```


```{r chunk104}

setwd(dir_experimento) #para R

#para VM
write_yaml(PARAM, file = "PARAM.yml")

```

```{r chunk105}


# CONTROL FILAS
# : Submission must have 164313 rows

filas_prediccion_ff <- nrow(tb_prediccion_false_future)
filas202104 <- nrow(dataset[foto_mes == 202104])

print(paste("'tb_prediccion_ff' tiene", filas_prediccion_ff, "filas."))
print(paste("'202104' tiene", filas202104, "filas."))
```

## Ganancias xseed @ best corte ensamble
- Se identifica el corte óptimo usando el ENSEMBLE PROMEDIO (promedio de 10 semillas)
- Luego se evalúa CADA SEMILLA INDIVIDUAL en ese mismo corte óptimo
- Resultado: Vector de 10 ganancias con criterio de decisión uniforme
- Justificación: Simula decisión real donde se elige UN corte y se aplica uniformemente
```{r}

setwd(dir_experimento)
cat("\n=== Identificando corte óptimo del ensemble ===\n")

mejor_ganancia <- -Inf
mejor_corte <- NA

setorder(tb_prediccion_false_future, -prob)

for(corte in PARAM$cortes) {
  tb_prediccion_false_future[, Predicted := 0L]
  tb_prediccion_false_future[1:corte, Predicted := 1L]
  
  res <- realidad_evaluar(drealidad, tb_prediccion_false_future)
  
  if(res$total > mejor_ganancia) {
    mejor_ganancia <- res$total
    mejor_corte <- corte
  }
}

cat("Corte óptimo ensemble:", mejor_corte, "envíos\n")
cat("Ganancia óptima ensemble:", format(mejor_ganancia, big.mark=","), "\n\n")


# Ver ganancia del ensemble en varios cortes alrededor de 11500
cortes_revisar <- c(10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000)

cat("\n=== Ganancias del ENSEMBLE PROMEDIO en cortes clave ===\n")

for(corte_test in cortes_revisar) {
  tb_test <- copy(tb_prediccion_false_future)
  setorder(tb_test, -prob)
  tb_test[, Predicted := 0L]
  tb_test[1:corte_test, Predicted := 1L]
  
  res <- realidad_evaluar(drealidad, tb_test)  
  
  cat("Corte", corte_test, ": ", format(res$total, big.mark=","), "\n")
}

cat("=== ganancias individuales por semilla en el corte óptimo del promedio ===\n")


ganancias_por_semilla <- data.table(
  semilla = integer(),
  ganancia = numeric()
)

for(i in 1:length(PARAM$semillas)) {
  archivo_pred <- paste0("prediccion_false_future_seed_", PARAM$semillas[i], ".txt")
  tb_pred <- fread(archivo_pred)
  
  setorder(tb_pred, -prob)
  tb_pred[, Predicted := 0L]
  tb_pred[1:mejor_corte, Predicted := 1L]
  
  res <- realidad_evaluar(drealidad, tb_pred)
  
  ganancias_por_semilla <- rbind(ganancias_por_semilla,
    data.table(semilla = PARAM$semillas[i], ganancia = res$total))
  
  cat("Semilla", PARAM$semillas[i], "- Ganancia:", format(res$total, big.mark=","), "\n")
}

archivo_salida <- paste0("ganancias_individuales_", PARAM$experimento, ".txt")
fwrite(ganancias_por_semilla, file = archivo_salida, sep = "\t")

cat("\n Ganancias individuales guardadas en:", archivo_salida, "\n")
```

```{r}

setwd(dir_experimento)
cat("\n=== Calculando ganancias de TODAS las semillas en TODOS los cortes ===\n")

ganancias_completas <- data.table()

for(i in 1:length(PARAM$semillas)) {
  
  archivo_pred <- paste0("prediccion_false_future_seed_", PARAM$semillas[i], ".txt")
  tb_pred <- fread(archivo_pred)
  
  setorder(tb_pred, -prob)
  
  for(corte in PARAM$cortes) {
    tb_pred[, Predicted := 0L]
    tb_pred[1:corte, Predicted := 1L]
    
    res <- realidad_evaluar(drealidad, tb_pred) 
    
    ganancias_completas <- rbind(ganancias_completas,
      data.table(
        semilla = PARAM$semillas[i],
        corte = corte,
        ganancia = res$total
      ))
  }
  
  cat("✓ Semilla", PARAM$semillas[i], "completada\n")
}

library(ggplot2)

p_heatmap <- ggplot(ganancias_completas, aes(x = corte, y = as.factor(semilla), fill = ganancia)) +
  geom_tile() +
  scale_fill_gradient2(low = "red", mid = "white", high = "green", 
                       midpoint = 0, labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  labs(
    title = paste("Ganancias por Semilla y Corte -", PARAM$experimento),
    x = "Corte (cantidad de envíos)",
    y = "Semilla",
    fill = "Ganancia"
  ) +
  theme_minimal()

print(p_heatmap)

p_lineas <- ggplot(ganancias_completas, aes(x = corte, y = ganancia, color = as.factor(semilla), group = semilla)) +
  geom_line(size = 0.8, alpha = 0.7) +
  geom_vline(xintercept = mejor_corte, linetype = "dashed", color = "red", size = 1) +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  labs(
    title = paste("Curvas de Ganancia por Semilla -", PARAM$experimento),
    subtitle = paste("Línea roja: corte óptimo =", mejor_corte),
    x = "Corte (cantidad de envíos)",
    y = "Ganancia",
    color = "Semilla"
  ) +
  theme_minimal()

print(p_lineas)

setwd(dir_experimento)
archivo_completo <- paste0("ganancias_todas_semillas_cortes_", PARAM$experimento, ".txt")
fwrite(ganancias_completas, file = archivo_completo, sep = "\t")

cat("\n✓ Ganancias completas guardadas en:", archivo_completo, "\n")
```


# WILCOXON ENTERO
Necesito:
modelos cargados por semilla
predicciones sobre futuro sin clase por semilla
dataset original en memoria (generado dtrainfinal)

## armo dataset_ con clase revelada_revelado

```{r}
# cargo dataset con información de futuro
# 
dataset_revelado <- fread("https://storage.googleapis.com/silvanacontreras76_bukito3/datasets/competencia_02_crudo.csv.gz", stringsAsFactors= TRUE)
# 


```

```{r}

dataset_revelado <- dataset_revelado[foto_mes >= 202101 & foto_mes <= 202108]

```

```{r}
#CLASE TERNARIA DATASET REVELADO
# calculo el periodo0 consecutivo
dsimple <- dataset_revelado[, list(
    "pos" = .I,
    numero_de_cliente,
    periodo0 = as.integer(foto_mes/100)*12 +  foto_mes%%100 ) ]


# ordeno
setorder( dsimple, numero_de_cliente, periodo0 )

# calculo topes
periodo_ultimo <- dsimple[, max(periodo0) ]
periodo_anteultimo <- periodo_ultimo - 1


# calculo orden 1 y 2
dsimple[, c("periodo1", "periodo2") :=
    shift(periodo0, n=1:2, fill=NA, type="lead"),  numero_de_cliente ]

# assign most common class values = "CONTINUA"
dsimple[ periodo0 < periodo_anteultimo, clase_ternaria := "CONTINUA" ]

# calculo BAJA+1
dsimple[ periodo0 < periodo_ultimo &
    ( is.na(periodo1) | periodo0 + 1 < periodo1 ),
    clase_ternaria := "BAJA+1" ]

# calculo BAJA+2
dsimple[ periodo0 < periodo_anteultimo & (periodo0+1 == periodo1 )
    & ( is.na(periodo2) | periodo0 + 2 < periodo2 ),
    clase_ternaria := "BAJA+2" ]


# pego el resultado en el dataset original y grabo
setorder( dsimple, pos )
dataset_revelado[, clase_ternaria := dsimple$clase_ternaria ]

```

### creo dataset_revelado_consolidado
```{r}
#sobreescribo clase ternaria de dataset con la info de dataset_revelado

dataset_revelado_consolidado <- copy(dataset)

# Actualizar MAYO y JUNIO
# dataset_revelado_consolidado[foto_mes %in% c(202105, 202106), 
#   clase_ternaria := dataset_revelado[foto_mes %in% c(202105, 202106), clase_ternaria]]

# 1) Preparar tabla con la verdad de mayo/junio
aux <- dataset_revelado[
  foto_mes %in% c(202105, 202106),
  .(numero_de_cliente, foto_mes, clase_ternaria)
]

# 2) Hacer update por join, alineando por (numero_de_cliente, foto_mes)
dataset_revelado_consolidado[
  aux, on = .(numero_de_cliente, foto_mes),
  clase_ternaria := i.clase_ternaria
]


setorder(dataset_revelado_consolidado, foto_mes, clase_ternaria, numero_de_cliente)
dataset_revelado_consolidado[, .N, list(foto_mes, clase_ternaria)]


```


```{r}

getwd()
fwrite(dataset, file = "competencia01_fe123_rankshort_revelado.csv.gz", sep = ",")
```

```{r}
# dataset_revelado_consolidado <- fread(
#   "C:/Users/Silvana/Documents/Maestria Exactas/DMEyF/competencia 1/competencia01_fe123_rankshort_revelado.csv.gz",
#   stringsAsFactors = TRUE
# )
```

## funciones para evaluar ganancia
```{r chunk90}

# particionar agrega una columna llamada fold a un dataset
#   que consiste en una particion estratificada segun agrupa
# particionar( data=dataset, division=c(70,30),
#  agrupa=clase_ternaria, seed=semilla)   crea una particion 70, 30

particionar <- function(data, division, agrupa= "", campo= "fold", start= 1, seed= NA) {
  if (!is.na(seed)) set.seed(seed, "L'Ecuyer-CMRG")

  bloque <- unlist(mapply(
    function(x, y) {rep(y, x)},division, seq(from= start, length.out= length(division))))

  data[, (campo) := sample(rep(bloque,ceiling(.N / length(bloque))))[1:.N],by= agrupa]
}
```

```{r chunk91}

# iniciliazo el dataset de realidad, para medir ganancia
realidad_inicializar <- function( pfuture, pparam) {

  # datos para verificar la ganancia
  drealidad <- pfuture[, list(numero_de_cliente, foto_mes, clase_ternaria)]

  particionar(drealidad,
    division= c(3, 7),
    agrupa= "clase_ternaria",
    seed= PARAM$semilla_kaggle
  )

  return( drealidad )
}
```

```{r chunk92}

# evaluo ganancia en los datos de la realidad

realidad_evaluar <- function( prealidad, pprediccion) {

  prealidad[ pprediccion,
    on= c("numero_de_cliente", "foto_mes"),
    predicted:= i.Predicted
  ]

  tbl <- prealidad[, list("qty"=.N), list(fold, predicted, clase_ternaria)]

  res <- list()
  res$public  <- tbl[fold==1 & predicted==1L, sum(qty*ifelse(clase_ternaria=="BAJA+2", 780000, -20000))]/0.3
  res$private <- tbl[fold==2 & predicted==1L, sum(qty*ifelse(clase_ternaria=="BAJA+2", 780000, -20000))]/0.7
  res$total <- tbl[predicted==1L, sum(qty*ifelse(clase_ternaria=="BAJA+2", 780000, -20000))]

  prealidad[, predicted:=NULL]
  return( res )
}
```

### REQUIERE dataset_train en memoria

```{r chunk70}
# los campos que se van a utilizar

campos_buenos <- setdiff(
  colnames(dataset_train),
  c("clase_ternaria", "clase01", "azar", "training")
)
```

##drealidad_true

```{r}
setwd(dir_experimento)

dfuture <- dataset[foto_mes %in% PARAM$future]

predicciones_lista <- list()
for(i in 1:length(PARAM$semillas)) {
  archivo_pred <- paste0("predicciones_indiv_", PARAM$experimento, "_", PARAM$semillas[i], ".txt")
  tb_temp <- fread(archivo_pred)
  predicciones_lista[[i]] <- tb_temp$prob
}

prediccion <- rowMeans(do.call(cbind, predicciones_lista))

cat("\n=== Calculando corte óptimo del ensemble ===\n")

tb_pred_promedio <- dfuture[, list(numero_de_cliente, foto_mes)]
tb_pred_promedio[, prob := prediccion]

drealidad_true <- dataset_revelado_consolidado[foto_mes %in% PARAM$future, 
  list(numero_de_cliente, foto_mes, clase_ternaria)]

particionar(drealidad_true,
  division= c(3, 7),
  agrupa= "clase_ternaria",
  seed= PARAM$semilla_kaggle
)

mejor_ganancia <- -Inf
mejor_corte <- NA

setorder(tb_pred_promedio, -prob)

for(corte in PARAM$cortes) {
  tb_pred_promedio[, Predicted := 0L]
  tb_pred_promedio[1:corte, Predicted := 1L]
  
  res <- realidad_evaluar(drealidad_true, tb_pred_promedio)
  
  if(res$total > mejor_ganancia) {
    mejor_ganancia <- res$total
    mejor_corte <- corte
  }
}

cat("Corte óptimo:", mejor_corte, "\n")
cat("Ganancia óptima:", format(mejor_ganancia, big.mark=","), "\n\n")

cat("=== Calculando ganancias individuales por semilla ===\n")

ganancias_por_semilla_true <- data.table(
  semilla = integer(),
  ganancia = numeric()
)

for(i in 1:length(PARAM$semillas)) {
  archivo_pred <- paste0("predicciones_indiv_", PARAM$experimento, "_", PARAM$semillas[i], ".txt")
  tb_pred <- fread(archivo_pred)
  
  setorder(tb_pred, -prob)
  tb_pred[, Predicted := 0L]
  tb_pred[1:mejor_corte, Predicted := 1L]
  
  res <- realidad_evaluar(drealidad_true, tb_pred)
  
  ganancias_por_semilla_true <- rbind(ganancias_por_semilla_true,
    data.table(semilla = PARAM$semillas[i], ganancia = res$total))
  
  cat("Semilla", PARAM$semillas[i], "- Ganancia:", format(res$total, big.mark=","), "\n")
}

archivo_salida <- paste0("ganancias_true_future_", PARAM$experimento, ".txt")
fwrite(ganancias_por_semilla_true, file = archivo_salida, sep = "\t")

cat("\n Ganancias guardadas en:", archivo_salida, "\n")
```
```{r}
# Ver ganancia del ensemble en varios cortes alrededor de 11500
cortes_revisar <- c(10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000)

cat("\n=== Ganancias del ENSEMBLE PROMEDIO en cortes clave ===\n")

for(corte_test in cortes_revisar) {
  tb_test <- copy(tb_pred_promedio)
  setorder(tb_test, -prob)
  tb_test[, Predicted := 0L]
  tb_test[1:corte_test, Predicted := 1L]
  
  res <- realidad_evaluar(drealidad_true, tb_test)
  
  cat("Corte", corte_test, ": ", format(res$total, big.mark=","), "\n")
}

```

```{r}
setwd(dir_experimento)
cat("\n=== Calculando ganancias de TODAS las semillas en TODOS los cortes ===\n")

ganancias_completas <- data.table()

for(i in 1:length(PARAM$semillas)) {
  archivo_pred <- paste0("predicciones_indiv_", PARAM$experimento, "_", PARAM$semillas[i], ".txt")
  tb_pred <- fread(archivo_pred)
  
  setorder(tb_pred, -prob)
  
  for(corte in PARAM$cortes) {
    tb_pred[, Predicted := 0L]
    tb_pred[1:corte, Predicted := 1L]
    
    res <- realidad_evaluar(drealidad_true, tb_pred)
    
    ganancias_completas <- rbind(ganancias_completas,
      data.table(
        semilla = PARAM$semillas[i],
        corte = corte,
        ganancia = res$total
      ))
  }
  
  cat("✓ Semilla", PARAM$semillas[i], "completada\n")
}


library(ggplot2)
p_lineas <- ggplot(ganancias_completas, aes(x = corte, y = ganancia, color = as.factor(semilla), group = semilla)) +
  geom_line(size = 0.8, alpha = 0.7) +
  geom_vline(xintercept = mejor_corte, linetype = "dashed", color = "red", size = 1) +
  scale_y_continuous(labels = scales::comma) +
  scale_x_continuous(labels = scales::comma) +
  labs(
    title = paste("Curvas de Ganancia por Semilla -", PARAM$experimento),
    subtitle = paste("Línea roja: corte óptimo =", mejor_corte),
    x = "Corte (cantidad de envíos)",
    y = "Ganancia",
    color = "Semilla"
  ) +
  theme_minimal()

print(p_lineas)

setwd(dir_experimento)
archivo_completo <- paste0("ganancias_todas_semillas_cortes_", PARAM$experimento, ".txt")
fwrite(ganancias_completas, file = archivo_completo, sep = "\t")

cat("\n✓ Ganancias completas guardadas en:", archivo_completo, "\n")

```

```{r chunk106}
format(Sys.time(), "%a %b %d %X %Y")
```


